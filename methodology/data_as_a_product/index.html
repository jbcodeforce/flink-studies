<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/flink-studies/methodology/data_as_a_product/ rel=canonical><link href=../../coding/cep/ rel=prev><link href=../coe/ rel=next><link rel=icon href=../../images/logo-blue.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.2"><title>Data as a product - Apache and Confluent Flink Studies</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#moving-to-a-data-as-a-product-architecture class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Apache and Confluent Flink Studies" class="md-header__button md-logo" aria-label="Apache and Confluent Flink Studies" data-md-component=logo> <img src=../../images/flink-header-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Apache and Confluent Flink Studies </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Data as a product </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/flink-studies.git title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Foundations </a> </li> <li class=md-tabs__item> <a href=../../cookbook/ class=md-tabs__link> Cookbook </a> </li> <li class=md-tabs__item> <a href=../../coding/flink-sql-clients/ class=md-tabs__link> Flink_App_Coding </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Methodology </a> </li> <li class=md-tabs__item> <a href=../../techno/ccloud-flink/ class=md-tabs__link> Related_Technologies </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io/eda-studies class=md-tabs__link> EDA </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Apache and Confluent Flink Studies" class="md-nav__button md-logo" aria-label="Apache and Confluent Flink Studies" data-md-component=logo> <img src=../../images/flink-header-logo.svg alt=logo> </a> Apache and Confluent Flink Studies </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/flink-studies.git title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> Foundations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Foundations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> Flink Key Concepts </span> </a> </li> <li class=md-nav__item> <a href=../../coding/getting-started/ class=md-nav__link> <span class=md-ellipsis> Getting started </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/flink-sql/ class=md-nav__link> <span class=md-ellipsis> Flink SQL concepts </span> </a> </li> <li class=md-nav__item> <a href=../../labs/ class=md-nav__link> <span class=md-ellipsis> Code&Demos </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/agentic_flink/ class=md-nav__link> <span class=md-ellipsis> Agentic applications </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Cookbook </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Cookbook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../cookbook/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/considerations/ class=md-nav__link> <span class=md-ellipsis> Considerations </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/cluster_mgt/ class=md-nav__link> <span class=md-ellipsis> Cluster management </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/job_lifecycle/ class=md-nav__link> <span class=md-ellipsis> Job Lifecycle </span> </a> </li> <li class=md-nav__item> <a href=../../coding/k8s-deploy/ class=md-nav__link> <span class=md-ellipsis> FKO & CMF Deployment </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/terraform/ class=md-nav__link> <span class=md-ellipsis> Confluent Cloud Terraform </span> </a> </li> <li class=md-nav__item> <a href=../../techno/fk-k8s-monitor/ class=md-nav__link> <span class=md-ellipsis> Monitoring </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Flink_App_Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Flink_App_Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Flink SQL coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Flink SQL coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/flink-sql-clients/ class=md-nav__link> <span class=md-ellipsis> SQL Clients </span> </a> </li> <li class=md-nav__item> <a href=../../coding/flink-sql-1/ class=md-nav__link> <span class=md-ellipsis> Create Table (SQL) </span> </a> </li> <li class=md-nav__item> <a href=../../coding/flink-sql-2/ class=md-nav__link> <span class=md-ellipsis> SQL DML </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> Java - Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Java - Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/table-api/ class=md-nav__link> <span class=md-ellipsis> Table API </span> </a> </li> <li class=md-nav__item> <a href=../../coding/datastream/ class=md-nav__link> <span class=md-ellipsis> DataStreams API </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../coding/udf_sql/ class=md-nav__link> <span class=md-ellipsis> UDFs & PTFs </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex=0> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/shift_left_utils/ class=md-nav__link> <span class=md-ellipsis> Manage CC Flink projects </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5 id=__nav_3_5_label tabindex=0> <span class=md-ellipsis> More advanced topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> More advanced topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/stateful-func/ class=md-nav__link> <span class=md-ellipsis> Stateful function </span> </a> </li> <li class=md-nav__item> <a href=../../coding/cep/ class=md-nav__link> <span class=md-ellipsis> Complex Event Processing </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Methodology </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Methodology </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Data as a product </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Data as a product </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#context class=md-nav__link> <span class=md-ellipsis> Context </span> </a> <nav class=md-nav aria-label=Context> <ul class=md-nav__list> <li class=md-nav__item> <a href=#operational-data-and-analytical-data class=md-nav__link> <span class=md-ellipsis> Operational Data and Analytical data </span> </a> </li> <li class=md-nav__item> <a href=#current-challenges class=md-nav__link> <span class=md-ellipsis> Current Challenges </span> </a> </li> <li class=md-nav__item> <a href=#core-principles-for-data-mesh class=md-nav__link> <span class=md-ellipsis> Core Principles for Data Mesh </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#a-data-product-approach class=md-nav__link> <span class=md-ellipsis> A Data Product Approach </span> </a> <nav class=md-nav aria-label="A Data Product Approach"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#data-as-a-product class=md-nav__link> <span class=md-ellipsis> Data as a Product </span> </a> </li> <li class=md-nav__item> <a href=#elements-of-a-data-product class=md-nav__link> <span class=md-ellipsis> Elements of a Data Product </span> </a> </li> <li class=md-nav__item> <a href=#data-contracts-for-streaming-products class=md-nav__link> <span class=md-ellipsis> Data Contracts for Streaming Products </span> </a> </li> <li class=md-nav__item> <a href=#dual-nature-storage-streaming-and-batch class=md-nav__link> <span class=md-ellipsis> Dual-Nature Storage: Streaming and Batch </span> </a> </li> <li class=md-nav__item> <a href=#methodology class=md-nav__link> <span class=md-ellipsis> Methodology </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#motivations-for-moving-to-data-stream-processing class=md-nav__link> <span class=md-ellipsis> Motivations for Moving to Data Stream Processing </span> </a> <nav class=md-nav aria-label="Motivations for Moving to Data Stream Processing"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#assessment-questions class=md-nav__link> <span class=md-ellipsis> Assessment questions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#migration-context class=md-nav__link> <span class=md-ellipsis> Migration Context </span> </a> </li> <li class=md-nav__item> <a href=#measuring-data-product-slos-in-flink class=md-nav__link> <span class=md-ellipsis> Measuring Data Product SLOs in Flink </span> </a> <nav class=md-nav aria-label="Measuring Data Product SLOs in Flink"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#freshness class=md-nav__link> <span class=md-ellipsis> Freshness </span> </a> </li> <li class=md-nav__item> <a href=#completeness class=md-nav__link> <span class=md-ellipsis> Completeness </span> </a> </li> <li class=md-nav__item> <a href=#cost-per-product-finops class=md-nav__link> <span class=md-ellipsis> Cost-per-Product (FinOps) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#data-discovery-via-flink-catalogs class=md-nav__link> <span class=md-ellipsis> Data Discovery via Flink Catalogs </span> </a> <nav class=md-nav aria-label="Data Discovery via Flink Catalogs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#centralized-catalogs class=md-nav__link> <span class=md-ellipsis> Centralized Catalogs </span> </a> </li> <li class=md-nav__item> <a href=#making-products-discoverable class=md-nav__link> <span class=md-ellipsis> Making Products Discoverable </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#versioning-and-schema-evolution class=md-nav__link> <span class=md-ellipsis> Versioning and Schema Evolution </span> </a> <nav class=md-nav aria-label="Versioning and Schema Evolution"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#state-compatibility-challenges class=md-nav__link> <span class=md-ellipsis> State Compatibility Challenges </span> </a> </li> <li class=md-nav__item> <a href=#migration-strategies class=md-nav__link> <span class=md-ellipsis> Migration Strategies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#some-implementation-challenges class=md-nav__link> <span class=md-ellipsis> Some implementation challenges </span> </a> <nav class=md-nav aria-label="Some implementation challenges"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#git-project-organization class=md-nav__link> <span class=md-ellipsis> Git project organization </span> </a> </li> <li class=md-nav__item> <a href=#joins-considerations class=md-nav__link> <span class=md-ellipsis> Joins considerations </span> </a> </li> <li class=md-nav__item> <a href=#state-management-as-a-product-lifecycle-challenge class=md-nav__link> <span class=md-ellipsis> State Management as a Product Lifecycle Challenge </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#source-of-information-go-deeper class=md-nav__link> <span class=md-ellipsis> Source of information - go deeper </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/eda-studies/methodology/event-storming/ class=md-nav__link> <span class=md-ellipsis> Event Storming </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/shift_left_utils/ class=md-nav__link> <span class=md-ellipsis> Migrate to real-time processing tools </span> </a> </li> <li class=md-nav__item> <a href=../coe/ class=md-nav__link> <span class=md-ellipsis> Center of Excellence </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/flink_project_demos/c360/spark_project/ class=md-nav__link> <span class=md-ellipsis> A C360 data product demo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Related_Technologies </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Related_Technologies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/ccloud-flink/ class=md-nav__link> <span class=md-ellipsis> Confluent Cloud Flink </span> </a> </li> <li class=md-nav__item> <a href=../../techno/cp-flink/ class=md-nav__link> <span class=md-ellipsis> Confluent Platform for Flink </span> </a> </li> <li class=md-nav__item> <a href=../../architecture/kafka/ class=md-nav__link> <span class=md-ellipsis> Kafka Integration </span> </a> </li> <li class=md-nav__item> <a href=../../techno/cc-tableflow/ class=md-nav__link> <span class=md-ellipsis> Confluent TableFlow </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/techno/data/#data-related-technologies class=md-nav__link> <span class=md-ellipsis> Apache Iceberg </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/kafka-studies class=md-nav__link> <span class=md-ellipsis> Kafka-studies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/eda-studies class=md-nav__link> <span class=md-ellipsis> EDA </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#context class=md-nav__link> <span class=md-ellipsis> Context </span> </a> <nav class=md-nav aria-label=Context> <ul class=md-nav__list> <li class=md-nav__item> <a href=#operational-data-and-analytical-data class=md-nav__link> <span class=md-ellipsis> Operational Data and Analytical data </span> </a> </li> <li class=md-nav__item> <a href=#current-challenges class=md-nav__link> <span class=md-ellipsis> Current Challenges </span> </a> </li> <li class=md-nav__item> <a href=#core-principles-for-data-mesh class=md-nav__link> <span class=md-ellipsis> Core Principles for Data Mesh </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#a-data-product-approach class=md-nav__link> <span class=md-ellipsis> A Data Product Approach </span> </a> <nav class=md-nav aria-label="A Data Product Approach"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#data-as-a-product class=md-nav__link> <span class=md-ellipsis> Data as a Product </span> </a> </li> <li class=md-nav__item> <a href=#elements-of-a-data-product class=md-nav__link> <span class=md-ellipsis> Elements of a Data Product </span> </a> </li> <li class=md-nav__item> <a href=#data-contracts-for-streaming-products class=md-nav__link> <span class=md-ellipsis> Data Contracts for Streaming Products </span> </a> </li> <li class=md-nav__item> <a href=#dual-nature-storage-streaming-and-batch class=md-nav__link> <span class=md-ellipsis> Dual-Nature Storage: Streaming and Batch </span> </a> </li> <li class=md-nav__item> <a href=#methodology class=md-nav__link> <span class=md-ellipsis> Methodology </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#motivations-for-moving-to-data-stream-processing class=md-nav__link> <span class=md-ellipsis> Motivations for Moving to Data Stream Processing </span> </a> <nav class=md-nav aria-label="Motivations for Moving to Data Stream Processing"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#assessment-questions class=md-nav__link> <span class=md-ellipsis> Assessment questions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#migration-context class=md-nav__link> <span class=md-ellipsis> Migration Context </span> </a> </li> <li class=md-nav__item> <a href=#measuring-data-product-slos-in-flink class=md-nav__link> <span class=md-ellipsis> Measuring Data Product SLOs in Flink </span> </a> <nav class=md-nav aria-label="Measuring Data Product SLOs in Flink"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#freshness class=md-nav__link> <span class=md-ellipsis> Freshness </span> </a> </li> <li class=md-nav__item> <a href=#completeness class=md-nav__link> <span class=md-ellipsis> Completeness </span> </a> </li> <li class=md-nav__item> <a href=#cost-per-product-finops class=md-nav__link> <span class=md-ellipsis> Cost-per-Product (FinOps) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#data-discovery-via-flink-catalogs class=md-nav__link> <span class=md-ellipsis> Data Discovery via Flink Catalogs </span> </a> <nav class=md-nav aria-label="Data Discovery via Flink Catalogs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#centralized-catalogs class=md-nav__link> <span class=md-ellipsis> Centralized Catalogs </span> </a> </li> <li class=md-nav__item> <a href=#making-products-discoverable class=md-nav__link> <span class=md-ellipsis> Making Products Discoverable </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#versioning-and-schema-evolution class=md-nav__link> <span class=md-ellipsis> Versioning and Schema Evolution </span> </a> <nav class=md-nav aria-label="Versioning and Schema Evolution"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#state-compatibility-challenges class=md-nav__link> <span class=md-ellipsis> State Compatibility Challenges </span> </a> </li> <li class=md-nav__item> <a href=#migration-strategies class=md-nav__link> <span class=md-ellipsis> Migration Strategies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#some-implementation-challenges class=md-nav__link> <span class=md-ellipsis> Some implementation challenges </span> </a> <nav class=md-nav aria-label="Some implementation challenges"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#git-project-organization class=md-nav__link> <span class=md-ellipsis> Git project organization </span> </a> </li> <li class=md-nav__item> <a href=#joins-considerations class=md-nav__link> <span class=md-ellipsis> Joins considerations </span> </a> </li> <li class=md-nav__item> <a href=#state-management-as-a-product-lifecycle-challenge class=md-nav__link> <span class=md-ellipsis> State Management as a Product Lifecycle Challenge </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#source-of-information-go-deeper class=md-nav__link> <span class=md-ellipsis> Source of information - go deeper </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=moving-to-a-data-as-a-product-architecture>Moving to a Data as a Product Architecture<a class=headerlink href=#moving-to-a-data-as-a-product-architecture title="Permanent link">&para;</a></h1> <details class="- info"> <summary>Version</summary> <p>Created 01/2025 Update 12/2025</p> </details> <p>This chapter provides a practical overview of current data lake and lakehouse challenges, discusses the implementation of 'data as a product' principles, and demonstrates how real-time streaming can be effectively integrated into modern data architectures.</p> <h2 id=context>Context<a class=headerlink href=#context title="Permanent link">&para;</a></h2> <h3 id=operational-data-and-analytical-data>Operational Data and Analytical data<a class=headerlink href=#operational-data-and-analytical-data title="Permanent link">&para;</a></h3> <p>The classical data landscape is split between operational data, which powers real-time applications, and analytical data, which provides historical insights for decision-making and machine learning. This separation has created complex and fragile data architectures, marked by problematic ETL processes and intricate data pipelines. The challenge lies in effectively bridging these two distinct data planes to ensure seamless data flow and integration.</p> <figure> <img alt src=../diagrams/data_op_data_planes.drawio.png width=700> <figcaption>Two data planes: real-time applications, and analytical data</figcaption> </figure> <p>The initial data platform architecture comprised a database on one side and a data warehouse on the other, with ETL jobs facilitating data movement between them. This setup can lead to bottlenecks, especially when different teams are working on various parts of an application but all relying on the same data source. It might also complicate scalability and flexibility.</p> <p>To address scaling challenges and support unstructured data, the second generation of data platforms, emerging in the mid-2000s, adopted distributed object storage, leading to the development of the Data Lake.</p> <p>The medallion architecture, a three-layered approach, is a common framework for organizing data lakes. This structure, as illustrated in the figure below, is driven by several key motivations:</p> <figure> <img alt src=../diagrams/medallion_arch.drawio.png width=700> <figcaption>Medallion Architecture</figcaption> </figure> <ul> <li>Leveraging cloud object storage to accommodate large volumes of both structured and unstructured data.</li> <li>Implementing data pipelines to transform data progressively, from raw landing zones to business-level aggregates.</li> <li>Facilitating data management and governance through data cataloging and distributed query tools.</li> <li>Organizing data based on its transformation stage, rather than business domains or specific use cases.</li> </ul> <p>Data product and its extension with <strong>Data Mesh</strong> helps to restructure those two planes with a domain and use case centric approach, and not a technology stack.</p> <h3 id=current-challenges>Current Challenges<a class=headerlink href=#current-challenges title="Permanent link">&para;</a></h3> <p>In Lakehouse or data lake architecture: </p> <ul> <li>We observe complex ETL jobs landscape, with high failure rate.</li> <li>Not all data needs the three layers architecture, but a more service contract type of data usage. Data becoming a product like a microservice.</li> <li>There is a latency issue to get the data, we talk about T + 1 to get fresh data. The + 1 can be one day or one hour, but it has latency that may not what business requirements need.</li> <li>Simple transformations need to be done with the ETL or ELT tool with the predefined staging. Not specific use-case driven implementation of the data retrieval and processing. </li> <li>Data are <strong>pulled</strong> from their sources and between layers. It could be micro-batches, or long-running batches. At the bronze layer, the data are duplicated, and there is minimum of quality control done.</li> <li>In the silver layer the filtering and transformations are also generic with no specific business context.</li> <li>The gold layer includes all data of all use cases. This is where most of the work is done for data preparation and develop higher quality level. This is the layer with a lot of demands from end-user and continuous update and new aggregation developments. </li> <li>This is the final consumer of the data lake gold layer that are pulling the data with specific Service Level Objectives. </li> <li>Data created at the gold level, most likely needs to be reingected to the operational databases to be visible to operation applications. This introduces the concept of <strong>reverse ETL</strong>. </li> <li>Each layer may have dfferent actors responsible to process the data: data platform engineer, analytic engineers and data modelers, and at the application level, the application developers.</li> <li>Storing multiple copies of data across layers inflates cloud storage expenses. Data become quickly stale and unreliable.</li> <li>Constant movement of data through layers results in unnecessary processing and query inefficiencies.</li> <li>The operational estate is also continuously growing, by adding mobile applications, serverless functions, cloud native apps, etc...</li> </ul> <h3 id=core-principles-for-data-mesh>Core Principles for Data Mesh<a class=headerlink href=#core-principles-for-data-mesh title="Permanent link">&para;</a></h3> <p>To address the concerns of siloed and incompatible data, while addressing scaling to constant change of data landscape, adding more data source and consumers, adding more transformations and processing resources, the data mesh is based on four core principles:</p> <ol> <li> <p>Domain-oriented decentralized <strong>data ownership</strong> and architecture. The components are the analytical data, the metadata and the computer resources to serve it. Data ownership is linked to the DDD bounded context. For a product management use case, the bounded context of a <code>Product</code>, supports operational APIs and analytical data endpoints to address <em>active users, feature usage, and conversion rates</em>, for example: </p> <p><figure markdown=span> <img alt="Bounded context data product" src=../diagrams/bd_ctx_product.drawio.png width=500> <figcaption>Data as a product - Bounded context</figcaption> </figure></p> <p>Also multiple bounded contexts could be presented via their dependencies to other domain operational and analytical data endpoints.</p> </li> <li> <p><strong>Data as a product</strong>, includes clear scope definition, product ownership and metrics to ensure data quality, user acceptance, lead time for data consumption. Data as a product includes documenting the users, how they access the data, and for what kind of operations. The accountability of the data quality shifts to the source of the data. It encapsulates three structural components: <strong>1/ code</strong> (data pipelines, schema definitions, APIs, event processing, monitoring metrics, access control), <strong>2/ data and metadata</strong> in a polyglot form (events, REST, tables, graphs, batch files...), <strong>3/ infrastructure</strong> (to run code, store data and metadata).</p> <p><figure markdown=span> <img alt=data-product-components src=../diagrams/dp_components.drawio.png> <figcaption>Data as a product: component view</figcaption> </figure></p> </li> <li> <p><strong>Self-serve data infrastructure as a platform</strong>, to enable domain autonomy, as microservices are defined and orchestrated. It includes callable polyglot data storage, data products schema, data pipeline declaration and orchestration, data products lineage, compute and data locality. The capabilities includes 1/ <strong>infrastructure provisioning</strong> via code for storage, service accounts, access policies, server provisioning for running code and jobs, 2/ <strong>data product interface</strong>, declarative interfaces to manage the life cycle of a data product, 3/ <strong>supervision plane</strong> to present the relation between data products, support discovery, build data catalog, to execute semantic query.</p> <p><figure markdown=span> <img alt=DP-infrastructure-platform src=../diagrams/infrastructure_platform.drawio.png> <figcaption>Data as a product: infrastructure platform</figcaption> </figure></p> </li> <li> <p>Federated governance to address interoperability of the data products. This needs to support decentralized and domain self-sovereignty, interoperability through standardization. </p> </li> </ol> <details class="- warning"> <summary>Moving to Kafka and real-time processing is not the full story</summary> <p>Changing the batch pipeline processing technologies to real-time processing using the medallion architecture does not solve the previously mentionned problems. We still need to shift paradign and adopt a data as a product centric architecture. The following diagram illustrates the mediallon layers, done with Flink processing and Kafka topics for storage.</p> <p><figure markdown=span> <img alt src=../diagrams/hl-rt-integration.drawio.png> <figcaption>Real-time intgration</figcaption> </figure></p> <p>Using topics as data record storage and Flink statements for transforming, filtering and enriching to the silver layer, also using kafka topics is the same ETL approach but with different technologies. </p> <p>Another, more detailed view, using Kafka Connectors will look like in the diagram below, where the three layers are using the Kimdall practices of source processing, intermediates and sinks.</p> <p><figure markdown=span> <img alt src=../diagrams/generic_src_to_sink_flow.drawio.png> <figcaption>Generic source to sink pipeline</figcaption> </figure></p> <p>Even if append-logs are part of the data as a product architecture, there are more to address and to organize the component development.</p> </details> <h2 id=a-data-product-approach>A Data Product Approach<a class=headerlink href=#a-data-product-approach title="Permanent link">&para;</a></h2> <p>As seen previously, domains need to host and serve their domain datasets in an easily consumable way, rather than flowing the data from domains into a centrally owned data lake or platform. Dataset from one domain may be consumed by another domains in a format suitable for its own application. Consumer pulls the dataset.</p> <figure> <img alt src=../diagrams/rti_dps.drawio.png> <figcaption>Data product reused by other domains</figcaption> </figure> <p>So developing data as a product means shifting from push and ingest of ETL and ELT processes to serving and pull model across all domains. </p> <h3 id=data-as-a-product>Data as a Product<a class=headerlink href=#data-as-a-product title="Permanent link">&para;</a></h3> <p>Data products serve analytical data, they are self-contained, deployable, valuable and exhibit eight characteristics:</p> <ul> <li><strong>Discoverable</strong>: data consumers can easily find the data product for their use case. A common implementation is to have a registry, a data catalogue, of all available data products with their meta information. Domain data products need to register themselves to the catalog.</li> <li><strong>Addressable</strong>: with a unique address accessible programmatically. This implies to define naming convention and may be SDK code.</li> <li><strong>Self describable</strong>: Clear description of the purpose and usage patterns as well as the semantics and syntax. The schema definition and registry are used for that purpose. </li> <li><strong>Trustworthy</strong>: clear definition of the <a href=https://en.wikipedia.org/wiki/Service-level_objective>Service Level Objectives</a> and Service Level Indicators conformance. </li> <li><strong>Native access</strong>: adapt the data access interface to the consumer: APIs, events, SQL views, reports, widgets</li> <li><strong>Composable</strong>: integrate with other data products, for joining, filtering and aggregation. Nedd to define standards for field type formatting, identifying polysemes across different domains, datasets address conventions, common metadata fields, event formats such as CloudEvents. Federated identity may also being used to keep unique identifier cross domain for a business entity.</li> <li><strong>Valuable</strong>: represent a cohesive concept within its domain. Sourced from unstructured, semi-structured and structured data. To maximize value within a data mesh, data products should have narrow, specific definitions, enabling reusable blueprints and efficient management.</li> <li><strong>Secure</strong>: with access control rules and enforcement, and single sign on capability.</li> </ul> <p>To support the implementation of those characteristics, it is relevant to name a <em>domain data product owner</em>, who is also responsible to measure data quality, the decreased lead time of data consumption, and the data user satisfaction, or net promoter score. The most important questions a product owner should be able to answer are:</p> <ol> <li>Who are the data users?</li> <li>How do they use the data?</li> <li>What are the native methods that they are comfortable with to consume the data?</li> </ol> <p>Data products are not data applications, data warehouses, PDF reports, dashboards, tables (without proper metadata), or kafka topics. The data products may, and should be shared using streams, to be able to replay from sources of events and scale the consumption. </p> <h3 id=elements-of-a-data-product>Elements of a Data Product<a class=headerlink href=#elements-of-a-data-product title="Permanent link">&para;</a></h3> <p>The following elements are part of a data product owner to develop and manage, with application developers:</p> <ul> <li>Metadata of what the data product is, human readable, parseable for tool to build and deploy data product to orchestration layer. This includes using naming convention, and polyglot definition. </li> <li>API definition for request-response consumptions</li> <li>Event model definition for asynch consumptions</li> <li>Storage definition, service account, roles and access policies</li> <li>Table definitions</li> <li>Flink statement definitions for deduplication, enrichment, aggregation, and deployment definitions</li> <li>Microservice code implementation, packaging and deployment definitions</li> </ul> <p>All those elements can be defined as code in a git repository or between a gitops repo and a code repository. It is recommended to keep one bounded context per repository.</p> <details class="- info"> <summary>Data lake, lakehouse and data warehouse</summary> <p>Data lake is no more a central piece of the architecture with complex pipelines, they are becoming a node in the data mesh, to expose a dataset. It may not be used as the source of truth is becoming the immutable distributes logs and storage that holds the dataset available for replayability. Datawarehouse for business intelligence is also a node, and consumer of the data product.</p> </details> <h3 id=data-contracts-for-streaming-products>Data Contracts for Streaming Products<a class=headerlink href=#data-contracts-for-streaming-products title="Permanent link">&para;</a></h3> <p>In a Data Mesh, the "Product" is defined by its interface. The industry is moving toward <strong>Data Contracts</strong> as a formal mechanism to define the agreement between a Flink producer and downstream consumers.</p> <p>A Data Contract goes beyond simple schema definition to include:</p> <ul> <li><strong>Schema Definition</strong>: Using Protobuf or Avro with the Confluent Schema Registry as the enforcement mechanism. The schema registry provides version management and compatibility checking.</li> <li><strong>Watermark Alignment Expectations</strong>: The contract should specify the expected watermark strategy (e.g., bounded out-of-orderness with a maximum delay of 5 seconds).</li> <li><strong>Lateness Tolerance</strong>: Define how late-arriving data is handled. This includes specifying the allowed lateness window and what happens to events that arrive after the window closes.</li> <li><strong>Data Quality Guarantees</strong>: Expected completeness, accuracy thresholds, and error handling behavior.</li> </ul> <p>Example contract specification elements for a Flink streaming product:</p> <div class="language-yaml highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=nt>contract</span><span class=p>:</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">customer-events-v1</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=w>  </span><span class=nt>schema</span><span class=p>:</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=w>    </span><span class=nt>format</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">avro</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=w>    </span><span class=nt>registry</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">schema-registry.example.com</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=w>    </span><span class=nt>subject</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">customer-events-value</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=w>    </span><span class=nt>compatibility</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">BACKWARD</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=w>  </span><span class=nt>streaming</span><span class=p>:</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=w>    </span><span class=nt>watermark</span><span class=p>:</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=w>      </span><span class=nt>strategy</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">bounded-out-of-orderness</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=w>      </span><span class=nt>max_delay_seconds</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=w>    </span><span class=nt>lateness</span><span class=p>:</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=w>      </span><span class=nt>allowed_lateness_seconds</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=w>      </span><span class=nt>late_data_handling</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">side_output</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=w>  </span><span class=nt>slo</span><span class=p>:</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a><span class=w>    </span><span class=nt>freshness_seconds</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=w>    </span><span class=nt>completeness_percent</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">99.5</span>
</span></code></pre></div> <p>The contract becomes the primary interface documentation, enabling consumers to understand not just <em>what</em> data they receive, but <em>when</em> and <em>how reliably</em> they can expect to receive it.</p> <h3 id=dual-nature-storage-streaming-and-batch>Dual-Nature Storage: Streaming and Batch<a class=headerlink href=#dual-nature-storage-streaming-and-batch title="Permanent link">&para;</a></h3> <p>Many analytical consumers still require SQL/Batch access while real-time applications need live streams. A Flink-based data product can simultaneously exist as a "Live Stream" (Kafka) and a "Historical Table" (Iceberg/Lakehouse).</p> <p><strong>Apache Iceberg</strong> and <strong>Confluent TableFlow</strong> serve as bridges between these two access patterns:</p> <ul> <li>Flink SQL can write to a unified table that serves both real-time alerts and long-term BI queries.</li> <li>The same data product exposes multiple access interfaces: a Kafka topic for streaming consumers and an Iceberg table for batch analytics.</li> </ul> <div class="language-sql highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1>-- Flink SQL writing to both Kafka and Iceberg</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=k>INSERT</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=n>iceberg_catalog</span><span class=p>.</span><span class=n>db</span><span class=p>.</span><span class=n>customer_events</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>kafka_customer_events</span><span class=p>;</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=c1>-- The Iceberg table is queryable by Spark, Trino, or other batch engines</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=c1>-- while Kafka consumers receive the same events in real-time</span>
</span></code></pre></div> <p>This dual-nature approach fulfills the "Polyglot" requirement of a data product, allowing each consumer to access data in their preferred format without requiring separate data pipelines.</p> <details class="- info"> <summary>TableFlow for Unified Access</summary> <p>Confluent TableFlow provides automatic materialization of Kafka topics into Iceberg tables, maintaining consistency between the streaming and batch representations. This reduces the operational burden of maintaining separate pipelines for different access patterns.</p> </details> <h3 id=methodology>Methodology<a class=headerlink href=#methodology title="Permanent link">&para;</a></h3> <p>Defining, designing and implementing data products follow the same principles as other software development and should start by the end goal and use case. This should solidify clear product objectives. Domain discovery is part of the DDD methodology, and in the data product a domain may be more oriented to source and some to consumers. But use cases and what needs to be created as analytical data should be the main goals of the design and implementation activities. Source domain datasets represent the <strong>facts of the business</strong>. The source domain datasets capture the data that is mapped very closely to what the operational systems of their origin, generate.</p> <p>Consumer domain datasets, on the other hand, are built to serve a tightly coupled group of use cases. Distinct from source domain datasets, they undergo more structural modifications as they process source domain events into aggregated formats optimized for a specific access model.</p> <h4 id=formalize-the-use-cases-user-stories>Formalize the use cases / user stories<a class=headerlink href=#formalize-the-use-cases-user-stories title="Permanent link">&para;</a></h4> <p>The following table illustrates some use cases:</p> <style>
table th:first-of-type {
    width: 60%;
}
table th:nth-of-type(2) {
    width: 40%;
}
</style> <table> <thead> <tr> <th>User Story</th> <th>Data as a Product</th> </tr> </thead> <tbody> <tr> <td>As a <strong>marketing strategist</strong>, I need to provide predictive churn scores and customer segmentation based on behavior and demographics. This will allow me to proactively target at-risk customers with personalized retention campaigns and optimize marketing spend.</td> <td><ul><li>Churn probability scores for each customer.</li><li>Customer segments based on churn risk and value.</li> <li>Key factors influencing churn.</li></ul></td> </tr> <tr> <td>As a <strong>product manager</strong>, I need to visualize key product usage metrics and performance indicators. This will enable me to monitor product adoption, identify usage patterns, and make data-driven decisions for product improvements.</td> <td><ul><li>Active users, feature usage, and conversion rates.</li><li>Historical trends and comparisons of product performance.</li><li>Breakdowns of product usage by customer segment</li><li>Alerts for anomalies or significant changes in product usage</li></ul></td> </tr> <tr> <td>As a <strong>supply chain manager</strong>, I need to get real-time visibility into inventory levels, supplier performance, and delivery timelines. This will help me proactively identify potential disruptions, optimize inventory management, and ensure timely product delivery.</td> <td><ul><li>Real-time inventory levels across all warehouses.</li><li>Supplier performance metrics, such as on-time delivery rates and quality scores.</li><li>Predictive alerts for potential stockouts or delivery delays.</li><li>Visualizations of delivery routes and timelines.</li><li>Historical data that can be used to perform trend analysis, and find bottlenecks.</li></ul></td> </tr> <tr> <td>As a <strong>Consultant Director</strong>, I need to be able to continuously access a holistic view of each consultant, including their skill level, matching resume, current training and skill levels, and certification status, so that I can effectively staff projects, identify skill gaps, plan professional development, and ensure compliance.</td> <td><ul><li>Aggregated and real-time data on consultant skill levels</li> <li>Matching resumes (potentially key skills extracted)</li><li>Current training completions and skill levels derived from training</li><li>Certification statuses</li><li>Visualization of skill gaps by practice area or project type</li></ul></td> </tr> </tbody> </table> <p>Using a classical system context diagram for the supply chain management use case, we may define the high level view of a data product as:</p> <figure> <img alt src=../diagrams/dp_sys_ctx.drawio.png> <figcaption>Data as a product: system context view</figcaption> </figure> <p>The skill analysis use case may define the following data product:</p> <ul> <li> <p>Certification Compliance Tracker:</p> <ul> <li>Data: Real-time status of all consultant certifications, including expiration dates and renewal progress.</li> <li>Value Proposition: Ensures the organization maintains necessary certifications for compliance and client engagements, mitigating potential risks and penalties.</li> <li>Potential Features: Automated alerts for upcoming expirations; reporting on certification coverage by practice area or client; integration with certification management platforms.</li> </ul> </li> </ul> <h4 id=using-bounded-context>Using Bounded Context<a class=headerlink href=#using-bounded-context title="Permanent link">&para;</a></h4> <p>Data as a product is designed with a domain-driven model combined with analytical and operational use cases. </p> <p>The methodology to define data product may be compared to the event-driven microservice adoption. Business, operational application, manages their aggregates but also are responsible to publish the business events, as facts, to share their datasets. Aggregation processing is considered as a service pushing data product to other consumers. The aggregate models make specific queries on other data products and serve the results with SLOs.</p> <p>The design starts by the user input, which are part of a business domain and bounded context. The data may be represented as DDD aggregate with a semantic model. <a href=https://jbcodeforce.github.io/eda-studies/methodology/ddd/#entities-and-value-objects>Entities and Value objects</a> are represented to assess the need to reuse other data product and potentially assess the need for anti-corruption layer. </p> <details class="- info"> <summary>What should be part of a bounded context for data as a product</summary> <p>A Bounded Context should encapsulate everything needed to model and implement a specific business capability or set of related capabilities. This typically includes:</p> <ul> <li><strong>Entities</strong>: Domain objects with identity that persist over time and represent core concepts of the subdomain. Their behavior and attributes are specific to this context.</li> <li><strong>Value Objects</strong>: Immutable objects that describe characteristics of entities. Their meaning is specific to the context.</li> <li><strong>Aggregates</strong>: Clusters of related entities and value objects that are treated as a single unit for data changes. One entity within the aggregate serves as the root and is responsible for maintaining the consistency of the entire aggregate. Transactions should operate on aggregates.</li> <li><strong>Domain Services</strong>: Operations that don't naturally belong to an entity or value object but are still part of the domain logic within this context. They often involve interactions between multiple aggregates or external systems.</li> <li><strong>Domain Events</strong>: Significant occurrences within the domain that the business cares about. They are immutable records of something that has happened and can trigger actions within the same or other bounded contexts.</li> <li><strong>Repositories</strong>: Interfaces for persisting and retrieving aggregates within the bounded context. The actual implementation of the repository might use a specific database technology.</li> <li><strong>Factories</strong>: Objects responsible for creating complex domain objects, often encapsulating complex instantiation logic.</li> <li><strong>Use Cases/Application Services</strong>: (Sometimes considered outside the core domain but within the Bounded Context) These orchestrate interactions between domain objects to fulfill specific user requests or system behaviors. They reside at the application layer and interact with repositories and domain services.</li> <li><strong>Data Transfer Objects (DTOs)</strong>: Objects used to transfer data across boundaries (e.g., between layers or bounded contexts). Their structure is often optimized for transport rather than representing the domain model directly.</li> <li><strong>Infrastructure Concerns</strong>: Code related to persistence, messaging, external service integrations, and UI specific to this bounded context.</li> </ul> </details> <p>In data products, DDD bounded context, translates to defining clear boundaries for the data products, ensuring each product serves a specific business domain. A <code>customer data product</code> and a <code>product inventory data product</code> would be distinct bounded contexts, each with its own data model and terminology.</p> <p>Data pushed to higher consumer are part of the semantic model, and of the event-driven design. Analytics Engineers and Data Modellers building aggregate Data Products know exactly what to collect and what quality metrics to serve. The aggregation may use lean-pull mechanism, focusing on their use case needs only. The data is becoming a product as close to the operational source, so shifting the processing to the left of the architecture. This Shift-Left approach where quality controls, validation, and governance mechanisms are embedded as early in the data lineage map as possible. The consumption patterns are designed as part of the data product, and may include APIs, events, real-time streams or even scheduled batch. </p> <p>Source data domains need to make easily consumable historical snapshots of their datasets available, not just timed events. These snapshots should be aggregated based on a time frame that matches the typical rate of change within their domain.</p> <p>Even though domains now own their datasets instead of a central platform, the essential tasks of cleansing, preparing, aggregating, and serving data persist, as does the use of data pipelines, which are now integrated into domain logic.</p> <figure> <img alt src=../diagrams/bdctx_dp_view.drawio.png> <figcaption>Data product within bounded context</figcaption> </figure> <p>Each domain dataset must establish Service Level Objectives for the quality of the data it provides: timeliness, error rates...</p> <p>Moving from technical data delivery to product thinking requires changes in how organizations approach data management. The data product is decomposed of real-time events exposed on event streams, and aggregated analytical data exposed as serialized files on an object store.</p> <p>New requirements are added to the context of the source semantic model.</p> <h2 id=motivations-for-moving-to-data-stream-processing>Motivations for Moving to Data Stream Processing<a class=headerlink href=#motivations-for-moving-to-data-stream-processing title="Permanent link">&para;</a></h2> <p>The Data integration adoption is evolving with new needs to act on real-time data and reduce batch processing cost and complexity. The following table illustrates the pros and cons of data integration practices for two axes: time to insights and data integity</p> <style>
table th:first-of-type {
    width: 20%;
}
table th:nth-of-type(2) {
    width: 40%;
}
table th:nth-of-type(3) {
    width: 40%;
}
</style> <table> <thead> <tr> <th>Time to insights</th> <th></th> <th>Data integrity</th> </tr> </thead> <tbody> <tr> <td></td> <td><strong>Low</strong></td> <td><strong>High</strong></td> </tr> <tr> <td><strong>High</strong></td> <td><strong>Lakehouse or ELT:</strong> <ul><li>+ Self-service</li><li>- Runaway cost</li><li>- No knowledge of data lost</li><li>- complext data governance</li><li>- data silos.</li></ul></td> <td><strong>Data Stream Platform:</strong> <ul><li>+ RT decision making</li><li>+ Operation and analytics on same platform</li><li>+ Single source of truth</li><li>+ Reduced TCO</li><li>+ Governance</li></ul></td> </tr> <tr> <td><strong>Low</strong></td> <td><strong>Hand coding:</strong> <ul><li>+ customized solution specific to needs.</li><li>- Slow</li><li>- difficult to scale</li><li>- opaque</li><li>- challenging governance.</li></ul></td> <td><strong>ETL:</strong><ul><li>+ Rigorous</li><li>+ data model design</li><li>+ governed</li><li>+ reliable</li><li>- Slow</li><li>- Point to point</li><li>- Difficult to scale.</li></ul></td> </tr> </tbody> </table> <h3 id=assessment-questions>Assessment questions<a class=headerlink href=#assessment-questions title="Permanent link">&para;</a></h3> <p>Try to get an understanding of the data integration requirements by looking at:</p> <ul> <li>Current data systems and data producers to a messaging system like Kafka</li> <li>Development time to develop new streaming logic or ETL job</li> <li>What are the different data landing zones and for what purpose. Review zone ownership.</li> <li>Level of Lakehouse adoption and governance, which technology used (Iceberg?)</li> <li>Is there a data loop back from the data lake to the OLTP?</li> <li>Where data cleaning is done?</li> <li>Is there any micro-batching jobs currently done, at which frequency, for which consuners?</li> <li>What data governance used?</li> <li>How data quality control is done?</li> </ul> <h2 id=migration-context>Migration Context<a class=headerlink href=#migration-context title="Permanent link">&para;</a></h2> <p>A direct "lift and shift" approachwhere batch SQL scripts are converted to Flink statements on a one-to-one basis may not be recommended. Refactoring is essential, as SQL processing often differs significantly when addressing complex and stateful operators, such as joins.</p> <p>Most of the filtering and selection scripts can be ported 1 to 1. While most stateful processing needs to be refactorized and deeply adapted to better manage states and complexity.</p> <p>There is <a href=https://jbcodeforce.github.io/shift_left_utils/ >a repository</a> with tools to process existing Spark/dbt project to find dependencies between tables, use local LLM to do some migration, and create Flink query pipelines per sink table. </p> <h2 id=measuring-data-product-slos-in-flink>Measuring Data Product SLOs in Flink<a class=headerlink href=#measuring-data-product-slos-in-flink title="Permanent link">&para;</a></h2> <p>The "Trustworthy" and "Reliable" pillars of Data Mesh require measurable Service Level Objectives (SLOs). A Data Product Owner should monitor specific metrics for each Flink job that powers their data product.</p> <h3 id=freshness>Freshness<a class=headerlink href=#freshness title="Permanent link">&para;</a></h3> <p>Freshness measures how current the data is. In Flink, this is tracked by comparing the <code>currentEmitWatermark</code> against the wall clock time.</p> <ul> <li><strong>Metric</strong>: <code>currentProcessingTime - currentEmitWatermark</code></li> <li><strong>Target</strong>: Define acceptable lag (e.g., &lt; 30 seconds for near-real-time products)</li> <li><strong>Monitoring</strong>: Expose via Flink metrics and alert when the gap exceeds thresholds</li> </ul> <div class="language-sql highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=c1>-- Flink SQL hint for watermark configuration</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>orders</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=w>    </span><span class=n>order_id</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=w>    </span><span class=n>order_time</span><span class=w> </span><span class=k>TIMESTAMP</span><span class=p>(</span><span class=mi>3</span><span class=p>),</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=w>    </span><span class=n>WATERMARK</span><span class=w> </span><span class=k>FOR</span><span class=w> </span><span class=n>order_time</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>order_time</span><span class=w> </span><span class=o>-</span><span class=w> </span><span class=nb>INTERVAL</span><span class=w> </span><span class=s1>&#39;5&#39;</span><span class=w> </span><span class=k>SECOND</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(...);</span>
</span></code></pre></div> <h3 id=completeness>Completeness<a class=headerlink href=#completeness title="Permanent link">&para;</a></h3> <p>Completeness addresses how the product handles late-arriving data. Flink provides mechanisms to track and report on data that arrives after the allowed lateness window.</p> <ul> <li><strong>Side Outputs for Late Data</strong>: Configure side outputs to capture late events rather than silently dropping them. These become part of the product's quality report.</li> <li><strong>Late Data Metrics</strong>: Track the percentage of events arriving late and the distribution of lateness.</li> </ul> <div class="language-java highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1>// Java DataStream API - capturing late data</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>OutputTag</span><span class=o>&lt;</span><span class=n>Event</span><span class=o>&gt;</span><span class=w> </span><span class=n>lateOutputTag</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>new</span><span class=w> </span><span class=n>OutputTag</span><span class=o>&lt;</span><span class=n>Event</span><span class=o>&gt;</span><span class=p>(</span><span class=s>&quot;late-data&quot;</span><span class=p>){};</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=n>SingleOutputStreamOperator</span><span class=o>&lt;</span><span class=n>Result</span><span class=o>&gt;</span><span class=w> </span><span class=n>result</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>stream</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=w>    </span><span class=p>.</span><span class=na>keyBy</span><span class=p>(</span><span class=n>Event</span><span class=p>::</span><span class=n>getKey</span><span class=p>)</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=w>    </span><span class=p>.</span><span class=na>window</span><span class=p>(</span><span class=n>TumblingEventTimeWindows</span><span class=p>.</span><span class=na>of</span><span class=p>(</span><span class=n>Time</span><span class=p>.</span><span class=na>minutes</span><span class=p>(</span><span class=mi>5</span><span class=p>)))</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=w>    </span><span class=p>.</span><span class=na>allowedLateness</span><span class=p>(</span><span class=n>Time</span><span class=p>.</span><span class=na>minutes</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=w>    </span><span class=p>.</span><span class=na>sideOutputLateData</span><span class=p>(</span><span class=n>lateOutputTag</span><span class=p>)</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=w>    </span><span class=p>.</span><span class=na>process</span><span class=p>(</span><span class=k>new</span><span class=w> </span><span class=n>MyWindowFunction</span><span class=p>());</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=c1>// Late data stream for quality reporting</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=n>DataStream</span><span class=o>&lt;</span><span class=n>Event</span><span class=o>&gt;</span><span class=w> </span><span class=n>lateStream</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>result</span><span class=p>.</span><span class=na>getSideOutput</span><span class=p>(</span><span class=n>lateOutputTag</span><span class=p>);</span>
</span></code></pre></div> <p><a href=https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/side_output/ >See Flink side output mecanism</a> and <a href=https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/learn-flink/streaming_analytics/#late-events>Late events</a>.</p> <h3 id=cost-per-product-finops>Cost-per-Product (FinOps)<a class=headerlink href=#cost-per-product-finops title="Permanent link">&para;</a></h3> <p>Resource tagging enables "Billback" or "Showback" for specific data products.</p> <ul> <li><strong>Resource Labels</strong>: Tag Flink jobs, Kafka topics, and storage with product identifiers.</li> <li><strong>Compute Metrics</strong>: Track CPU, memory, and checkpoint storage per product.</li> <li><strong>Cost Allocation</strong>: Map infrastructure costs to business domains based on resource consumption.</li> </ul> <table> <thead> <tr> <th>Metric</th> <th>Description</th> <th>Target</th> </tr> </thead> <tbody> <tr> <td>Freshness Lag</td> <td>Watermark delay vs wall clock</td> <td>&lt; 30s</td> </tr> <tr> <td>Completeness</td> <td>Percentage of on-time events</td> <td>&gt; 99.5%</td> </tr> <tr> <td>Late Data Rate</td> <td>Events arriving after allowed lateness</td> <td>&lt; 0.1%</td> </tr> <tr> <td>Cost per Million Events</td> <td>Infrastructure cost normalized by throughput</td> <td>Varies by product</td> </tr> </tbody> </table> <h2 id=data-discovery-via-flink-catalogs>Data Discovery via Flink Catalogs<a class=headerlink href=#data-discovery-via-flink-catalogs title="Permanent link">&para;</a></h2> <p>One of the biggest hurdles in Data Mesh is <strong>Discoverability</strong>. If a Flink job creates a view or a table, users in other domains need a mechanism to find it.</p> <h3 id=centralized-catalogs>Centralized Catalogs<a class=headerlink href=#centralized-catalogs title="Permanent link">&para;</a></h3> <p>The Flink Catalog serves as the registration point for data products:</p> <ul> <li><strong>Hive Catalog</strong>: Traditional metadata store, widely supported across batch and streaming engines.</li> <li><strong>AWS Glue Catalog</strong>: Managed catalog service integrated with AWS analytics ecosystem.</li> <li><strong>DataHub Integration</strong>: Connect Flink metadata to a company-wide data discovery portal.</li> </ul> <div class="language-sql highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1>-- Register a catalog in Flink OSS SQL</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=k>CREATE</span><span class=w> </span><span class=k>CATALOG</span><span class=w> </span><span class=n>iceberg_catalog</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=w>    </span><span class=s1>&#39;type&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;iceberg&#39;</span><span class=p>,</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=w>    </span><span class=s1>&#39;catalog-type&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;hive&#39;</span><span class=p>,</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=w>    </span><span class=s1>&#39;uri&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;thrift://hive-metastore:9083&#39;</span><span class=p>,</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=w>    </span><span class=s1>&#39;warehouse&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;s3://data-lake/warehouse&#39;</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a><span class=p>);</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a><span class=c1>-- Create a table that becomes discoverable</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>iceberg_catalog</span><span class=p>.</span><span class=n>sales</span><span class=p>.</span><span class=n>daily_revenue</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a><span class=w>    </span><span class=n>date_key</span><span class=w> </span><span class=nb>DATE</span><span class=p>,</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a><span class=w>    </span><span class=n>region</span><span class=w> </span><span class=n>STRING</span><span class=p>,</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a><span class=w>    </span><span class=n>total_revenue</span><span class=w> </span><span class=nb>DECIMAL</span><span class=p>(</span><span class=mi>18</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(...);</span>
</span></code></pre></div> <h3 id=making-products-discoverable>Making Products Discoverable<a class=headerlink href=#making-products-discoverable title="Permanent link">&para;</a></h3> <p>"Data as a Product" means registering Flink SQL metadata so it is searchable in a company-wide data portal, not hidden in JAR files or deployment scripts.</p> <ul> <li><strong>Automated Registration</strong>: CI/CD pipelines should register table metadata to the catalog upon deployment.</li> <li><strong>Rich Metadata</strong>: Include descriptions, ownership, SLOs, and lineage information.</li> <li><strong>Search and Browse</strong>: Users can find products by domain, data type, or semantic meaning.</li> </ul> <h2 id=versioning-and-schema-evolution>Versioning and Schema Evolution<a class=headerlink href=#versioning-and-schema-evolution title="Permanent link">&para;</a></h2> <p>Data products must remain "Secure" and "Interoperable" even when they change. Versioning a streaming product presents <a href=https://jbcodeforce.github.io/flink-studies/architecture/cookbook/#query-evolution>unique challenges. (see query evolution in cookbook chapter)</a>.</p> <h3 id=state-compatibility-challenges>State Compatibility Challenges<a class=headerlink href=#state-compatibility-challenges title="Permanent link">&para;</a></h3> <p>When the data product logic changes (e.g., a new calculation for "Churn Probability"), the Flink job's state must be considered:</p> <ul> <li><strong>Savepoint Compatibility</strong>: Changes to state schema may require state migration or fresh start.</li> <li><strong>Versioned Output Topics</strong>: Use versioned Kafka topics (e.g., <code>customer-churn-v2</code>) to avoid breaking downstream consumers.</li> <li><strong>Blue-Green Deployment</strong>: Run both old and new versions in parallel during transition periods.</li> </ul> <div class="language-bash highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Blue-Green deployment pattern</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=c1># 1. Deploy new version reading from same source, writing to new topic</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>flink<span class=w> </span>run<span class=w> </span>-s<span class=w> </span>savepoint-path<span class=w> </span>job-v2.jar<span class=w> </span>--output-topic<span class=w> </span>customer-churn-v2
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=c1># 2. Migrate consumers to v2 topic</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=c1># 3. Deprecate and eventually stop v1 job</span>
</span></code></pre></div> <p><a href=https://jbcodeforce.github.io/shift_left_utils/blue_green_deploy/ >See shift_left tool to support blue/green deployment</a></p> <h3 id=migration-strategies>Migration Strategies<a class=headerlink href=#migration-strategies title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Strategy</th> <th>Use Case</th> <th>Trade-offs</th> </tr> </thead> <tbody> <tr> <td>In-place upgrade</td> <td>Compatible schema changes</td> <td>Requires savepoint compatibility</td> </tr> <tr> <td>Blue-Green</td> <td>Breaking changes</td> <td>Double resource cost during transition</td> </tr> <tr> <td>Versioned topics</td> <td>Major logic changes</td> <td>Consumers must migrate</td> </tr> <tr> <td>Parallel processing</td> <td>Gradual rollout</td> <td>Increased complexity</td> </tr> </tbody> </table> <h2 id=some-implementation-challenges>Some implementation challenges<a class=headerlink href=#some-implementation-challenges title="Permanent link">&para;</a></h2> <h3 id=git-project-organization>Git project organization<a class=headerlink href=#git-project-organization title="Permanent link">&para;</a></h3> <p>For closely related Bounded Contexts within the same application we can have a repository with different folders per bounded context.</p> <p>The internal structure of each Bounded Context folder typically follows a layered or modular architecture:</p> <ul> <li>domain/: Contains the core domain logic: entities, value objects, aggregates, domain services, and domain events. This layer should be independent of any infrastructure concerns.</li> <li>application/: Contains use cases or application services that orchestrate interactions with the domain layer to fulfill specific business requirements. It often handles transactions and authorization.</li> <li> <p>infrastructure/: Contains the implementation details for interacting with the outside world:</p> <ul> <li>persistence/: Repository implementations using specific database technologies.</li> <li>messaging/: Implementations for sending and receiving domain events or commands using message queues or kafka topics.</li> <li>external-services/: Clients for interacting with other systems or APIs.</li> </ul> </li> <li> <p>interfaces/ (or api/, web/): Contains the entry points to the Bounded Context, such as REST API controllers, GraphQL resolvers, or UI-specific code. It's responsible for request handling and response formatting, often using DTOs to translate between the interface and the application layer.</p> </li> <li>tests/: Contains unit tests, integration tests, and potentially end-to-end tests for the Bounded Context.</li> <li>shared/ (within a Bounded Context): Might contain utilities or helper classes specific to this Bounded Context.</li> </ul> <p>This structure can be in <code>src/main/java</code> for Java project, or src for python/ Fast API project.</p> <h3 id=joins-considerations>Joins considerations<a class=headerlink href=#joins-considerations title="Permanent link">&para;</a></h3> <p>The SQL, LEFT JOIN, joins records that match and dont match on the condition specified. For non matching record the left columns are populated with NULL. SQL supports LEFT ANTI JOIN, but not Flink. So one solution in Flink SQL is to use a null filter on the left join condition:</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=k>from</span><span class=w> </span><span class=n>table_left</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=k>left</span><span class=w> </span><span class=k>join</span><span class=w> </span><span class=n>table_right</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a><span class=w>    </span><span class=k>on</span><span class=w> </span><span class=n>table_left</span><span class=p>.</span><span class=n>column_used_for_join</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>table_right</span><span class=p>.</span><span class=n>column_used_for_join</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a><span class=w>    </span><span class=k>where</span><span class=w> </span><span class=n>table_right</span><span class=p>.</span><span class=n>column_used_for_join</span><span class=w> </span><span class=k>is</span><span class=w> </span><span class=k>NULL</span><span class=p>;</span>
</span></code></pre></div> <h3 id=state-management-as-a-product-lifecycle-challenge>State Management as a Product Lifecycle Challenge<a class=headerlink href=#state-management-as-a-product-lifecycle-challenge title="Permanent link">&para;</a></h3> <p>In a data product, the Flink <strong>state</strong> (the checkpoint and savepoint) is part of the product's technical debt and asset. "Owning the data" also means "owning the state."</p> <p><strong>State as a Product Asset:</strong></p> <ul> <li><strong>Savepoints are Data</strong>: Savepoints contain aggregated values, window contents, and keyed state that may represent hours or days of processing. They are not disposable artifacts.</li> <li><strong>Recovery Dependency</strong>: The ability to recover a data product depends on savepoint availability and compatibility.</li> <li><strong>Migration Complexity</strong>: State schema evolution requires careful planning and testing.</li> </ul> <p><strong>Operational Considerations:</strong></p> <ul> <li><strong>Savepoint Retention Policy</strong>: Define how long savepoints are retained and archived. Consider compliance and recovery requirements.</li> <li><strong>State Size Monitoring</strong>: Large state can impact checkpoint duration and recovery time. Track state size as a product metric.</li> <li><strong>Compatibility Testing</strong>: Before upgrading job logic, validate that the new code can restore from existing savepoints.</li> </ul> <div class="language-bash highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Taking a savepoint before upgrade</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>flink<span class=w> </span>savepoint<span class=w> </span>&lt;job-id&gt;<span class=w> </span>s3://savepoints/product-name/
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=c1># Validating state compatibility (conceptual)</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>flink<span class=w> </span>run<span class=w> </span>--dry-run<span class=w> </span>--restore-from<span class=w> </span>savepoint-path<span class=w> </span>new-job.jar
</span></code></pre></div> <p><strong>State Ownership Questions:</strong></p> <ol> <li>Who is responsible for savepoint storage and retention?</li> <li>What is the recovery point objective (RPO) for this data product?</li> <li>How is state migration tested before production deployment?</li> <li>What is the fallback strategy if state restoration fails?</li> </ol> <p>Including state management in the data product definition ensures that operational concerns are addressed alongside functional requirements.</p> <h2 id=source-of-information-go-deeper>Source of information - go deeper<a class=headerlink href=#source-of-information-go-deeper title="Permanent link">&para;</a></h2> <ul> <li><a href=https://martinfowler.com/articles/designing-data-products.html>Martin Fowler - Designing Data Product</a></li> <li><a href=https://martinfowler.com/articles/data-mesh-principles.html>Data Mesh Principals - Zhamak Dehghani</a></li> <li><a href=https://martinfowler.com/articles/data-monolith-to-mesh.html>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh - Zhamak Dehghani</a></li> <li><a href=https://www.confluent.io/blog/implementing-streaming-data-products/ >Confluent Blog - Data Products, Data Contracts, and Change Data Capture - Adam Bellemare</a></li> </ul> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../coding/cep/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Complex Event Processing"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Complex Event Processing </div> </div> </a> <a href=../coe/ class="md-footer__link md-footer__link--next" aria-label="Next: Center of Excellence"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Center of Excellence </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2026 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["content.code.annotation", "content.code.copy", "content.tooltips", "content.tabs.link", "search.suggest", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> </body> </html>