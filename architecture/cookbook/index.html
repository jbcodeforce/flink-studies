<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://jeromeboyer.net/flink-studies/architecture/cookbook/ rel=canonical><link rel=icon href=../../images/logo-blue.drawio.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.2"><title>Flink Cookbook - Apache and Confluent Flink Studies</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#flink-cookbook class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Apache and Confluent Flink Studies" class="md-header__button md-logo" aria-label="Apache and Confluent Flink Studies" data-md-component=logo> <img src=../../images/flink-header-logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Apache and Confluent Flink Studies </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Flink Cookbook </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jbcodeforce/flink-studies.git title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Foundations </a> </li> <li class=md-tabs__item> <a href=../../cookbook/ class=md-tabs__link> Cookbook </a> </li> <li class=md-tabs__item> <a href=../../coding/flink-sql-clients/ class=md-tabs__link> Flink_App_Coding </a> </li> <li class=md-tabs__item> <a href=../../methodology/data_as_a_product/ class=md-tabs__link> Methodology </a> </li> <li class=md-tabs__item> <a href=../../techno/ccloud-flink/ class=md-tabs__link> Related_Technologies </a> </li> <li class=md-tabs__item> <a href=https://jbcodeforce.github.io/eda-studies class=md-tabs__link> EDA </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Apache and Confluent Flink Studies" class="md-nav__button md-logo" aria-label="Apache and Confluent Flink Studies" data-md-component=logo> <img src=../../images/flink-header-logo.svg alt=logo> </a> Apache and Confluent Flink Studies </label> <div class=md-nav__source> <a href=https://github.com/jbcodeforce/flink-studies.git title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex=0> <span class=md-ellipsis> Foundations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Foundations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> Flink Key Concepts </span> </a> </li> <li class=md-nav__item> <a href=../../coding/getting-started/ class=md-nav__link> <span class=md-ellipsis> Getting started </span> </a> </li> <li class=md-nav__item> <a href=../../concepts/flink-sql/ class=md-nav__link> <span class=md-ellipsis> Flink SQL concepts </span> </a> </li> <li class=md-nav__item> <a href=../../labs/ class=md-nav__link> <span class=md-ellipsis> Code&Demos </span> </a> </li> <li class=md-nav__item> <a href=../agentic_flink/ class=md-nav__link> <span class=md-ellipsis> Agentic applications </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Cookbook </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Cookbook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../cookbook/ class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/considerations/ class=md-nav__link> <span class=md-ellipsis> Considerations </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/cluster_mgt/ class=md-nav__link> <span class=md-ellipsis> Cluster management </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/job_lifecycle/ class=md-nav__link> <span class=md-ellipsis> Job Lifecycle </span> </a> </li> <li class=md-nav__item> <a href=../../coding/k8s-deploy/ class=md-nav__link> <span class=md-ellipsis> FKO & CMF Deployment </span> </a> </li> <li class=md-nav__item> <a href=../../cookbook/terraform/ class=md-nav__link> <span class=md-ellipsis> Confluent Cloud Terraform </span> </a> </li> <li class=md-nav__item> <a href=../../techno/fk-k8s-monitor/ class=md-nav__link> <span class=md-ellipsis> Monitoring </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Flink_App_Coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Flink_App_Coding </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Flink SQL coding </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Flink SQL coding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/flink-sql-clients/ class=md-nav__link> <span class=md-ellipsis> SQL Clients </span> </a> </li> <li class=md-nav__item> <a href=../../coding/flink-sql-1/ class=md-nav__link> <span class=md-ellipsis> Create Table (SQL) </span> </a> </li> <li class=md-nav__item> <a href=../../coding/flink-sql-2/ class=md-nav__link> <span class=md-ellipsis> SQL DML </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> Java - Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Java - Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/table-api/ class=md-nav__link> <span class=md-ellipsis> Table API </span> </a> </li> <li class=md-nav__item> <a href=../../coding/datastream/ class=md-nav__link> <span class=md-ellipsis> DataStreams API </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../coding/udf_sql/ class=md-nav__link> <span class=md-ellipsis> UDFs & PTFs </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex=0> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/shift_left_utils/ class=md-nav__link> <span class=md-ellipsis> Manage CC Flink projects </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5 id=__nav_3_5_label tabindex=0> <span class=md-ellipsis> More advanced topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_5_label aria-expanded=false> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> More advanced topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../coding/stateful-func/ class=md-nav__link> <span class=md-ellipsis> Stateful function </span> </a> </li> <li class=md-nav__item> <a href=../../coding/cep/ class=md-nav__link> <span class=md-ellipsis> Complex Event Processing </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Methodology </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Methodology </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../methodology/data_as_a_product/ class=md-nav__link> <span class=md-ellipsis> Data as a product </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/eda-studies/methodology/event-storming/ class=md-nav__link> <span class=md-ellipsis> Event Storming </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/shift_left_utils/ class=md-nav__link> <span class=md-ellipsis> Migrate to real-time processing tools </span> </a> </li> <li class=md-nav__item> <a href=../../methodology/coe/ class=md-nav__link> <span class=md-ellipsis> Center of Excellence </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/flink_project_demos/c360/spark_project/ class=md-nav__link> <span class=md-ellipsis> A C360 data product demo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Related_Technologies </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Related_Technologies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../techno/ccloud-flink/ class=md-nav__link> <span class=md-ellipsis> Confluent Cloud Flink </span> </a> </li> <li class=md-nav__item> <a href=../../techno/cp-flink/ class=md-nav__link> <span class=md-ellipsis> Confluent Platform for Flink </span> </a> </li> <li class=md-nav__item> <a href=../kafka/ class=md-nav__link> <span class=md-ellipsis> Kafka Integration </span> </a> </li> <li class=md-nav__item> <a href=../../techno/cc-tableflow/ class=md-nav__link> <span class=md-ellipsis> Confluent TableFlow </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/techno/data/#data-related-technologies class=md-nav__link> <span class=md-ellipsis> Apache Iceberg </span> </a> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/kafka-studies class=md-nav__link> <span class=md-ellipsis> Kafka-studies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=https://jbcodeforce.github.io/eda-studies class=md-nav__link> <span class=md-ellipsis> EDA </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#understand-the-flink-ui class=md-nav__link> <span class=md-ellipsis> Understand the Flink UI </span> </a> </li> <li class=md-nav__item> <a href=#classical-deployment-pattern class=md-nav__link> <span class=md-ellipsis> Classical deployment pattern </span> </a> </li> <li class=md-nav__item> <a href=#exactly-once-delivery class=md-nav__link> <span class=md-ellipsis> Exactly-once-delivery </span> </a> <nav class=md-nav aria-label=Exactly-once-delivery> <ul class=md-nav__list> <li class=md-nav__item> <a href=#flink-context class=md-nav__link> <span class=md-ellipsis> Flink context </span> </a> </li> <li class=md-nav__item> <a href=#end-to-end-solution class=md-nav__link> <span class=md-ellipsis> End-to-end solution </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#troubleshooting class=md-nav__link> <span class=md-ellipsis> Troubleshooting </span> </a> <nav class=md-nav aria-label=Troubleshooting> <ul class=md-nav__list> <li class=md-nav__item> <a href=#a-sql-statement-not-returning-any-result class=md-nav__link> <span class=md-ellipsis> A SQL statement not returning any result </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#high-availability-and-disaster-recovery class=md-nav__link> <span class=md-ellipsis> High Availability and Disaster Recovery </span> </a> </li> <li class=md-nav__item> <a href=#security class=md-nav__link> <span class=md-ellipsis> Security </span> </a> </li> <li class=md-nav__item> <a href=#deduplication class=md-nav__link> <span class=md-ellipsis> Deduplication </span> </a> </li> <li class=md-nav__item> <a href=#change-data-capture class=md-nav__link> <span class=md-ellipsis> Change Data Capture </span> </a> </li> <li class=md-nav__item> <a href=#late-data class=md-nav__link> <span class=md-ellipsis> Late Data </span> </a> </li> <li class=md-nav__item> <a href=#exactly-once class=md-nav__link> <span class=md-ellipsis> Exactly once </span> </a> </li> <li class=md-nav__item> <a href=#query-evolution class=md-nav__link> <span class=md-ellipsis> Query Evolution </span> </a> <nav class=md-nav aria-label="Query Evolution"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#schema-compatibility class=md-nav__link> <span class=md-ellipsis> Schema compatibility </span> </a> </li> <li class=md-nav__item> <a href=#handling-changes-with-cdc class=md-nav__link> <span class=md-ellipsis> Handling Changes with CDC </span> </a> </li> <li class=md-nav__item> <a href=#statement-evolution class=md-nav__link> <span class=md-ellipsis> Statement evolution </span> </a> </li> <li class=md-nav__item> <a href=#flink-on-kubernetes class=md-nav__link> <span class=md-ellipsis> Flink on Kubernetes </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#testing-sql-statement-during-pipeline-development class=md-nav__link> <span class=md-ellipsis> Testing SQL statement during pipeline development </span> </a> </li> <li class=md-nav__item> <a href=#measuring-latency class=md-nav__link> <span class=md-ellipsis> Measuring Latency </span> </a> </li> <li class=md-nav__item> <a href=#other-sources class=md-nav__link> <span class=md-ellipsis> Other sources </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=flink-cookbook>Flink Cookbook<a class=headerlink href=#flink-cookbook title="Permanent link">&para;</a></h1> <details class="- info"> <summary>Chapter updates</summary> <ul> <li>Created 12/2024 </li> <li>Updated 1/28/2025: env, testing and statement life cycle.</li> </ul> </details> <p>There is a <a href=https://github.com/confluentinc/flink-cookbook>Confluent cookbook for best practices</a> to run Flink into production. The content of this page is to get a summary of those practices, enhanced from other customers' engagements. It also references hands-on exercises within this repository. </p> <p>Examples may be run inside from Terminal or using Kubernetes cluster locally, they are on Flink 1.20.1 or Flink 2.1. Use Java 11 or 17, see <a href=https://sdkman.io/ >sdkman</a> to manage different java version. </p> <h2 id=understand-the-flink-ui>Understand the Flink UI<a class=headerlink href=#understand-the-flink-ui title="Permanent link">&para;</a></h2> <p>The Flink Web UI helps to debug misbehaving jobs. </p> <p>The Flink Web UI is well described <a href=https://developer.confluent.io/courses/apache-flink/web-ui-exercise/ >in Confluent David Anderson's article</a>, The <a href>Apache Flink doc for Web UI</a>, and link to the important <a href=https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/explain/ >execution plan understanding with EXPLAIN</a>.</p> <p>With OSS the Web UI is accessible when the <code>start_cluster.sh</code> is started. Local URL is <a href=http://localhost:8081>http://localhost:8081</a>. The Web UI offers the following important features:</p> <ul> <li>Navigating to get the running Jobs, the view is updated periodically. The job graph, which matches the EXPLAIN output, presents the tasks running one or more operators of the DAG.</li> <li>Task metrics are <strong>backpressure, busyness, and data skew</strong>.<ul> <li><strong>backpressure:</strong> percentage of time that the subtask was unable to send output downstream because the downstream subtask had fallen behind, and (temporarily) couldn't receive any more records. <code>Backpressured max</code> is the maximum backpressure across all of the parallel subtasks for a given period.</li> <li><strong>busy</strong> reports percentage of time spent doing useful work, aggregated at the task level for a time period.</li> <li><strong>data skew</strong> measures the degree of variation in the number of records processed per second by each of the parallel subtasks. 100% is max skew.</li> </ul> </li> <li>Examining the history of checkpoints</li> <li>Monitoring for any potential backpressure</li> <li>Analyzing watermarks</li> <li>Retrieving the job logs</li> </ul> <p>Network metrics (Bytes Received / Records Received ) are inside the Flink cluster, not for source and sink to external systems.</p> <p>In Concluent Cloud the Query Profiler has the same capability then the Flink UI and accessible at the Statement View level: <img alt src=../images/query-profiler.png></p> <h2 id=classical-deployment-pattern>Classical deployment pattern<a class=headerlink href=#classical-deployment-pattern title="Permanent link">&para;</a></h2> <p>For Confluent Cloud for Flink we want to map environments to physical environments, like dev, Staging and Production. Any stream processing has a set of source producer that will be part of an ingestion layer. In the Kafka architecture, it will be a set of Kafka Connector cluster, with for example Change Data Capture connector like <a href=https://debezium.io/ >Debezium</a>. Most of the time the dev environment may not have the streams coming from this ingestion layer. </p> <p>For the discussion each business application can have one to many pipelines. A Pipeline is a set of Flink jobs running SQL statements or Table API programs. A generic pattern of a pipeline involves at least, the following steps:</p> <figure> <img alt=1 src=../diagrams/generic_src_to_sink_flow.drawio.png> <figcaption>Figure 1: Generic pipeline structure</figcaption> </figure> <ol> <li>A CDC source connector injects data in Kafka topic. Avro schemas are defined for the Key and the Value.</li> <li>A first set of statements are doing deduplication logic, or filtering to ensure only relevant messages are processed by the pipeline</li> <li>There will be zero to many intermediate tables, depending of the business logic needed for the application. Those intermediate tables/topics may get enrichement, aggregation or joins.</li> <li>The final step is to prepare the data for sink processing. The statements may includes joins and filtering out, even may be some deduplication logic too.</li> <li>The data need to land to sink external system, so Kafka Sink connectors are deployed to write to those systems. Here the example illustrate a datawarehouse system based on Postgreql, on which a business intelligent component will implement adhoc queries and dashboards. </li> </ol> <p>The artifacts for development are the DDL and DML statements and test data.</p> <p>Finally to support the deployment and quality control of those pipelines deployment, the following figures illustrates a classical deployment pattern:</p> <figure> <img alt=2 src=../diagrams/env-architecture.drawio.png width=600> <figcaption>Figure 2: Environment mapping</figcaption> </figure> <ol> <li>Each environment has its own schema registry</li> <li>Once Kafka Cluster per env, with different ACL rules to control who can create topic, read and write.</li> <li>For each application it may be relevant to isolate them in their own Flink Compute pool</li> <li>CI/CD can define infrastructure as code for the Flink Compute pool, the Kafka Cluster, the Kafka Connector cluster and connectors configuration, the input topics, the ACLs, the schema registry.</li> </ol> <p>This architecture helps to clearly separate schema management per environment, and help to promote real-time processing pipelines from dev to staging to production in a control manner using a GitOps approach.</p> <details class=info open=open> <summary>Gitops</summary> <p>The core concept of <a href=https://opengitops.dev/ >GitOps</a> is to maintain a single Git repository that consistently holds declarative descriptions of the desired infrastructure in the production environment. An automated process ensures that the production environment aligns with the state described in the repository. The methodology and tools support changing infrastructure using feature branches, PR, PR review, </p> </details> <h2 id=exactly-once-delivery>Exactly-once-delivery<a class=headerlink href=#exactly-once-delivery title="Permanent link">&para;</a></h2> <p>Flink's internal exactly-once guarantee is robust, but for the results to be accurate in the external system, that system (the sink) must cooperate.</p> <p>This is a complex subject to address and context is important on how to assess exactly-once-delivery: within Flink processing, versus with an end-to-end solution context. </p> <h3 id=flink-context>Flink context<a class=headerlink href=#flink-context title="Permanent link">&para;</a></h3> <p>For Flink, "Each incoming event affects the final Flink statement results exactly once." as said <a href="https://www.youtube.com/watch?v=rh7wdvZXTOo">Piotr Nowojski during his presentation at the Flink Forward 2017 conference</a>. No data duplication and no data loss. Flink achieves it through a combination of checkpointing, state management, and transactional sinks. Checkpoints save the state of the stream processing application at regular intervals. State management maintains the consistency of data between the checkpoints. Transactional sinks ensure that data gets written out exactly once, even during failures </p> <p>Flink uses transactions when writing messages into Kafka. Kafka messages are only visible when the transaction is actually committed as part of a Flink checkpoint. <code>read_committed</code> consumers will only get the committed messages. <code>read_uncommitted</code> consumers see all messages.</p> <p>Below is an example of creating Flink Table in Confluent Cloud with reading committed only message (opposite property will be: 'kafka.consumer.isolation-level'='read-uncommitted'):</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>exactly_once</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=w>  </span><span class=k>WITH</span><span class=p>(</span><span class=s1>&#39;kafka.consumer.isolation-level&#39;</span><span class=o>=</span><span class=s1>&#39;read-committed&#39;</span><span class=p>)</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=w>  </span><span class=k>AS</span><span class=w> </span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=o>`</span><span class=n>transactions</span><span class=o>`</span><span class=p>..</span>
</span></code></pre></div> <p>As the default checkpoint interval is set to 60 seconds, <code>read_committed</code> consumers will see up to one minute latency: a Kafka message sent just before the commit will have few second latency, while older messages will be above 60 seconds.</p> <p>When multiple Flink statements are chained in a pipeline, the latency may be even bigger, as Flink Kafka source connector uses <code>read_committed</code> isolation.</p> <p>The checkpoints frequency can be updated but could not go below 10s. Shorter interval improves fault tolerance, but adds persistence and performance overhead.</p> <h3 id=end-to-end-solution>End-to-end solution<a class=headerlink href=#end-to-end-solution title="Permanent link">&para;</a></h3> <p>On the sink side, Flink has a <a href=https://nightlies.apache.org/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html>2 phase commit sink function</a> on specific data sources, which includes Kafka, message queue and JDBC. </p> <p>For stream processing requiring an <strong>upsert</strong> capability (insert new records or update existing ones based on a key), the approach is to assess:</p> <ul> <li>if the sink kafka connector support upsert operations: it emits only the latest state for each key, and a tombstone message for delete (which is crucial for Kafka's log compaction to work correctly).</li> <li>For Datsabase, be sure to use a JDBC connector, with upsert support. Achieving exactly-once to a traditional database is done by leveraging the sink's implementation of Flink's Two-Phase Commit protocol. The database's transactions must be compatible with this to make sure writes are only committed when a Flink checkpoint successfully completes.</li> <li>For Lakehouse Sink</li> </ul> <p>The external system must provide support for transactions that integrates with a two-phase commit protocol. </p> <p>When using transactions on sink side, there is a pre-commit phase which starts from the checkpointing: the Job Manager injects a checkpoint barrier to seperate streamed in records before or after the barrier. As the barrier flows to the operators, each one of them, takes a snapshot or their state. The sink operators that support transactions, need to start the transaction in the precommit phase while saving its state to the state backend. After a successful pre-commit phase, the commit must guarantee the success for all operators. In case of any failure, the tx is aborted and rolled back.</p> <ul> <li><a href=https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/ >Article: An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a>.</li> <li><a href=https://docs.confluent.io/cloud/current/flink/concepts/delivery-guarantees.html>Confluent documentation</a>.</li> <li><a href=https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#isolation-level>Confluent Platform - Kafka consumer isolation level property.</a></li> </ul> <h2 id=troubleshooting>Troubleshooting<a class=headerlink href=#troubleshooting title="Permanent link">&para;</a></h2> <h3 id=a-sql-statement-not-returning-any-result>A SQL statement not returning any result<a class=headerlink href=#a-sql-statement-not-returning-any-result title="Permanent link">&para;</a></h3> <p>This could be linked to multiple reasons so verify the following:</p> <ul> <li>Verify there is no exception in the statement itself</li> <li>Query logic being too restrictive or the joins may not match any records. </li> <li>For aggregation, assess if the field used get null values.</li> <li>Source table may be empty, or it consumes the table from a different starting offset (specified via <code>scan.bounded.mode</code>) then expected.</li> <li>Use <code>show create table &lt;table_name&gt;</code> to assess the starting offset strategy or specific values</li> <li>Count all records in a table using <code>SELECT COUNT(*) FROM table_name;</code>, it should be greater then 0.</li> <li>When the statement uses event-time based operation like <code>windowing, top N, OVER, MATCH_RECOGNIZE</code> and temporal joins then verify the watermarks. The following example is from Confluent Cloud for Flink query using the event time from the record, and it should return result. Check if you have produced a minimum of records per Kafka partition, or if the producer has stopped producing data all together.</li> </ul> <div class="language-sql highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=k>SELECT</span><span class=w> </span><span class=n>ROW_NUMBER</span><span class=p>()</span><span class=w> </span><span class=n>OVER</span><span class=w> </span><span class=p>(</span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=err>$</span><span class=n>rowtime</span><span class=w> </span><span class=k>ASC</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=nb>number</span><span class=p>,</span><span class=w> </span><span class=o>*</span><span class=w>   </span><span class=k>FROM</span><span class=w> </span><span class=o>&lt;</span><span class=k>table_name</span><span class=o>&gt;</span>
</span></code></pre></div> <ul> <li>When Data are in topic but not seen by flink <code>select * from &lt;table_name&gt;</code> statement, it may be due to idle partitions and the way watermarks advance and are propagated. Flink automatically marks a Kafka partition as idle if no events come within <code>sql.tables.scan.idle-timeout</code> duration. When a partition is marked as idle, it does not contribute to the watermark calculation until a new event arrives. Try to set the idle timeout for table scans to ensure that Flink considers partitions idle after a certain period of inactivity. Try to create a table with a watermark definition to handle idle partitions and ensure that watermarks advance correctly.</li> </ul> <h2 id=high-availability-and-disaster-recovery>High Availability and Disaster Recovery<a class=headerlink href=#high-availability-and-disaster-recovery title="Permanent link">&para;</a></h2> <p>In Flink high availability goal is to keep the application running and being able to process data. The focus is more on streaming applications then batch. JobManager is a single point of failure but can be configured with standby JobManagers. The coordination can be done with Zookeeper for self managed deployment or via Kubernetes operator.</p> <ul> <li>For <a href=http://localhost:8000/flink-studies/techno/ccloud-flink/#disaster-recovery>Confluent Cloud for Flink see the DR section</a>.</li> </ul> <h2 id=security>Security<a class=headerlink href=#security title="Permanent link">&para;</a></h2> <p>TO BE DONE</p> <h2 id=deduplication>Deduplication<a class=headerlink href=#deduplication title="Permanent link">&para;</a></h2> <p>Deduplication is documented <a href=../../coding/flink-sql-1/#table-creation-how-tos>here</a> and <a href=https://docs.confluent.io/cloud/current/flink/reference/queries/deduplication.html#flink-sql-deduplication>here</a> and at its core principal, it uses a CTE to add a row number, as a unique sequential number to each row. The columns used to de-duplicate are defined in the partitioning. Ordering is using a timestamp to keep the last record or first record. Flink support only ordering on time.</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=k>SELECT</span><span class=w> </span><span class=p>[</span><span class=n>column_list</span><span class=p>]</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=k>FROM</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=w>   </span><span class=k>SELECT</span><span class=w> </span><span class=p>[</span><span class=n>column_list</span><span class=p>],</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=w>     </span><span class=n>ROW_NUMBER</span><span class=p>()</span><span class=w> </span><span class=n>OVER</span><span class=w> </span><span class=p>([</span><span class=n>PARTITION</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>column1</span><span class=p>[,</span><span class=w> </span><span class=n>column2</span><span class=p>...]]</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=w>       </span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>time_attr</span><span class=w> </span><span class=p>[</span><span class=k>asc</span><span class=o>|</span><span class=k>desc</span><span class=p>])</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>rownum</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=w>   </span><span class=k>FROM</span><span class=w> </span><span class=k>table_name</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=p>)</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=n>rownum</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>1</span>
</span></code></pre></div> <p>When using Kafka Topic to persist Flink table, it is possible to use the <code>upsert</code> or <code>retract</code> change log mode, and define the primary key(s) to remove duplicate records as only the last records will be used:</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>orders_deduped</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=w>  </span><span class=k>PRIMARY</span><span class=w> </span><span class=k>KEY</span><span class=p>(</span><span class=w> </span><span class=n>order_id</span><span class=p>,</span><span class=w> </span><span class=n>member_id</span><span class=p>)</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=n>ENFORCED</span><span class=p>)</span><span class=w> </span><span class=n>DISTRIBUTED</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=p>(</span><span class=n>order_id</span><span class=p>,</span><span class=w> </span><span class=n>member_id</span><span class=p>)</span><span class=w> </span><span class=k>INTO</span><span class=w> </span><span class=mi>1</span><span class=w> </span><span class=n>BUCKETS</span><span class=w> </span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=k>WITH</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=w>  </span><span class=s1>&#39;changelog.mode&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;upsert&#39;</span><span class=p>,</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=w>  </span><span class=s1>&#39;value.fields-include&#39;</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;all&#39;</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=p>)</span><span class=w> </span><span class=k>AS</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=k>SELECT</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=w>  </span><span class=o>*</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=k>FROM</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=w>  </span><span class=k>SELECT</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=w>      </span><span class=o>*</span><span class=p>,</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=w>      </span><span class=n>ROW_NUMBER</span><span class=p>()</span><span class=w> </span><span class=n>OVER</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=w>        </span><span class=n>PARTITION</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=o>`</span><span class=n>order_id</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=o>`</span><span class=n>member_id</span><span class=o>`</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a><span class=w>        </span><span class=k>ORDER</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=w>          </span><span class=k>BY</span><span class=w> </span><span class=err>$</span><span class=n>rowtime</span><span class=w> </span><span class=k>DESC</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a><span class=w>      </span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>row_num</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a><span class=w>    </span><span class=k>FROM</span><span class=w> </span><span class=n>orders_raw</span>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a><span class=p>)</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=n>row_num</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>1</span><span class=p>;</span>
</span></code></pre></div> <p>To validate there is no duplicate records in the output table, use a query with tumble window like:</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=w>  </span><span class=k>SELECT</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=w>      </span><span class=o>`</span><span class=n>order_id</span><span class=o>`</span><span class=p>,</span><span class=w> </span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=w>      </span><span class=o>`</span><span class=n>user_id</span><span class=o>`</span><span class=p>,</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=w>    </span><span class=k>COUNT</span><span class=p>(</span><span class=o>*</span><span class=p>)</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=n>cnt</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=w>    </span><span class=k>FROM</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=w>    </span><span class=k>TABLE</span><span class=p>(</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a><span class=w>        </span><span class=n>tumble</span><span class=p>(</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a><span class=w>        </span><span class=k>TABLE</span><span class=w> </span><span class=n>orders_raw</span><span class=p>,</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a><span class=w>        </span><span class=k>DESCRIPTOR</span><span class=p>(</span><span class=err>$</span><span class=n>rowtime</span><span class=p>),</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a><span class=w>        </span><span class=nb>INTERVAL</span><span class=w> </span><span class=s1>&#39;1&#39;</span><span class=w> </span><span class=k>MINUTE</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a><span class=w>        </span><span class=p>)</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a><span class=w>    </span><span class=p>)</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a><span class=w>    </span><span class=k>GROUP</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a><span class=w>    </span><span class=k>BY</span><span class=w>  </span><span class=o>`</span><span class=n>order_id</span><span class=o>`</span><span class=p>,</span><span class=w> </span><span class=o>`</span><span class=n>user_id</span><span class=o>`</span><span class=w> </span><span class=k>HAVING</span><span class=w> </span><span class=k>COUNT</span><span class=p>(</span><span class=o>*</span><span class=p>)</span><span class=w> </span><span class=o>&gt;</span><span class=w> </span><span class=mi>1</span><span class=p>;</span>
</span></code></pre></div> <p>Duplicates may still occur on the Sink side of the pipeline, as it is linked to the type of connector used and its configuration, for example reading un-committed offset. Few connector support the retract semantic.</p> <h2 id=change-data-capture>Change Data Capture<a class=headerlink href=#change-data-capture title="Permanent link">&para;</a></h2> <p>TO BE DONE</p> <h2 id=late-data>Late Data<a class=headerlink href=#late-data title="Permanent link">&para;</a></h2> <p>TO BE DONE</p> <h2 id=exactly-once>Exactly once<a class=headerlink href=#exactly-once title="Permanent link">&para;</a></h2> <h2 id=query-evolution>Query Evolution<a class=headerlink href=#query-evolution title="Permanent link">&para;</a></h2> <p>In this section, I address streaming architecture only, integrated with Kafka, most likely CDC tables and sink connectors, a classical architecture, simplified in this figure:</p> <figure style="width=900"> <img alt src=../images/simple_arch.drawio.png> </figure> <p>Once a Flink query is deployed and run 'forever', how to change it? to fix issue or adapt to schema changes. </p> <p>By principles any Flink DAG code is immutable, so statement needs to be stopped and a new version started! This is not as simple as this, as there will be impact to any consumers of the created data, specially in Kafka topic or in non-idempotent consumers. </p> <p>The Flink SQL statements has limited parts that are mutables. See the <a href=https://docs.confluent.io/cloud/current/flink/concepts/schema-statement-evolution.html>Confluent Cloud product documentation for details</a>. In Confluent Cloud the principal name and compute pool metadata are mutable when stopping and resuming the statement. Developers may stop and resume a statement using Console, CLI, API or even Terraform scripts.</p> <p>Here are example using confluent cli:</p> <div class="language-sh highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># $1 is the statement name</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>confluent<span class=w> </span>flink<span class=w> </span>statement<span class=w> </span>stop<span class=w> </span><span class=nv>$1</span><span class=w> </span>--cloud<span class=w> </span><span class=k>$(</span>CLOUD<span class=k>)</span><span class=w> </span>--region<span class=w> </span><span class=k>$(</span>REGION<span class=k>)</span><span class=w> </span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>confluent<span class=w> </span>flink<span class=w> </span>statement<span class=w> </span>resume<span class=w> </span><span class=nv>$1</span><span class=w> </span>--cloud<span class=w> </span><span class=k>$(</span>CLOUD<span class=k>)</span><span class=w> </span>--region<span class=w> </span><span class=k>$(</span>REGION<span class=k>)</span><span class=w> </span>
</span></code></pre></div> <p>When a SQL statement is started, it reads the source tables from the beginning (or any specified offset) and the operators, defined in the statement, build their internal state. </p> <p>Source operators use the latest schema version for key and value at the time of deployment. There is a snapshot of the different dependency configuration saved for the statement: the reference to the dependants tables, user-defined functions... Any modifications to these objects are not propagated to running statement, which means that:</p> <ul> <li>A change to the schema of the source topic is not picked up by the running statement that references the topic.</li> <li>Same applies to other objects in the catalog like watermark, UDF etc.</li> </ul> <p>First let review the schema definition evolution best practices for Flink processing.</p> <h3 id=schema-compatibility>Schema compatibility<a class=headerlink href=#schema-compatibility title="Permanent link">&para;</a></h3> <p>Flink works best when consuming topics with FULL_TRANSITIVE compatibility mode, which allows addition or deletion of fields with default values only. Fields added are ignored by the running Flink Statement</p> <p>The following table lists the schema compatibility types, with the Flink impacts:</p> <table> <thead> <tr> <th>Compatibility type</th> <th>Change allowed</th> <th>Flink impact</th> </tr> </thead> <tbody> <tr> <td>BACKWARD</td> <td>Delete fields, add optional field</td> <td>Does not allow to replay from earliest</td> </tr> <tr> <td>BACKWARD_TRANSITIVE</td> <td>Delete fields, add optional fields with default value.</td> <td>Require all Statements reading from impacted topic to be updated prior to the schema change.</td> </tr> <tr> <td>FORWARD</td> <td>Add fields, delete optional fields</td> <td>Does not allow to replay from earliest</td> </tr> <tr> <td>FORWARD_TRANSITIVE</td> <td>Add fields, delete optional fields</td> <td>Does not allow to replay from earliest</td> </tr> <tr> <td>FULL</td> <td>Add optional fields, delete optional fields</td> <td>Does not allow to replay from earliest</td> </tr> <tr> <td>FULL_TRANSITIVE</td> <td>Add optional fields, delete optional fields</td> <td>Reprocessing and bootstrapping is always possible: Statements do not need to be updated prior to compatible schema changes. Compatibility rules &amp; migration rules can be used for incompatible changes.</td> </tr> <tr> <td>NONE</td> <td>All changes accepted</td> <td>Very risky. Does not allow to replay from earliest</td> </tr> </tbody> </table> <p>With FULL_TRANSITIVE, old data can be read with the new schema, and new data can also be read between the schemas X, X-1, and X-2. Which means in a pipeline, downstream Flink statements can read newly created schema, and will be able to replay messages from a previous schemas. Therefore, you can upgrade the producers and consumers independently.</p> <p>Confluent Schema Registry supports Avro, Json or Protobuf, but Avro was designed to support schema evolution, so this is the preferred serialization mechanism to use.</p> <h3 id=handling-changes-with-cdc>Handling Changes with CDC<a class=headerlink href=#handling-changes-with-cdc title="Permanent link">&para;</a></h3> <p>The CDC component may create schema automatically reflecting the source table. <a href=https://debezium.io/documentation/reference/stable/connectors/mysql.html>See Debezium documentation about schema evolution</a>.</p> <p>Developers may only delete optional columns. During the migration, all current source tables coming as outcome of the Debezium process are not deletable. If a NOT NULL constraint is added to a column, older records with no value will violate the constraint. To handle such situation, there is a way to configure CDC connector (Debezium) to use the <code>envelop structure</code> which uses <code>key, before, after, _ts_ms, op</code> fields. This will result in a standard schema for Flink tables, which allows the source table to evolve by adding / dropping both NULL &amp; NON NULL COLUMNS. </p> <p>The Debezium Envelope pattern offers the most flexibility for upstream Schema Evolution without impacting downstream Flink Statements.</p> <ul> <li>Adding Columns do not break the connectors or the RUNNING DML Statements. But if developers need to access new field, the JSON_* built-in functions may be used, so a new Flink statement version is needed. </li> </ul> <details class=danger open=open> <summary>Key schema evolution</summary> <p>It is discouraged from any changes to the key schema in order to do not adversely affect partitioning for the new topic/ table.</p> </details> <details class=warning open=open> <summary>Updating schema in schema registry</summary> <p>Adding a field directly in the avro-schema with default value, will be visible in the next command like: <code>show create table &lt;table_name&gt;</code>, as tables in flink are virtuals, and the Confluent Cloud schema definition comes from the Schema Registry. The RUNNING Flink DML is not aware of the new added column. Even STOP &amp; RESUME of the Flink DML is not going to pick the new column neither. </p> <p>Only new deployed statement will see the new schema. For column drop, Debezium connector will drop the new column and register a schema version for the topic (if the alteration resulted in a schema that doesnt match with previous versions). Same as above runnning statements will go into <code>degraded</code> mode.</p> </details> <details class=warning open=open> <summary>Migrate a NULL column to NOT NULL with retro update of rows with a default value</summary> <p>Most likely such operation will result in a connector failure as you are adding a NOT NULL column without a default value, even if you retro updated all the rows. </p> <p>If you have control of the change do the following <div class="language-sql highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=k>UPDATE</span><span class=w> </span><span class=n>Customers</span><span class=w> </span><span class=k>set</span><span class=w> </span><span class=n>zipcode</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>Customers</span><span class=w> </span><span class=k>alter</span><span class=w> </span><span class=k>column</span><span class=w> </span><span class=n>zipcode</span><span class=w> </span><span class=nb>INT</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>NULL</span><span class=w> </span><span class=k>CONSTRAINT</span><span class=w> </span><span class=n>add_zip</span><span class=w> </span><span class=k>DEFAULT</span><span class=w> </span><span class=mi>0</span><span class=p>;</span>
</span></code></pre></div></p> <p>If the upstream has already updated without default value, change the compatibility mode to NONE for the connector to recover and then: <div class="language-sql highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>Customers</span><span class=w> </span><span class=k>ADD</span><span class=w> </span><span class=k>CONSTRAINT</span><span class=w> </span><span class=n>add_zip</span><span class=w> </span><span class=k>DEFAULT</span><span class=w> </span><span class=mi>0</span><span class=p>;</span>
</span></code></pre></div></p> </details> <details class=info open=open> <summary>Drop a NULL column</summary> <p><div class="language-sql highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>Customers</span><span class=w> </span><span class=k>DROP</span><span class=w> </span><span class=k>CONSTRAINT</span><span class=w> </span><span class=n>add_zip</span><span class=p>;</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>Customers</span><span class=w> </span><span class=k>DROP</span><span class=w> </span><span class=k>COLUMN</span><span class=w> </span><span class=n>zipcode</span>
</span></code></pre></div> Running Flink DML is not aware of the dropped <code>zipcode</code> column. User may STOP &amp; RESUME the Flink DML with no impact. But the statement will go to DEGRADED mode if the dropped column is part of the SELECT statement. For new deployment, user may update the statement to remove the dropped column and start from the beginning or specific offsets.</p> </details> <h3 id=statement-evolution>Statement evolution<a class=headerlink href=#statement-evolution title="Permanent link">&para;</a></h3> <p>The general strategy for query evolution is to replace the existing statement and the corresponding tables it maintains with a new statement and new tables. The process is described in <a href=https://docs.confluent.io/cloud/current/flink/concepts/schema-statement-evolution.html#query-evolution>this product chapter</a> and should be viewed within two folds depending of stateless or statefulness of the statement. The initial state of the process involves a pipeline of Flink SQL statements and consumers processing Kafka records from various topics. We assume that the blue records are processed end-to-end, and the upgrade process begins at a specific point, from where all green records to be processed. The migration should start with the Flink statement that needs modification and proceed step by step to the statement creating the sink.</p> <p>For <ins>stateless statement evolution</ins> the figure below illustrates the process:</p> <p><img alt src=../diagrams/stateless_evolution.drawio.png></p> <p><strong>Figure: Stateless schema evolution process</strong></p> <ol> <li>The blue statement, version 1, is stopped, and from the output, developer gets the last offset</li> <li>The green statement includes a DDL to create the new table with schema v2.</li> <li>The green DML statement, version 2, is started from the last offset or timestamp and produces records to the new table/topic</li> <li>Consumers are stopped. They have produced so far records to their own output topic with source from blue records.</li> <li>Consumers restarted with the new table name. Which means changing their own SQL statements, but now producing output with green records as source.</li> </ol> <p>At a high level, stateless statements can be updated by stopping the old statement, creating a new one, and transferring the offsets from the old statement. As mentioned, we need to support FULL TRANSITIVE updates to add or delete optional columns/fields.</p> <p>For <ins>stateful statements</ins>, it is necessary to bootstrap from history, which the below process accomplishes:</p> <p><img alt src=../diagrams/stateful_evolution.drawio.png></p> <p><strong>Figure: Stateful schema evolution process</strong></p> <p>The migration process consists of the following steps:</p> <ol> <li>Create a DDL statement to define the new schema for <code>table_v2</code> which means topic v2.</li> <li>Deploy the new statement with the v2 name, starting from the earliest records to ensure semantic consistency for the blue records, when they are stateful, or from the last offset when stateless.</li> <li>Once the new statement is running, it will build its own state and continue processing new records. Wait for it to retrieve the latest messages from the source tables before migrating existing consumers to the new table v2. The old blue records will be re-emitted. While the new statement </li> <li>Stop processing first statement.</li> <li>Halt any downstream consumers, retaining their offsets if those are stateless or idempotent, if they are stateful they will process from the earliest.</li> <li>Reconnect the consumers to the new table. For Flink statements acting as consumers, they will need to manage their own upgrades. To avoid duplicates or not missing records, the offset for the consumers to the new topic needs ot be carrefuly selected.</li> <li>Once all consumers have migrated to the new output topic, the original statement and output topic can be deprovisioned.</li> </ol> <p>This process requires to centrally control the deployment of those pipeline.</p> <p>Using Open Source Flink, creating a snapshot is one potential solution for restarting. However, if the DML logic has changed, it may not be possible to rebuild the DAG and the state for a significantly altered flow. Thus, in a managed service, the approach is to avoid using snapshots to restart the statements.</p> <p>Also when the state size is too large, consider separating the new statement into a different compute pool than the older one.</p> <h4 id=restart-a-statement-from-a-specific-time-or-offset>Restart a statement from a specific time or offset<a class=headerlink href=#restart-a-statement-from-a-specific-time-or-offset title="Permanent link">&para;</a></h4> <p>Using time window, it may be relevant to restart from the beginning of the time window when the job was terminated. Which means using a <code>WHERE event_time &gt; window_start_time_stamp</code> to get the records from the source table for an aligned time.</p> <p>Use the <code>/*+ options</code> at the table level or a <code>set statement</code> for all the table to read from.</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=k>FROM</span><span class=w> </span><span class=n>orders</span><span class=w> </span><span class=cm>/*+ OPTIONS(&#39;scan.startup.mode&#39; = &#39;timestamp&#39;, &#39;scan.startup.timestamp-millis&#39; = &#39;1717763226336&#39;) */</span>
</span></code></pre></div> <p>For offset, the <code>status.latest_offsets</code> includes the lastest offset read for each partition. Note that reading from an offset is applicable only for stateless statements to ensure exactly-once delivery. or</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=k>SET</span><span class=w> </span><span class=o>`</span><span class=k>sql</span><span class=p>.</span><span class=n>tables</span><span class=p>.</span><span class=n>scan</span><span class=p>.</span><span class=n>startup</span><span class=p>.</span><span class=k>mode</span><span class=o>`=</span><span class=w> </span><span class=ss>&quot;earliest&quot;</span>
</span></code></pre></div> <h4 id=materialized-table>Materialized Table<a class=headerlink href=#materialized-table title="Permanent link">&para;</a></h4> <p><a href=https://cwiki.apache.org/confluence/display/FLINK/FLIP-435%3A+Introduce+a+New+Materialized+Table+for+Simplifying+Data+Pipelines>FLIP 435</a> presents a new table construct to simplify streaming and batch pipelines. Users can define a data transformation with a single declarative</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=k>CREATE</span><span class=w> </span><span class=n>MATERIALIZED</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=p>...</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=k>SELECT</span><span class=w> </span><span class=p>...</span><span class=w> </span>
</span></code></pre></div> <p>statement and specify a desired FRESHNESS interval. Flink then automatically creates and manages the underlying data refresh pipeline, intelligently choosing between a continuous streaming job for low-latency updates or a scheduled batch job for less frequent refreshes. This eliminates the need for separate codebases, manual job orchestration, and complex parameter tuning. The entire lifecycle of the data pipeline, including pausing, resuming, manual backfills, and altering data freshness, can be managed through simple SQL commands, allowing developers to focus on business logic rather than the complexities of stream and batch execution.</p> <p>See the <a href=https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/materialized-table/overview/ >Flink 2.x documentation</a>.</p> <ul> <li>It supports the FRESHNESS keyword, to define the maximum amount of time that the materialized tables content should lag behind updates to the base tables.</li> <li>By default the CONTINUOUS mode is a 3 minutes: so the refresh pipeline is a streaming job with a checkpoint interval of 3 minutes. For FULL mode, the pipeline is a scheduled job executed every 1 hour. <div class="language-sql highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=k>CREATE</span><span class=w> </span><span class=n>MATERIALIZED</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>my_materialized_table_full</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a><span class=w>  </span><span class=n>REFRESH_MODE</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>FULL</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=w>  </span><span class=k>AS</span><span class=w> </span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>source_table</span><span class=p>;</span>
</span></code></pre></div></li> <li>Alter materialized table allows developer to suspend and resume refresh pipeline. When suspending a table in CONTINUOUS mode, the job will be paused using STOP WITH SAVEPOINT by default.</li> <li>Altering an existing materialized table, schema evolution currently only supports adding nullable columns to the end of the original materialized tables schema.</li> <li>Materialized Tables must be created through SQL Gateway,</li> </ul> <hr> <p>TO CONTINUE</p> <h4 id=change-stateful-statement>Change stateful statement<a class=headerlink href=#change-stateful-statement title="Permanent link">&para;</a></h4> <p>We have seen the <a href=../../concepts/#state-management>stateful processing</a> leverages checkpoints and savepoints. With the open source Flink, developers need to enable checkpointing and manually triggering a savepoint when they need to restart from a specific point in time.</p> <p>Deploy the new statement to compute the stateful operation, and use a template like the following. Then stop the first statement.</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=k>create</span><span class=w> </span><span class=k>table</span><span class=w> </span><span class=n>table_v2</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=k>as</span><span class=w> </span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=k>select</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>from</span><span class=w> </span><span class=n>table_v1</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a><span class=k>where</span><span class=w> </span><span class=n>window_time</span><span class=w> </span><span class=o>&lt;=</span><span class=w> </span><span class=n>a_time_stamp_when_stopped</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a><span class=k>union</span><span class=w> </span>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a><span class=k>select</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>from</span><span class=w> </span><span class=p>(</span><span class=n>tumble</span><span class=w> </span><span class=k>table</span><span class=p>)</span>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=k>where</span><span class=w> </span><span class=n>the</span><span class=w> </span><span class=err>$</span><span class=n>rowtime</span><span class=w> </span><span class=o>&gt;</span><span class=w> </span><span class=n>a_time_stamp_when_stopped</span>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a><span class=k>order</span><span class=w> </span><span class=k>by</span><span class=w> </span><span class=n>window_time</span>
</span></code></pre></div> <p>It is possible to do an in-place upgrade if the table uses primary key.</p> <p>Restarting a job is not a retry mechanism but a fault tolerance one. Normally only cluster level issues should ever cause a job to restart. When doing Java or Python application, developers need to do not throw exceptions within the main function but handle them and perform retries, backoff, and loop forever. as part of the exception management it is important to provide diagnostic data to administrators.</p> <p>When integrated with Kafka, networking latency may trigger losing connection, or some user errors like deleting the cluster, a topic, or a connector, may lead the Flink job getting in retry loop. </p> <p>Savepoints are manually triggered snapshots of the job state, which can be used to upgrade a job or to perform manual recovery.</p> <p>Full checkpoints and savepoints take a long time, but incremental checkpoints are faster.</p> <h4 id=demonstrate-in-place-upgrade-of-stateless-statement>Demonstrate in-place upgrade of stateless statement<a class=headerlink href=#demonstrate-in-place-upgrade-of-stateless-statement title="Permanent link">&para;</a></h4> <h4 id=demonstrate-stateful-statement-upgrade>Demonstrate stateful statement upgrade<a class=headerlink href=#demonstrate-stateful-statement-upgrade title="Permanent link">&para;</a></h4> <h3 id=flink-on-kubernetes>Flink on Kubernetes<a class=headerlink href=#flink-on-kubernetes title="Permanent link">&para;</a></h3> <ul> <li>To trigger a savepoints</li> <li></li> </ul> <h2 id=testing-sql-statement-during-pipeline-development>Testing SQL statement during pipeline development<a class=headerlink href=#testing-sql-statement-during-pipeline-development title="Permanent link">&para;</a></h2> <p>We should differentiate two types of testing: Flink statement developer testing, like unit / component tests, and integration tests with other tables and with real data streams.</p> <p>The objective of a test harness for developer and system integration is to validate the quality of a new Flink SQL statement deployed on Confluent Cloud (or Flink managed service) to address at least the following needs:</p> <ol> <li>be able to deploy a flink statement (the ones we want to focus on are DMLs, or CTAS)</li> <li>be able to generate test data from schema registry - and with developer being able to tune test data for each test cases.</li> <li>produce test data to n source topics, consumes from output topic and validates expected results. All this flow being one test case. This may be automated for non-regression testing to ensure continuous quality.</li> <li>support multiple testcase definitions</li> <li>tear done topics and data.</li> </ol> <p>The following diagram illustrates the global infrastructure deployment context:</p> <p><img alt src=../diagrams/test_frwk_infra.drawio.png></p> <p>The following diagram illustrates the target unit testing environment:</p> <p><img alt src=../diagrams/test_frwk_design.drawio.png></p> <ul> <li>The Kafka Cluster is a shared cluster with topics getting real-time streams from production</li> </ul> <h2 id=measuring-latency>Measuring Latency<a class=headerlink href=#measuring-latency title="Permanent link">&para;</a></h2> <p>The goal is to measure the end-to-end data latency, which is the time from when data is created at its source to when it becomes available for end-user consumption. In any Apache Flink solution, especially those with chained Flink jobs, measuring this latency is a critical and planned activity within the deployment process.</p> <p>For example, consider a typical real-time data processing architecture. We'll use this architecture to illustrate how to measure and represent latency throughout the data flow:</p> <figure> <img alt src=../images/perf_test_basic.drawio.png> </figure> <p>The following timestamps may be considered:</p> <ol> <li>UpdatedDate column in the transactional database may be used to know when a record was created / updated, this will serve to measure end-to-end latency. </li> <li>Source CDC connector like Debezium, may add a timestamp in their message envelop that could be used, if injected in the messages to measure messaging latency.</li> <li>Within the Flink processing, event time, may be mapped to the Kafka Topic record time. Ths could be used to measure Flink pipeline latency.</li> </ol> <p>It is important to note that Flink latency results may seem inconsistent. This is normal, and due to the semantic of the Flink processing. To ensure exactly-once delivery, Flink uses transactions when writing messages into Kafka. It is part of a consume-process-produce pattern, and adds the offset of the messages it has consumed to the transaction. </p> <p>Flink persists its state, including the offsets of the Kafka source, via checkpoints, which are done once per minute. The frequency may be configured. Queries submitted through Confluent Cloud Flink SQL Workbench or inspecting the content of a topic in the console generate output based on <code>read_uncommitted</code> isolation. The latency may seem reasonalbe during iterative development in the console, it may increase during more rigorous tests that use a read_committed consumer.</p> <p>Consumer reading committed messages will observe latency (consumer's property of <code>isolation.level= read-committed</code>). Recall that, when a consumer starts up, or when a partition is newly assigned to it, the consumer will check the <code>__consumer_offsets</code> topic to find the latest committed offset for its consumer group and partition. It will then begin reading messages from the next offset. Consumer will wait and not advance its position until the transaction is either committed or aborted. This ensures it never sees messages from aborted transactions and only sees a complete, consistent set of records. </p> <p>When producer iniates a transaction, and writes messages to topic/partition, those messages are not yet visible to consumers configured to read committed data, when there is no error, the producer commits the transaction. </p> <p>When consumer applications may tolerate at-least once semantics, they may simply configure all consumers with <code>read_uncommitted</code> isolation, at the risk that during failure recovery and scaling activities, the Flink statement will reingest messages from the last checkpoint, causing duplicates and time-travel for end consumers.</p> <details class="- info"> <summary>Flink transaction process</summary> <p>Flink's core mechanism for fault tolerance is checkpointing. It periodically takes a consistent snapshot of the entire application state, including the offsets of the Kafka source and the state of all operators.</p> <ul> <li>Flink's Kafka sink connector uses a two-phase commit protocol.</li> <li>When a checkpoint is triggered, the Flink Kafka producer (the sink) writes any pending data to Kafka within a transaction. It then prepares to commit this transaction but waits for a signal from the Flink JobManager.</li> <li>Once the JobManager confirms that all operators have successfully snapshotted their state, it tells the Kafka producer to commit the transaction. This makes the new data visible to read_committed consumers.</li> <li>If any part of the checkpoint fails (e.g., a Flink task manager crashes), the transaction is aborted. The uncommitted messages become "ghost" records on the Kafka topic, invisible to read_committed consumers. When the Flink job restarts, it will restore its state from the last successful checkpoint and reprocess the data from that point, avoiding data loss or duplicate.</li> </ul> </details> <p>The <a href=https://jbcodeforce.github.io/shift_left_utils/coding/test_harness/#integration-tests>shift_left utility has an integration tests harness feature to do end-to-end testing with timestamp</a>.</p> <h2 id=other-sources>Other sources<a class=headerlink href=#other-sources title="Permanent link">&para;</a></h2> <p>The current content is sourced from the following cookbooks and lesson learnt while engaging with customers.</p> <ul> <li><a href=https://github.com/confluentinc/flink-cookbook>Confluent Flink Cookbook</a></li> <li><a href=https://github.com/ververica/flink-sql-cookbook/blob/main/README.md>Ververica Flink cookbook</a></li> </ul> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2018 - 2026 Jerome Boyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/jbcodeforce target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://linkedin.com/in/jeromeboyer target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["content.code.annotation", "content.code.copy", "content.tooltips", "content.tabs.link", "search.suggest", "search.highlight", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.tracking", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> </body> </html>