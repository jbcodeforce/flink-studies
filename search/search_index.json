{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udea7 The Complete Guide to Apache Flink and Confluent Flink","text":"<p>Welcome to \"A Guide to Apache Flink and Confluent Flink\" - a comprehensive, hands-on resource for mastering stream processing with Apache Flink and its enterprise distributions. I started this living book, in 2018 while working as IBM Distinguished Enginer / CTO for Event-Driven Architecture, and continue to enhance it while working at AWS and Confluent. It addresses practical implementations, methodologies and best practices I can share with my customers. In 2024 and 2025 I am focusing on Confluent Flink and Data mesh. All this content come from public content, articles, youtube, product documentations, git repositories.</p> <p>To make it fun, here is the book cover! Very imaginative for a book that will never go to press.</p> A virtual book about Apache Flink - 2018 to 2025 Site updates <ul> <li>Created 2018 </li> <li>Updates 10/2024: Reorganized content, separated SQL vs Java, added Confluent Cloud/Platform integration</li> <li>Updates 01/25: Added Terraform deployment examples, expanded SQL samples</li> <li>Updates 07/25: Added e2e demos, Flink estimator webapp, Data mesh</li> <li>update 09/2025: Work on new demonstration, code samples, improve deployment for Confluent Platform for Flink</li> </ul>"},{"location":"#what-youll-discover","title":"What You'll Discover","text":"<p>This site is designed to share studies of the Flink ecosystem, covering everything from fundamental concepts to advanced real-world implementations:</p>"},{"location":"#foundations-architecture","title":"Foundations &amp; Architecture","text":"<ul> <li>Core Flink concepts and runtime architecture</li> <li>Stream processing fundamentals and event time semantics</li> <li>Fault tolerance, checkpointing, and exactly-once processing</li> <li>State management and stateful stream processing patterns</li> </ul>"},{"location":"#programming-with-flink","title":"Programming with Flink","text":"<ul> <li>Flink SQL: From basic queries to complex analytical pipelines</li> <li>DataStream API: Building robust streaming applications in Java</li> <li>Table API: Bridging SQL and programmatic approaches</li> <li>PyFlink: Stream processing with Python</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Kubernetes-native deployments with Flink Operator</li> <li>Infrastructure as Code</li> <li>Monitoring, alerting, and performance optimization</li> <li>High availability and disaster recovery patterns</li> </ul>"},{"location":"#real-world-streaming-architectures","title":"Real-World Streaming Architectures","text":"<ul> <li>Change Data Capture (CDC) pipelines with Debezium</li> <li>Apache Kafka integration and event-driven architectures</li> <li>Data Lake integration with Apache Iceberg</li> <li>Real-time Analytics with complex event processing</li> </ul> <p>\ud83d\udea7 This guide is still under heavy development. Content, examples, and structure may change frequently. Check back often for updates! \ud83d\udea7</p>"},{"location":"#enterprise-ready-with-confluent","title":"Enterprise-Ready with Confluent","text":"<p>This book provides extensive coverage of Confluent Platform for Apache Flink and Confluent Cloud Flink, including:</p> <ul> <li>Confluent Cloud Flink compute pools and managed services</li> <li>Confluent Platform on-premises deployment patterns</li> <li>ksqlDB to Flink migration strategies and best practices</li> <li>Integration with Confluent's Schema Registry and Connect ecosystem</li> </ul>"},{"location":"#hands-on-learning-approach","title":"Hands-On Learning Approach","text":"<p>Every concept is reinforced with practical implementations:</p>"},{"location":"#code-examples","title":"Code Examples","text":"<ul> <li>20+ Java applications demonstrating DataStream patterns</li> <li>50+ SQL scripts covering joins, aggregations, and window functions</li> <li>Python Table API examples and integration patterns</li> <li>Complete end-to-end demonstrations with multiple components</li> </ul>"},{"location":"#end-to-end-demonstrations","title":"End-to-End Demonstrations","text":"<ul> <li>E-commerce Analytics Pipeline: Real-time user behavior analysis</li> <li>CDC Deduplication: Change data capture with transformation patterns</li> <li>Financial Transaction Processing: Complex event processing for fraud detection</li> <li>IoT Data Processing: Time-series analysis and alerting</li> </ul>"},{"location":"#deployment-ready-infrastructure","title":"Deployment-Ready Infrastructure","text":"<ul> <li>Docker Compose environments for local development</li> <li>Kubernetes manifests for production deployment</li> <li>Terraform modules for cloud infrastructure</li> <li>Monitoring stack with Prometheus and Grafana</li> </ul>"},{"location":"#who-this-guide-is-for","title":"Who This Guide Is For","text":"<p>Whether you're a data engineer building streaming pipelines, a software architect designing event-driven systems, or a platform engineer deploying Flink clusters, this guide provides the depth and breadth you need.</p> <p>Prerequisites: Basic familiarity with distributed systems, SQL, and Java or Python programming.</p>"},{"location":"#learning-path","title":"Learning Path","text":"<p>This book is structured to support both linear reading and focused deep-dives:</p> Quick Start Track (1-2 weeks)\"Foundation Track (3-4 weeks)\"Advanced Track (4-6 weeks)Production Track (6-8 weeks) <ol> <li>Getting Started - Your first Flink application</li> <li>Flink SQL Basics - Stream processing with SQL</li> <li>Local Deployment - Running Flink on Kubernetes</li> </ol> <ol> <li>Architecture Deep Dive - Understanding Flink internals</li> <li>State Management - Stateful stream processing</li> <li>Fault Tolerance - Exactly-once guarantees</li> </ol> <ol> <li>DataStream Programming - Complex stream processing logic</li> <li>Kafka Integration - Event-driven architectures</li> <li>Production Deployment - Enterprise deployment patterns</li> </ol> <ol> <li>Performance Tuning - Optimization techniques</li> <li>Monitoring &amp; Observability - Production operations</li> <li>End-to-End Projects - Real-world implementations</li> </ol>"},{"location":"#what-makes-this-guide-unique","title":"What Makes This Guide Unique","text":""},{"location":"#living-documentation","title":"Living Documentation","text":"<ul> <li>Continuously Updated: Content evolves with Flink releases and best practices</li> <li>Community Driven: Open source with contributions from practitioners</li> <li>Practical Focus: Every concept backed by working code examples</li> </ul>"},{"location":"#production-tested","title":"Production Tested","text":"<ul> <li>All examples are tested and validated in real environments</li> <li>Performance benchmarks and optimization guidelines</li> <li>Troubleshooting guides based on real-world challenges</li> </ul>"},{"location":"#multi-platform-coverage","title":"Multi-Platform Coverage","text":"<ul> <li>Apache Flink OSS - Complete open-source implementation</li> <li>Confluent Cloud - Fully managed cloud service</li> <li>Confluent Platform - Enterprise on-premises deployment</li> </ul> <p>\ud83d\udca1 Tip: This guide is designed to be read online at https://jbcodeforce.github.io/flink-studies/ with full navigation, search, and interactive examples. You can also clone the GitHub repository to run all examples locally.</p>"},{"location":"contributing/","title":"Contributing to this repository","text":"<p>This chapter addresses how to support the development of this open source project.</p> <p>Anyone can contribute to this repository and associated projects.</p> <p>There are multiple ways to contribute: report bugs and suggest improvements, improve the documentation, and contribute code.</p>"},{"location":"contributing/#bug-reports-documentation-changes-and-feature-requests","title":"Bug reports, documentation changes, and feature requests","text":"<p>If you would like to contribute to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list.</p> <p>Before opening a new issue, please check the existing list to make sure a similar or duplicate item does not already exist.  When you create your issues, please be as explicit as possible and be sure to include the following:</p> <ul> <li> <p>Bug reports</p> <ul> <li>Specific project version</li> <li>Deployment environment</li> <li>A minimal, but complete, setup of steps to recreate the problem</li> </ul> </li> <li> <p>Documentation changes</p> <ul> <li>URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation)</li> <li>Updates required to correct current inconsistency</li> <li>If possible, a link to a project fork, sample, or workflow to expose the gap in documentation.</li> </ul> </li> <li> <p>Feature requests</p> <ul> <li>Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created.</li> <li>A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap.</li> </ul> </li> </ul> <p>The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.  When creating the GitHub issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project).  These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.</p>"},{"location":"contributing/#code-contributions","title":"Code contributions","text":"<p>We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below.  If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only.  These are a few projects that help on-board new contributors to the overall process.</p>"},{"location":"contributing/#coding-and-pull-requests-best-practices","title":"Coding and Pull Requests best practices","text":"<ul> <li> <p>Please ensure you follow the coding standard and code formatting used throughout the existing code base.</p> <ul> <li>This may vary project by project, but any specific diversion from normal language standards will be explicitly noted.</li> </ul> </li> <li> <p>One feature / bug fix / documentation update per pull request</p> <ul> <li>Always pull the latest changes from upstream and rebase before creating any pull request.  </li> <li>New pull requests should be created against the <code>integration</code> branch of the repository, if available.</li> <li>This ensures new code is included in full-stack integration tests before being merged into the <code>main</code> branch</li> </ul> </li> <li> <p>All new features must be accompanied by associated tests.</p> <ul> <li>Make sure all tests pass locally before submitting a pull request.</li> <li>Include tests with every feature enhancement, improve tests with every bug fix</li> </ul> </li> </ul>"},{"location":"contributing/#github-and-git-flow","title":"Github and git flow","text":"<p>The internet is littered with guides and information on how to use and understand git.</p> <p>However, here's a compact guide that follows the suggested workflow that we try to follow:</p> <ol> <li> <p>Fork the desired repo in github.</p> </li> <li> <p>Clone your repo to your local computer.</p> </li> <li> <p>Add the upstream repository</p> <p>Note: Guide for step 1-3 here: forking a repo</p> </li> <li> <p>Create new development branch off the targeted upstream branch.  This will often be <code>main</code>.</p> <pre><code>git checkout -b &lt;my-feature-branch&gt; main\n</code></pre> </li> <li> <p>Do your work:</p> </li> <li> <p>Write your code</p> </li> <li>Write your tests</li> <li>Pass your tests locally</li> <li>Commit your intermediate changes as you go and as appropriate</li> <li> <p>Repeat until satisfied</p> </li> <li> <p>Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature).</p> <pre><code>git fetch upstream\n</code></pre> </li> <li> <p>Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict.</p> <pre><code>git branch --set-upstream-to=upstream/main\ngit rebase\n</code></pre> <p>Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI.</p> </li> <li> <p>Push the changes to your repository</p> <pre><code>git push origin &lt;my-feature-branch&gt;\n</code></pre> </li> <li> <p>Create a pull request against the same targeted upstream branch.</p> <p>Creating a pull request</p> </li> </ol> <p>Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronize your remote and local forked github repository <code>main</code> branch with the upstream main branch. To do so:</p> <ol> <li> <p>Pull to your local forked repository the latest changes upstream (that is, the pull request).</p> <pre><code>git pull upstream main\n</code></pre> </li> <li> <p>Push those latest upstream changes pulled locally to your remote forked repository.</p> <pre><code>git push origin main\n</code></pre> </li> </ol>"},{"location":"contributing/#what-happens-next","title":"What happens next?","text":"<ul> <li>All pull requests will be automatically built with GitHub Action, when implemented by that specific project.</li> <li>You can determine if a given project is enabled for GitHub Action workflow by the existence of a <code>./github/workflow</code> folder in the root of the repository or branch.</li> <li>When in use, unit tests must pass completely before any further review or discussion takes place.</li> <li>The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch.</li> <li>Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch.</li> <li>Commits passing this stage will make it into the next release cycle for the given project.</li> </ul>"},{"location":"contributing/#environment-set-up","title":"Environment set up","text":"<p>We are using uv as a new Python package manager. See uv installation documentation then follow the next steps to set your environment for development:</p> <ul> <li>Clone this repository: </li> </ul> <pre><code>git clone  https://github.com/jbcodeforce/flink-studies.git\ncd flink-studies\n</code></pre> <ul> <li>Create a new virtual environment in any folder, but could be the : <code>uv venv</code></li> <li>Activate the environment:</li> </ul> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"architecture/","title":"Flink architecture","text":"Update <ul> <li>Created 2018 </li> <li>Updated 11/2024 - review done.</li> <li>12/2024: move fault tolerance in this chapter</li> </ul>"},{"location":"architecture/#runtime-architecture","title":"Runtime architecture","text":"<p>Flink consists of a Job Manager and <code>n</code> Task Managers deployed on <code>k</code> hosts. </p> Main Flink Components <p>The JobManager controls the execution of a single application. Developers submit their application (jar file or SQL statements) via CLI, or k8s manifest. Job Manager receives the Flink application for execution and builds a Task Execution Graph from the defined JobGraph. It manages job submission which parallelizes the job and distributes slices of the Data Stream flow, the developers have defined. Each parallel slice of the job is a task that is executed in a task slot.  </p> <p>Once the job is submitted, the Job Manager is scheduling the job to different task slots within the Task Manager. The Job manager may create resources from a computer pool, or when deployed on kubernetes, it creates pods. </p> <p>The Resource Manager manages Task Slots and leverages an underlying orchestrator, like Kubernetes or Yarn (deprecated).</p> <p>A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager:</p> Sequence flow from job submission <p>The Dispatcher exposes API to submit applications for execution. It hosts the user interface too.</p> <p>Once the job is running, the Job Manager is responsible to coordinate the activities of the Flink cluster, like checkpointing, and restarting task manager that may have failed.</p> <p>Tasks are loading the data from sources, do their own processing and then send data among themselves for repartitioning and rebalancing, to finally push results out to the sinks.</p> <p>When Flink is not able to process a real-time event, it may have to buffer it, until other necessary data has arrived. This buffer has to be persisted in longer storage, so data are not lost if a task manager fails and has to be restarted. In batch mode, the job can reload the data from the beginning. In batch the results are computed once the job is done (count the number of record like <code>select count(*) AS</code>count<code>from bounded_pageviews;</code> return one result), while in streaming mode, each event may be the last one received, so results are produced incrementally, after every events or after a certain period of time based on timers.</p> Parameters <ul> <li>taskmanager.numberOfTaskSlots: 2</li> </ul> <p>Once Flink is started (for example with the docker image), Flink Dashboard http://localhost:8081/#/overview presents the execution reporting:</p> Flink User Interface <p>The execution is from one of the training examples, the number of task slot was set to 4, and one job is running.</p> <p>Spark Streaming is using microbatching which is not a true real-time processing while Flink is a RT engine. Both Flink and Spark support batch processing. </p> <p>Only one Job Manager is active at a given point of time, and there may be <code>n</code> Task Managers. It is a single point of failure, but it startes quickly and can leverage the checkpoints data to restart its processing.</p> <p>There are different deployment models: </p> <ul> <li>Deploy on executing cluster, this is the session mode. Use session cluster to run multiple jobs: we need a separate JobManager container for that. </li> <li>Per job mode: spin up a cluster per job submission. This provides better resource isolation. </li> <li>Application mode: creates a cluster per app with the <code>main()</code> function executed on the JobManager. It can include multiple jobs but they run inside the app. It allows for saving the required CPU cycles, but also save the bandwidth required for downloading the dependencies locally.</li> </ul> <p>Flink can run on any common resource manager like Hadoop Yarn, Mesos, or Kubernetes. For development purpose, we can use docker images to deploy a Session or Job cluster.</p> <p>See also deployment to Kubernetes</p> <p>The new K8s operator, deploys and monitors Flink Application and Session deployments.</p>"},{"location":"architecture/#batch-processing","title":"Batch processing","text":"<p>Process all the data in one job with bounded dataset. It is used when we need all the data, to assess trend, develop AI model, and with a focus on throughput instead of latency. Jobs are run when needed, on input that can be pre-sorted by time or by any other key.</p> <p>The results are reported at the end of the job execution. Any failure means to do of full restart of the job.</p> <p>Hadoop was designed to do batch processing. Flink has capability to replace the Hadoop map-reduce processing.</p> <p>When latency is a major requirements, like monitoring and alerting, fraud detection then streaming is the only choice.</p>"},{"location":"architecture/#state-management","title":"State management","text":"<p>See 'working with state' from Flink documentation.</p> <ul> <li> <p>All data maintained by a task and used to compute the results of a function belong to the state of the task. Function may use  pairs to store values, and may implement The ChekpointedFunctions to make local state fault tolerant. <li> <p>While processing the data, the task can read and update its state and computes its results based on its input data and state.</p> </li> <li>State management may address very large states, and no state is lost in case of failures.</li> <li>Within a DAG, each operator needs to register its state.</li> <li>Operator state is scoped to an operator task: all records processed by the same parallel task have access to the same state.</li> <li>Keyed state is maintained and accessed with respect to a key defined in the records of an operator\u2019s input stream. Flink maintains one state instance per key value and Flink partitions all records with the same key to the operator task that maintains the state for this key. The key-value map is sharded across all parallel tasks:</li> Keyes states <ul> <li>Each task maintains its state locally to improve latency. For small state, the state backends will use JVM heap, but for larger state RocksDB is used. A state backend takes care of checkpointing the state of a task to a remote and persistent storage.</li> <li>With stateful distributed processing, scaling stateful operators, enforces state repartitioning and assigning to more or fewer parallel tasks. Keys are organized in key-groups, and key groups are assigned to tasks. Operators with operator list state are scaled by redistributing the list entries. Operators with operator union list state are scaled by broadcasting the full list of state entries to each task.</li> </ul> State Backend <ul> <li>Embedded rockdb will persist on Task manager local data directories. It saves asynchronously. Serializes using bytes. But there is a limit to the size per key and valye of 2^31 bytes. Supports incremental checkpoints</li> <li>ForStState use tree structured k-v store. May use object storage for remote file systems. Allows Flink to scale the state size beyond the local disk capacity of the TaskManager. </li> <li><code>HashMapStateBackend</code> use Java heap to keep state, as java object. So unsafe to reuse!.</li> </ul>"},{"location":"architecture/#high-availability","title":"High Availability","text":"<p>With Task managers running in parallel, if one fails the number of available slots drops, and the JobManager asks the Resource Manager to get new processing slots. The application's restart strategy determines how often the JobManager restarts the application and how long it waits between restarts.</p> <p>Flink OSS uses Zookeeper to manage multiple JobManagers and select the leader to control the execution of the streaming jobs. Application's tasks checkpoints and other states are saved in a remote storage, but metadata are saved in Zookeeper. When a JobManager fails, all tasks that belong to its application are automatically cancelled. A new JobManager that takes over the work by getting information of the storage from Zookeeper, and then restarts the process with the JobManager.</p>"},{"location":"architecture/#fault-tolerance","title":"Fault Tolerance","text":"<p>The two major Flink features to support fault tolerance are the checkpoints and savepoints. </p>"},{"location":"architecture/#checkpointing","title":"Checkpointing","text":"<p>Checkpoints are snapshots of the input data stream, capturing the state of each operator, of the DAG, at a specific point in time. They are created automatically and periodically by Flink. The saved states are used to recover from failures, and checkpoints are optimized for quick recovery.</p> <p>Checkpoints allow a streaming dataflow to be resumed from a checkpoint while maintaining consistency through exactly-once processing semantics. When a failure occurs, Flink can restore the state of the operators and replay the records starting from the checkpoint.</p> <p>In the event of a failure in a parallel execution, Flink halts the stream flow and restarts the operators from the most recent checkpoints. During data partition reallocation for processing, the associated states are also reallocated. States are stored in distributed file systems, and when Kafka is used as the data source, the committed read offsets are included in the checkpoint data.</p> <p>Checkpointing is coordinated by the Job Manager, it knows the location of the latest completed checkpoint which will get important later on. This checkpointing and recovery mechanism can provide exactly-once consistency for application state, given that all operators checkpoint and restore all of their states and that all input streams are reset to the position up to which they were consumed when the checkpoint was taken. This will work perfectly with Kafka, but not with sockets or queues where messages are lost once consumed. Therefore exactly-once state consistency can be ensured only if all input streams are from reset-able data sources.</p> <p>As part of the checkpointing process, Flink saves the 'offset read commit' information of the append log, so in case of a failure, Flink recovers the stateful streaming application by restoring its state from a previous checkpoint and resetting the read position on the append log.</p> <p>During the recovery and depending on the sink operators of an application, some result records might be emitted multiple times to downstream systems. Downstream systems need to be idempotent.</p> <p>Flink utilizes the concept of Checkpoint Barriers to delineate records. These barriers separate records so that those received after the last snapshot are included in the next checkpoint, ensuring a clear and consistent state transition.</p> <p>Barrier can be seen as a mark, a tag, in the data stream and aims to close a snapshot. </p> Checkpointing concepts <p>Checkpoint barriers flow with the stream, allowing them to be distributed across the system. When a sink operator \u2014 located at the end of a streaming Directed Acyclic Graph (DAG) \u2014 receives <code>barrier n</code> from all its input streams, it acknowledges <code>snapshot n</code> to the checkpoint coordinator.</p> <p>Once all sink operators have acknowledged a snapshot, it is considered completed. After <code>snapshot n</code> is finalized, the job will not request any records from the source prior to that snapshot, ensuring data consistency and integrity.</p> <p>State snapshots are stored in a state backend, which can include options such as in-memory storage, HDFS, object storage or RocksDB. This flexibility allows for optimal performance and scalability based on the application\u2019s requirements.</p> <p>In the context of a KeyedStream, Flink functions as a key-value store where the key corresponds to the key in the stream. State updates do not require transactions, simplifying the update process.</p> <p>For DataSet (Batch processing) there is no checkpoint, so in case of failure the stream is replayed from the beginning.</p> <p>When addressing exactly once processing, it is crucial to consider the following steps:</p> <ul> <li>Read Operation from the Source: Ensuring that the data is read exactly once is foundational. Flink's source connectors are designed to handle this reliably through mechanisms like checkpointing.</li> <li>Apply Processing Logic which involves operations such as window aggregation or other transformations, which can also be executed with exactly-once semantics when properly configured.</li> <li>Generate Results to a Sink introduces more complexity. While reading from the source and applying processing logic can be managed to ensure exactly-once semantics, generating a unique result to a sink depends on the target technology and its capabilities. Different sink technologies may have varying levels of support for exactly-once processing, requiring additional strategies such as idempotent writes or transactional sinks to achieve the desired consistency.</li> </ul> End-to-end exactly once <p>After reading records from Kafka, processing them, and generating results, if a failure occurs, Flink will revert to the last committed read offset. This means it will reload the records from Kafka and reprocess them. As a result, this can lead to duplicate entries being generated in the sink:</p> End-to-end recovery <p>Since duplicates may occur, it is crucial to assess how downstream applications handle idempotence. Many distributed key-value stores are designed to provide consistent results even after retries, which can help manage duplicate entries effectively.</p> <p>To achieve end-to-end exactly-once delivery, it is essential to utilize a sink that supports transactions and implements a two-phase commit protocol. In the event of a failure, this allows for the rollback of any output generated, ensuring that only successfully processed data is committed. However, it's important to note that implementing transactional outputs can impact overall latency.</p> <p>Flink takes checkpoints periodically \u2014 typically every 10 seconds \u2014 which establishes the minimum latency we can expect at the sink level. This periodic checkpointing is a critical aspect of maintaining state consistency while balancing the need for timely data processing.</p> <p>For Kafka Sink connector, as kafka producer, we need to set the <code>transactionId</code>, and the delivery guarantee type:</p> <pre><code>new KafkaSinkBuilder&lt;String&gt;()\n    .setBootstrapServers(bootstrapURL)\n    .setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE)\n    .setTransactionalIdPrefix(\"store-sol\")\n</code></pre> <p>With transaction ID, a sequence number is sent by the Kafka producer API to the broker, and so the partition leader will be able to remove duplicate retries.</p> End-to-end with Kafka transaction id <p>When the checkpointing period is set, we need to also configure <code>transaction.max.timeout.ms</code> of the Kafka broker and <code>transaction.timeout.ms</code> for the producer (sink connector) to a higher timeout than the checkpointing interval plus the max expected Flink downtime. If not the Kafka broker will consider the connection has failed and will remove its state management.</p> <p>The evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime.</p> Event-driven application as a sequence of Flink apps <p>State is always accessed locally, which helps Flink applications achieve high throughput and low-latency. Developers can choose to keep state on the JVM heap, or if it is too large, save it on-disk.</p> Different State Storage"},{"location":"architecture/#savepoints","title":"Savepoints","text":"<p>Savepoints are user triggered snapshot at a specific point in time. It is used during system operations like product upgrades. The Flink operator for kubernetes has custom resource definition to support the savepoint process. See also the end to end demo for savepoint in this folder.</p>"},{"location":"architecture/#faqs","title":"FAQs","text":""},{"location":"architecture/#checkpoints-impact-throughput","title":"Checkpoints impact throughput","text":"<ul> <li>The persistence to remote storage is done asynchronously, but at the level of a task. So too frequent checkpointing will impact throughput. Now it also depends if the tasks are compute or IO intensive. </li> </ul>"},{"location":"architecture/#interuption-while-writing-checkpoints","title":"Interuption while writing checkpoints","text":"<ul> <li>The processing will restart from the last persisted checkpoints so no data loss. Specially true when source of the data are coming from Kafka topics. The checkpoint points to last read-commited offset within topic/partition so Flink will reload from there</li> </ul>"},{"location":"architecture/#when-flink-cluster-has-10-nodes-what-happen-in-one-node-failure","title":"When Flink cluster has 10 nodes what happen in one node failure","text":"<p>It will depend of the operator allocation to the task to the task manager and what the operator needs (as state). At worse case the full DAG needs to be restored, every operator needs to rebuild their state so multiple task managers in the cluster.</p> <p>It can take sometime to recover. Reread data and reprocess it, will take many seconds, or minutes. </p> <p>With hot-hot deployment, it is possible to get the same running application running in parallel, and then switch the output sink / topic for the consumer. For real-time payment we can achieve around 3 to 7 seconds recovery time, with million of records per second.  </p>"},{"location":"architecture/#can-we-set-one-task-manager-one-task-to-run-all-a-dag-to-make-it-simple","title":"Can we set one task manager one task to run all a DAG to make it simple?","text":"<p>It will depend of the application state size and logic to operate. If all state stays in memory, yes this is a common pattern to use. If state are bigger than physical memory of the computer running the task manager, then the processing needs more computers, so more task managers and need to distribute data. Then it needs distributed storage to persist states. </p>"},{"location":"architecture/#network-stack","title":"Network Stack","text":"<p>The Flink network stack helps connecting work units across TaskManagers using Netty. Flink uses a credit-based flow control for managing buffer availability and preventing backpressure.</p> <p>See the Deep dive article in Flink network stack</p>"},{"location":"architecture/agentic_flink/","title":"Agentic applications cross systems","text":"<p>AI agentic applications, at scale will not only be triggered by users, but by systems using asynchronous events. It is assumed that AI Agents are becoming experts to certain tasks within a business workflow and using domain-specific knowledge, and acts on direct user query or from events coming from other systems. As humen collaborate in a business function and process, AI Agents will collaborate with AI Agents and humen.</p> <p>As part of the Agentic architecture, there is the planning phase of an agent, which has to use up-to-date data to define the best future actions. As two examples, AI Agents may predict maintenance needs, adjust operational parameters to prevent downtime, and ensure that energy production meets demand without excess waste. In healthcare, AI Agents may analyzing genetic data, medical histories, and real-time responses to various treatments.</p> <p>Flink's event capabilities in real-time distributed event processing, state management and exact-once consistency fault tolerance make it well-suited as a framework for building such system-triggered agents.</p> <p>Currently, agent frameworks contain some major inhibitors: data preparation and pipeline to build embedding and process semi-structured an unstructured data.</p>"},{"location":"architecture/agentic_flink/#needs","title":"Needs","text":"<ul> <li>Deliver fresh data from the transactional systems to the AI Agents: stale data leads to irrelevant recommendations and decisions. This is only uses during inference and in the LLM conversation context enrichment.</li> <li>Model fine tuning needs clean data that is prepared by Data engineers using traditional SQL query, and python code to build relevant AI features. The data are at rest.</li> <li>Search (full text search, vector search and graph search) is used to enrich the LLM context. But point-in-time lookups need to be supported with temporal windoes and lookup joins.</li> <li>Real-time data processing may need scoring computed by AI model, remotly accessed.</li> <li>Adopt an event-driven architecture for AI Agents integration and data exchanges using messaging: the orchestration is not hard coded into a work flow (Apache airflow) but more event oriented and AI agent consumers act on those events.</li> <li>Event processing can trigger AI agents to act to address customer's inqueries</li> </ul>"},{"location":"architecture/agentic_flink/#challenges","title":"Challenges","text":"<ul> <li>Data generated isn't delivered to AI systems fast enough</li> <li>Current Agentic SDK or framework lack replayability and production readiness</li> <li>Missing fresh representation of live operational events</li> <li>Separation between data processing and AI</li> </ul>"},{"location":"architecture/agentic_flink/#event-driven-ai-agent","title":"Event-driven AI Agent","text":"<p>Extending the Agentic reference architecture, introduced by Lilian Weng, which defines how agents should be designed, it is important to leverage the experience acquirred during microservice implementations to start adopting an event-driven AI agent architecture, which may be represented by the following high level figure:</p> <p></p>"},{"location":"architecture/agentic_flink/#technologies","title":"Technologies","text":"<ul> <li>Flink paired with Kafka is purpose-built for real-time data processing and event-driven microservices. Confluent AI with Flink SQL</li> <li>Agent communication protocols are available to define agent interations: Agent to Agent from Google and ACP from IBM/ Linux foundations</li> <li>Agent integrate with IT applications and services via Model Context Protocol from Anthropic</li> </ul>"},{"location":"architecture/agentic_flink/#assessment","title":"Assessment","text":"<ul> <li>What are the current GenAI goals and projects?</li> <li>How data are delivered as context to agents?</li> </ul>"},{"location":"architecture/agentic_flink/#confluent-cloud-flink-sql-constructs","title":"Confluent Cloud Flink SQL constructs","text":"<p>A set of function to call an AI or ML model remotely from SQL queries.</p>"},{"location":"architecture/cookbook/","title":"Flink Cookbook","text":"Chapter updates <ul> <li>Created 12/2024 </li> <li>Updated 1/28/2025: env, testing and statement life cycle.</li> </ul> <p>There is a Confluent cookbook for best practices to run Flink into production. The content of this page is to get a summary of those practices, enhanced from other customers' engagements. It also references hands-on exercises within this repository. </p> <p>Examples may be run inside from Terminal or using Kubernetes cluster locally, they are on Flink 1.20.1 or Flink 2.1. Use Java 11 or 17, see sdkman to manage different java version. </p>"},{"location":"architecture/cookbook/#understand-the-flink-ui","title":"Understand the Flink UI","text":"<p>The Flink Web UI helps to debug misbehaving jobs. </p> <p>The Flink Web UI is  well described in Confluent David Anderson's article, The Apache Flink doc for Web UI, and link to the important execution plan understanding with EXPLAIN.</p> <p>With OSS the Web UI is accessible when the <code>start_cluster.sh</code> is started. Local URL is http://localhost:8081. The Web UI offers the following important features:</p> <ul> <li>Navigating to get the running Jobs, the view is updated periodically. The job graph, which matches the EXPLAIN output, presents the tasks running one or more operators of the DAG.</li> <li>Task metrics are backpressure, busyness, and data skew.<ul> <li>backpressure: percentage of time that the subtask was unable to send output downstream because the downstream subtask had fallen behind, and (temporarily) couldn't receive any more records. <code>Backpressured max</code> is the maximum backpressure across all of the parallel subtasks for a given period.</li> <li>busy reports percentage of time spent doing useful work, aggregated at the task level for a time period.</li> <li>data skew measures the degree of variation in the number of records processed per second by each of the parallel subtasks. 100% is max skew.</li> </ul> </li> <li>Examining the history of checkpoints</li> <li>Monitoring for any potential backpressure</li> <li>Analyzing watermarks</li> <li>Retrieving the job logs</li> </ul> <p>Network metrics (Bytes Received / Records Received ) are inside the Flink cluster, not for source and sink to external systems.</p> <p>In Concluent Cloud the Query Profiler has the same capability then the Flink UI and accessible at the Statement View level: </p>"},{"location":"architecture/cookbook/#classical-deployment-pattern","title":"Classical deployment pattern","text":"<p>For Confluent Cloud for Flink we want to map environments to physical environments, like dev, Staging and Production. Any stream processing has a set of source producer that will be part of an ingestion layer. In the Kafka architecture, it will be a set of Kafka Connector cluster, with for example Change Data Capture connector like Debezium. Most of the time the dev environment may not have the streams coming from this ingestion layer. </p> <p>For the discussion each business application can have one to many pipelines. A Pipeline is a set of Flink jobs running SQL statements or Table API programs. A generic pattern of a pipeline involves at least, the following steps:</p> Figure 1: Generic pipeline structure <ol> <li>A CDC source connector injects data in Kafka topic. Avro schemas are defined for the Key and the Value.</li> <li>A first set of statements are doing deduplication logic, or filtering to ensure only relevant messages are processed by the pipeline</li> <li>There will be zero to many intermediate tables, depending of the business logic needed for the application. Those intermediate tables/topics may get enrichement, aggregation or joins.</li> <li>The final step is to prepare the data for sink processing. The statements may includes joins and filtering out, even may be some deduplication logic too.</li> <li>The data need to land to sink external system, so Kafka Sink connectors are deployed to write to those systems. Here the example illustrate a datawarehouse system based on Postgreql, on which a business intelligent component will implement adhoc queries and dashboards. </li> </ol> <p>The artifacts for development are the DDL and DML statements and test data.</p> <p>Finally to support the deployment and quality control of those pipelines deployment, the following figures illustrates a classical deployment pattern:</p> Figure 2: Environment mapping <ol> <li>Each environment has its own schema registry</li> <li>Once Kafka Cluster per env, with different ACL rules to control who can create topic, read and write.</li> <li>For each application it may be relevant to isolate them in their own Flink Compute pool</li> <li>CI/CD can define infrastructure as code for the Flink Compute pool, the Kafka Cluster, the Kafka Connector cluster and connectors configuration, the input topics, the ACLs, the schema registry.</li> </ol> <p>This architecture helps to clearly separate schema management per environment, and help to promote real-time processing pipelines from dev to staging to production in a control manner using a GitOps approach.</p> Gitops <p>The core concept of GitOps is to maintain a single Git repository that consistently holds declarative descriptions of the desired infrastructure in the production environment. An automated process ensures that the production environment aligns with the state described in the repository. The methodology and tools support changing infrastructure using feature branches, PR, PR review, </p>"},{"location":"architecture/cookbook/#sizing","title":"Sizing","text":"<p>Sizing a Flink cluster is a complex process influenced by many factors, including workload demands, application logic, data characteristics, expected state size, required throughput and latency, concurrency, and hardware. </p> <p>Because of those variables, every Flink deployment needs a unique sizing approach. The most effective method is to run a real job, on real hardware and tune Flink to that specific workload.</p> <p>For architects seeking sizing guidance, it's helpful to consider: * the workload semantic complexity, with the usage of aggregations, joins, windows, processing type,  * the input throughput (MB/s or records/second),  * the expected state size (GB),  * the expected latency.</p> <p>While Kafka sizing estimates are based on throughput and latency, this is a very crude method for Flink, as it overlooks many critical details. </p> <p>For new Flink deployments, a preliminary estimate can be provided, but it's important to stress its inexact nature.  A simple Flink job can process approximately 10,000 records per second per CPU. However, a more substantial job, based on benchmarks, might process closer to 5,000 records per second per CPU. Sizing may use record size, throughput, and Flink statement complexity to estimate CPU load.</p> Tool help manage Flink Cluster estimations <p>The flink-estimator git repository includes a web app with backend estimator for Flink Cluster sizing and locally manage your own configuration. To access this web app there is a docker image at dockerhub - flink-estimator. It can be started with <code>docker-compose start -d</code> or deployed to local kubernetes: <code>kubectl apply -k k8s</code>. Access via web browser http://localhost:8002/</p> <p></p>"},{"location":"architecture/cookbook/#exactly-once-delivery","title":"Exactly-once-delivery","text":"<p>Flink's internal exactly-once guarantee is robust, but for the results to be accurate in the external system, that system (the sink) must cooperate.</p> <p>This is a complex subject to address and context is important on how to assess exactly-once-delivery: within Flink processing, versus with an end-to-end solution context. </p>"},{"location":"architecture/cookbook/#flink-context","title":"Flink context","text":"<p>For Flink, \"Each incoming event affects the final Flink statement results exactly once.\" as said Piotr Nowojski during his presentation at the Flink Forward 2017 conference. No data duplication and no data loss. Flink achieves it through a combination of checkpointing, state management, and transactional sinks. Checkpoints save the state of the stream processing application at regular intervals. State management maintains the consistency of data between the checkpoints. Transactional sinks ensure that data gets written out exactly once, even during failures </p> <p>Flink uses transactions when writing messages into Kafka. Kafka messages are only visible when the transaction is actually committed as part of a Flink checkpoint. <code>read_committed</code> consumers will only get the committed messages. <code>read_uncommitted</code> consumers see all messages.</p> <p>As the default checkpoint interval is set to 60 seconds, <code>read_committed</code> consumers will see up to one minute latency: a Kafka message sent just before the commit will have few second latency, while older messages will be above 60 seconds.</p> <p>When multiple Flink statements are chained in a pipeline, the latency may be even bigger, as Flink Kafka source connector uses <code>read_committed</code> isolation.</p> <p>The checkpoints frequency can be updated but could not go below 10s. Shorter interval improves fault tolerance, but adds persistence and performance overhead.</p>"},{"location":"architecture/cookbook/#end-to-end-solution","title":"End-to-end solution","text":"<p>On the sink side, Flink has a 2 phase commit sink function on specific data sources, which includes Kafka, message queue and JDBC. </p> <p>For stream processing requiring an upsert capability (insert new records or update existing ones based on a key), the approach is to assess:</p> <ul> <li>if the sink kafka connector support upsert operations: it emits only the latest state for each key, and a tombstone message for delete (which is crucial for Kafka's log compaction to work correctly).</li> <li>For Datsabase, be sure to use a JDBC connector, with upsert support. Achieving exactly-once to a traditional database is done by leveraging the sink's implementation of Flink's Two-Phase Commit protocol. The database's transactions must be compatible with this to make sure writes are only committed when a Flink checkpoint successfully completes.</li> <li>For Lakehouse Sink</li> </ul> <p>The external system must provide support for transactions that integrates with a two-phase commit protocol. </p> <p>When using transactions on sink side, there is a pre-commit phase which starts from the checkpointing: the Job Manager injects a checkpoint barrier to seperate streamed in records before or after the barrier. As the barrier flows to the operators, each one of them, takes a snapshot or their state. The sink operators that support transactions, need to start the transaction in the precommit phase while saving its state to the state backend. After a successful pre-commit phase, the commit must guarantee the success for all operators. In case of any failure, the tx is aborted and rolled back.</p> <ul> <li>Article: An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!).</li> <li>Confluent documentation.</li> <li>Confluent Platform - Kafka consumer isolation level property.</li> </ul>"},{"location":"architecture/cookbook/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/cookbook/#a-sql-statement-not-returning-any-result","title":"A SQL statement not returning any result","text":"<p>This could be linked to multiple reasons so verify the following:</p> <ul> <li>Verify there is no exception in the statement itself</li> <li>Query logic being too restrictive or the joins may not match any records. </li> <li>For aggregation, assess if the field used get null values.</li> <li>Source table may be empty, or it consumes the table from a different starting offset (specified via <code>scan.bounded.mode</code>) then expected.</li> <li>Use <code>show create table &lt;table_name&gt;</code> to assess the starting offset strategy or specific values</li> <li>Count all records in a table using <code>SELECT COUNT(*) FROM table_name;</code>, it should be greater then 0.</li> <li>When the statement uses event-time based operation like <code>windowing, top N, OVER, MATCH_RECOGNIZE</code> and temporal joins then verify the watermarks. The following example is from Confluent Cloud for Flink query using the event time from the record, and it should return result. Check if you have produced a minimum of records per Kafka partition, or if the producer has stopped producing data all together.</li> </ul> <pre><code>SELECT ROW_NUMBER() OVER (ORDER BY $rowtime ASC) AS number, *   FROM &lt;table_name&gt;\n</code></pre> <ul> <li>When Data are in topic but not seen by flink <code>select * from &lt;table_name&gt;</code> statement, it may be due to idle partitions and the way watermarks advance and are propagated. Flink automatically marks a Kafka partition as idle if no events come within <code>sql.tables.scan.idle-timeout</code> duration. When a partition is marked as idle, it does not contribute to the watermark calculation until a new event arrives. Try to set the idle timeout for table scans to ensure that Flink considers partitions idle after a certain period of inactivity. Try to create a table with a watermark definition to handle idle partitions and ensure that watermarks advance correctly.</li> </ul>"},{"location":"architecture/cookbook/#high-availability-and-disaster-recovery","title":"High Availability and Disaster Recovery","text":"<p>In Flink high availability goal is to keep the application running and being able to process data. The focus is more on streaming applications then batch. JobManager is a single point of failure but can be configured with standby JobManagers. The coordination can be done with Zookeeper for self managed deployment or via Kubernetes operator.</p> <ul> <li>For Confluent Cloud for Flink see the DR section.</li> </ul>"},{"location":"architecture/cookbook/#security","title":"Security","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#deduplication","title":"Deduplication","text":"<p>Deduplication is documented here and here and at its core principal, it uses a CTE to add a row number, as a unique sequential number to each row. The columns used to de-duplicate are defined in the partitioning. Ordering is using a timestamp to keep the last record or first record. Flink support only ordering on time.</p> <pre><code>SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER ([PARTITION BY column1[, column2...]]\n       ORDER BY time_attr [asc|desc]) AS rownum\n   FROM table_name\n) WHERE rownum = 1\n</code></pre> <p>When using Kafka Topic to persist Flink table, it is possible to use the <code>upsert</code> or <code>retract</code> change log mode, and define the primary key(s) to remove duplicate records as only the last records will be used:</p> <pre><code>CREATE TABLE orders_deduped (\n  PRIMARY KEY( order_id, member_id) NOT ENFORCED) DISTRIBUTED BY (order_id, member_id) INTO 1 BUCKETS \nWITH (\n  'changelog.mode' = 'upsert',\n  'value.fields-include' = 'all'\n) AS\nSELECT\n  *\nFROM (\n  SELECT\n      *,\n      ROW_NUMBER() OVER (\n        PARTITION BY `order_id`, `member_id`\n        ORDER\n          BY $rowtime DESC\n      ) AS row_num\n    FROM orders_raw\n) WHERE row_num = 1;\n</code></pre> <p>To validate there is no duplicate records in the output table, use a query with tumble window like:</p> <pre><code>  SELECT\n      `order_id`, \n      `user_id`,\n    COUNT(*) AS cnt\n    FROM\n    TABLE(\n        tumble(\n        TABLE orders_raw,\n        DESCRIPTOR($rowtime),\n        INTERVAL '1' MINUTE\n        )\n    )\n    GROUP\n    BY  `order_id`, `user_id` HAVING COUNT(*) &gt; 1;\n</code></pre> <p>Duplicates may still occur on the Sink side of the pipeline, as it is linked to the type of connector used and its configuration, for example reading un-committed offset. Few connector support the retract semantic.</p>"},{"location":"architecture/cookbook/#change-data-capture","title":"Change Data Capture","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#late-data","title":"Late Data","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#exactly-once","title":"Exactly once","text":""},{"location":"architecture/cookbook/#query-evolution","title":"Query Evolution","text":"<p>In this section, I address streaming architecture only, integrated with Kafka, most likely CDC tables and sink connectors, a classical architecture, simplified in this figure:</p> <p>Once a Flink query is deployed and run 'foreever', how to change it? to fix issue or adapt to schema changes. </p> <p>By principles any Flink DAG code is immutable, so statement needs to be stopped and a new version started! This is not as simple as this as there will be impact to any consumers of the created data, specially in Kafka topic or in non-idempotent consumers. </p> <p>The Flink SQL statements has limited parts that are mutables. See the Confluent Cloud product documentation for details.  In Confluent Cloud the principal name and compute pool metadata are mutable when stopping and resuming the statement. Developers may stop and resume a statement using Console, CLI, API or even Terraform scripts.</p> <p>Here are example using confluent cli:</p> <pre><code># $1 is the statement name\nconfluent flink statement stop $1 --cloud $(CLOUD) --region $(REGION) \n\nconfluent flink statement resume $1 --cloud $(CLOUD) --region $(REGION) \n</code></pre> <p>When a SQL statement is started, it reads the source tables from the beginning (or any specified offset) and the operators, defined in the statement, build their internal state. Source operators use the latest schema version for key and value at the time of deployment. There is a snapshot of the different dependency configuration saved for the statement: the reference to the dependants tables, user-defined functions... Any modifications to these objects are not propagated to running statement.</p> <p>First let review the schema definition evolution best practices for Flink processing.</p>"},{"location":"architecture/cookbook/#schema-compatibility","title":"Schema compatibility","text":"<p>The CDC component may create schema automatically reflecting the source table. See Debezium documentation about schema evolution</p> <p>CC Flink works best when consuming topics with FULL_TRANSITIVE compatibility mode. The following table lists the schema compatibility types, with the Flink impacts:</p> Compatibility type Change allowed Flink impact BACKWARD Delete fields, add optional field Does not allow to replay from earliest BACKWARD_TRANSITIVE Delete fields, add optional fields with default value. Require all Statements reading from impacted topic to be updated prior to the schema change. FORWARD Add fields, delete optional fields Does not allow to replay from earliest FORWARD_TRANSITIVE Add fields, delete optional fields Does not allow to replay from earliest FULL Add optional fields, delete optional fields Does not allow to replay from earliest FULL_TRANSITIVE Add optional fields, delete optional fields Reprocessing and bootstrapping is always possible: Statements do not need to be updated prior to compatible schema changes. Compatibility rules &amp; migration rules can be used for incompatible changes. NONE All changes accepted Very risky. Does not allow to replay from earliest <p>With FULL_TRANSITIVE, old data can be read with the new schema, and new data can also be read between the schemas X, X-1, and X-2. Which means in a pipeline, downstream Flink statements can read newly created schema, and will be able to replay messages from a previous schemas. Therefore, you can upgrade the producers and consumers independently.</p> <p>Optional means, we can define fields with default values. Added fields are ignored by running statements.</p> <p>Confluent Schema Registry supports Avro, Json or Protobuf, but Avro was designed to support schema evolution, so this is the preferred approach.</p> <p>Assuming the goal is to cover FULL_TRANSITIVE, it means added column will have default value. Developers may only delete optional columns. During the migration, all current source tables coming as outcome of the Debezium process are not deletable. If a NOT NULL constraint is added to a column, older records with no value will violate the constraint. To handle such situation, there is a way to configure CDC connector (Debezium) to use the <code>envelop structure</code> which uses <code>key, before, after, _ts_ms, op</code> structure. This will result in a standard schema for Flink tables, which allows the source table to evolve by adding / dropping both NULL &amp; NON NULL COLUMNS.  The Debezium Envelope pattern offers the most flexibility for upstream Schema Evolution without impacting downstream Flink Statements.</p> <ul> <li>Adding Columns do not break the connectors or the RUNNING DML Statements</li> <li>Dropping Column needs co-ordination between teams to ensure the columns that are part of Running DML statements are not Dropped.</li> </ul> Key schema evolution <p>It is discouraged from any changes to the key schema in order to do not adversely affect partitioning for the new topic/ table.</p> Updating schema in schema registry <p>Adding a field directly in the avro-schema with default value, will be visible in the next command like: <code>show create table &lt;table_name&gt;</code>, as tables in flink are virtuals, and the Confluent Cloud schema definition comes from the Schema Registry. The RUNNING Flink DML is not aware of the new added column. Even STOP &amp; RESUME of the Flink DML is not going to pick the new column neither. Only new statement will see the new schema consuming from the beginning or from specific offsets. For column drop, Debezium connector will drop the new column and register a schema version for the topic (if the alteration resulted in a schema that doesnt match with previous versions). Same as above runnning statements will go degraded mode.</p>"},{"location":"architecture/cookbook/#statement-evolution","title":"Statement evolution","text":"<p>The general strategy for query evolution is to replace the existing statement and the corresponding tables it maintains with a new statement and new tables. The process is described in this product chapter and should be viewed within two folds depending of stateless or statefule statements. The initial state of the process involves a pipeline of Flink SQL statements and consumers processing Kafka records from various topics. We assume that the blue records are processed end-to-end, and the upgrade process begins at a specific point, from where all green records to be processed. The migration should start with the Flink statement that needs modification and proceed step by step to the statement creating the sink.</p> <p>For stateless statement evolution the figure below illustrates the process:</p> <p></p> <p>Figure: Stateless schema evolution process</p> <ol> <li>The blue statement, version 1, is stopped, and from the output, developer gets the last offset</li> <li>The green statement includes a DDL to create the new table with schema v2.</li> <li>The green DML statement, version 2, is started from the last offset or timestamp and produces records to the new table/topic</li> <li>Consumers are stopped. They have produced so far records to their own output topic with source from blue records.</li> <li>Consumers restarted with the new table name. Which means changing their own SQL statements, but now producing output with green records as source.</li> </ol> <p>At a high level, stateless statements can be updated by stopping the old statement, creating a new one, and transferring the offsets from the old statement. As mentioned, we need to support FULL TRANSITIVE updates to add or delete optional columns/fields.</p> <p>For stateful statements, it is necessary to bootstrap from history, which the below process accomplishes:</p> <p></p> <p>Figure: Stateful schema evolution process</p> <p>The migration process consists of the following steps:</p> <ol> <li>Create a DDL statement to define the new schema for <code>table_v2</code> which means topic v2.</li> <li>Deploy the new statement with the v2 name, starting from the earliest records to ensure semantic consistency for the blue records, when they are stateful, or from the last offset when stateless.</li> <li>Once the new statement is running, it will build its own state and continue processing new records. Wait for it to retrieve the latest messages from the source tables before migrating existing consumers to the new table v2. The old blue records will be re-emitted. While the new statement </li> <li>Stop processing first statement.</li> <li>Halt any downstream consumers, retaining their offsets if those are stateless or idempotent, if they are stateful they will process from the earliest.</li> <li>Reconnect the consumers to the new table. For Flink statements acting as consumers, they will need to manage their own upgrades. To avoid duplicates or not missing records, the offset for the consumers to the new topic needs ot be carrefuly selected.</li> <li>Once all consumers have migrated to the new output topic, the original statement and output topic can be deprovisioned.</li> </ol> <p>This process requires to centrally control the deployment of those pipeline.</p> <p>Using Open Source Flink, creating a snapshot is one potential solution for restarting. However, if the DML logic has changed, it may not be possible to rebuild the DAG and the state for a significantly altered flow. Thus, in a managed service, the approach is to avoid using snapshots to restart the statements.</p> <p>Also when the state size is too large, consider separating the new statement into a different compute pool than the older one.</p>"},{"location":"architecture/cookbook/#restart-a-statement-from-a-specific-time-or-offset","title":"Restart a statement from a specific time or offset","text":"<p>Using time window, it may be relevant to restart from the beginning of the time window when the job was terminated. Which means using a <code>WHERE event_time &gt; window_start_time_stamp</code> to get the records from the source table for an aligned time.</p> <p>Use the <code>/*+ options</code> at the table level or a <code>set statement</code> for all the table to read from.</p> <pre><code>FROM orders /*+ OPTIONS('scan.startup.mode' = 'timestamp', 'scan.startup.timestamp-millis' = '1717763226336') */\n</code></pre> <p>For offset, the <code>status.latest_offsets</code> includes the lastest offset read for each partition. Note that reading from an offset is applicable only for stateless statements to ensure exactly-once delivery. or</p> <pre><code>SET `sql.tables.scan.startup.mode`= \"earliest\"\n</code></pre> <p>TO CONTINUE</p>"},{"location":"architecture/cookbook/#change-stateful-statement","title":"Change stateful statement","text":"<p>We have seen the stateful processing leverages checkpoints and savepoints. With the open source Flink, developers need to enable checkpointing and manually triggering a savepoint when they need to restart from a specific point in time.</p> <p>Deploy the new statement to compute the stateful operation, and use a template like the following. Then stop the first statement.</p> <pre><code>create table table_v2\nas \nselect * from table_v1\nwhere window_time &lt;= a_time_stamp_when_stopped\nunion \nselect * from (tumble table)\nwhere the $rowtime &gt; a_time_stamp_when_stopped\norder by window_time\n</code></pre> <p>It is possible to do an in-place upgrade if the table uses primary key.</p> <p>Restarting a job is not a retry mechanism but a fault tolerance one. Normally only cluster level issues should ever cause a job to restart. When doing Java or Python application, developers need to do not throw exceptions within the main function but handle them and perform retries, backoff, and loop forever. as part of the exception management it is important to provide diagnostic data to administrators.</p> <p>When integrated with Kafka, networking latency may trigger losing connection, or some user errors like deleting the cluster, a topic, or a connector, may lead the Flink job getting in retry loop. </p> <p>Savepoints are manually triggered snapshots of the job state, which can be used to upgrade a job or to perform manual recovery.</p> <p>Full checkpoints and savepoints take a long time, but incremental checkpoints are faster.</p>"},{"location":"architecture/cookbook/#demonstrate-in-place-upgrade-of-stateless-statement","title":"Demonstrate in-place upgrade of stateless statement","text":""},{"location":"architecture/cookbook/#demonstrate-stateful-statement-upgrade","title":"Demonstrate stateful statement upgrade","text":""},{"location":"architecture/cookbook/#flink-on-kubernetes","title":"Flink on Kubernetes","text":"<ul> <li>To trigger a savepoints</li> <li></li> </ul>"},{"location":"architecture/cookbook/#testing-sql-statement-during-pipeline-development","title":"Testing SQL statement during pipeline development","text":"<p>We should differentiate two types of testing: Flink statement developer testing, like unit / component tests, and integration tests with other tables and with real data streams.</p> <p>The objective of a test harness for developer and system integration is to validate the quality of a new Flink SQL statement deployed on Confluent Cloud (or Flink managed service) to address at least the following needs:</p> <ol> <li>be able to deploy a flink statement (the ones we want to focus on are DMLs, or CTAS)</li> <li>be able to generate test data from schema registry - and with developer being able to tune test data for each test cases.</li> <li>produce test data to n source topics, consumes from output topic and validates expected results. All this flow being one test case. This may be automated for non-regression testing to ensure continuous quality.</li> <li>support multiple testcase definitions</li> <li>tear done topics and data.</li> </ol> <p>The following diagram illustrates the global infrastructure deployment context:</p> <p></p> <p>The following diagram illustrates the target unit testing environment:</p> <p></p> <ul> <li>The Kafka Cluster is a shared cluster with topics getting real-time streams from production</li> </ul>"},{"location":"architecture/cookbook/#measuring-latency","title":"Measuring Latency","text":"<p>The goal is to measure the end-to-end data latency, which is the time from when data is created at its source to when it becomes available for end-user consumption. In any Apache Flink solution, especially those with chained Flink jobs, measuring this latency is a critical and planned activity within the deployment process.</p> <p>For example, consider a typical real-time data processing architecture. We'll use this architecture to illustrate how to measure and represent latency throughout the data flow:</p> <p>The following timestamps may be considered:</p> <ol> <li>UpdatedDate column in the transactional database may be used to know when a record was created / updated, this will serve to measure end-to-end latency. </li> <li>Source CDC connector like Debezium, may add a timestamp in their message envelop that could be used, if injected in the messages to measure messaging latency.</li> <li>Within the Flink processing, event time, may be mapped to the Kafka Topic record time. Ths could be used to measure Flink pipeline latency.</li> </ol> <p>It is important to note that Flink latency results may seem inconsistent. This is normal, and due to the semantic of the Flink processing. To ensure exactly-once delivery, Flink uses transactions when writing messages into Kafka. It is part of a consume-process-produce pattern, and adds the offset of the messages it has consumed to the transaction. </p> <p>Flink persists its state, including the offsets of the Kafka source, via checkpoints, which are done once per minute. The frequency may be configured. Queries submitted through Confluent Cloud Flink SQL Workbench or inspecting the content of a topic in the console generate output based on <code>read_uncommitted</code> isolation. The latency may seem reasonalbe during iterative development in the console, it may increase during more rigorous tests that use a read_committed consumer.</p> <p>Consumer reading committed messages will observe latency (consumer's property of <code>isolation.level= read-committed</code>). Recall that, when a consumer starts up, or when a partition is newly assigned to it, the consumer will check the <code>__consumer_offsets</code> topic to find the latest committed offset for its consumer group and partition. It will then begin reading messages from the next offset. Consumer will wait and not advance its position until the transaction is either committed or aborted. This ensures it never sees messages from aborted transactions and only sees a complete, consistent set of records. </p> <p>When producer iniates a transaction, and writes messages to topic/partition, those messages are not yet visible to consumers configured to read committed data, when there is no error, the producer commits the transaction. </p> <p>When consumer applications may tolerate at-least once semantics, they may simply configure all consumers with <code>read_uncommitted</code> isolation, at the risk that during failure recovery and scaling activities, the Flink statement will reingest messages from the last checkpoint, causing duplicates and time-travel for end consumers.</p> Flink transaction process <p>Flink's core mechanism for fault tolerance is checkpointing. It periodically takes a consistent snapshot of the entire application state, including the offsets of the Kafka source and the state of all operators.</p> <ul> <li>Flink's Kafka sink connector uses a two-phase commit protocol.</li> <li>When a checkpoint is triggered, the Flink Kafka producer (the sink) writes any pending data to Kafka within a transaction. It then prepares to commit this transaction but waits for a signal from the Flink JobManager.</li> <li>Once the JobManager confirms that all operators have successfully snapshotted their state, it tells the Kafka producer to commit the transaction. This makes the new data visible to read_committed consumers.</li> <li>If any part of the checkpoint fails (e.g., a Flink task manager crashes), the transaction is aborted. The uncommitted messages become \"ghost\" records on the Kafka topic, invisible to read_committed consumers. When the Flink job restarts, it will restore its state from the last successful checkpoint and reprocess the data from that point, avoiding data loss or duplicate.</li> </ul> <p>The shift_left utility has an integration tests harness feature to do end-to-end testing with timestamp.</p>"},{"location":"architecture/cookbook/#other-sources","title":"Other sources","text":"<p>The current content is sourced from the following cookbooks and lesson learnt while engaging with customers.</p> <ul> <li>Confluent Flink Cookbook</li> <li>Ververica Flink cookbook</li> </ul>"},{"location":"architecture/fitforpurpose/","title":"Fit for purpose","text":""},{"location":"architecture/fitforpurpose/#difference-between-kafka-streams-and-flink","title":"Difference between Kafka Streams and Flink","text":"<ul> <li>Flink is a complete streaming computation system that supports HA, Fault-tolerance, self-monitoring, and a variety of deployment models.</li> <li>Kafka Streams is a library that any  standard Java application can embed and hence does not attempt to dictate a deployment method</li> <li>Kafka Streams within k8s will provide horizontal scaling. But it is bounded by the number of partitions. Resilience is ensured with Kafka topics.</li> <li>In term of application Life Cycle:<ul> <li>Flink: User\u2019s stream processing code is deployed and run as a job in the Flink cluster</li> <li>Kakfa Streams: User\u2019s stream processing code runs inside Java application</li> </ul> </li> <li>Flink supports data at rest or in motion, and multiple sources and sinks, no need to be only Kafka as KStream.</li> <li>Flink has Complex Event Processing capabilities to search for pattern of event occurences.</li> <li>Restorate State after Failure<ul> <li>Flink can restore state after failure from most recent incremental snapshot</li> <li>KStreams and KSQL Restore state after failure by replaying all messages </li> </ul> </li> <li>Coordination<ul> <li>Flink JobManager is part of the streaming application and orchestrate task manager. Job manager orchestration is done via Kubernetes scheduler.</li> <li>KStreams - Leverages the Kafka cluster for coordination, load balancing, and  fault-tolerance.</li> </ul> </li> <li>Bounded and unbounded data streams       &amp; Flink: Stream or Batch processing on Bounded<ul> <li>Kstreams: Stream only</li> </ul> </li> <li> <p>Language Flexibility</p> <ul> <li>Flink has a layered API - with most popular languages being Java, Python and SQL</li> <li>KStreams is Java only.</li> </ul> </li> <li> <p>Flink needs a custom implementation of <code>KafkaDeserializationSchema&lt;T&gt;</code> to read both key and value from Kafka topic.</p> </li> <li>Kafka streams is easier to define a pipeline for Kafka records and to do the <code>consume - process - produce</code> loop. </li> <li>KStreams uses the Kafka Record time stamp, while with Flink we need to implement how to deserialize the KafkaRecord and get the timestamp from it.</li> <li>Support of late arrival is easier with KStreams, while Flink uses the concept of watermark.</li> </ul>"},{"location":"architecture/fitforpurpose/#when-to-use-rule-engine-versus-flink","title":"When to use rule engine versus Flink","text":"<p>By rule engine, we are talking about libraries / products that are implementing the Rete Algorithm and extends from there.  Some of those engines are also supporting time windowing operators.  The major use case is to implement prescriptive logic based on <code>if ... then ...else</code> constructs and define the knowledge base as a set of rules.  This is the base of expert systems and it was part of the early years of Artificial Intelligence.  Expert systems have still their role in modern IT and AI solution. They help to:</p> <ul> <li>automate human's decisions as an expert will do. In fact it is better to say like a worker will apply his/her decisions on data and still be involved in addressing the more difficult decisions.</li> <li>have a clear understanding of the logic executed behind a decision, which is a real challenge in AI and deep learning models.</li> <li>reprocess rules when new facts are added so rule engine can be used to maintain a conversation with the client application  to enrich facts and take decision</li> <li>externalize the business logic from code:  it is easier to test and help to develop what-if scenarios with champion and challenger decision evaluation methodology</li> </ul> <p>Flink can do Complex Event Processing and Stream processing with time windowing.</p> <p>The technologies are indeed complementary: if we consider to get a stream of events from a event backbone like Kafka and then process those events with Flink we can also call a remote decision service via REST end point within the flink flow. </p> <p></p> <p>The figure above illustrates a generic processing, where event sources are injecting events to Kafka topics, Flink application processes the events as part of a situation detection pattern.  The situation detection is supported by the Flink processing and the rule engine: the responsability to implement the complex time windowing logic is assigned to a Developer, while the business logic to support scoring or assessing best action, may be done by business analysts using a high level rule language and a decision management platform.  It is important to note that once a situation is detected, it is important to publish it as a fact in a Kafka topic, to adopt an event sourcing and event-driven architecture approach.  The down stream processing is to compute the next best action. This component can enrich the data from the situation event received, so the best action decision can consider more data elements. This is a classical approach to develop rule based application. </p> <p>Once the action is decided, it is published to a topic, and this orchestration service (named here \"entity service\") may call different external services, like a business process execution environment, and robot process automation,...</p> <p>Another effective way is to embed the rule engine and the ruleset inside the Flink application:</p> <p></p> <p>The goal is to reduce latency and avoid unnecessary remote calls which adds complexity with retries, circuit breaker and fail over.</p>"},{"location":"architecture/flink-sql/","title":"Flink SQL","text":"Updates <ul> <li>Created 02/2021 </li> <li>Updated 12/20/24</li> </ul>"},{"location":"architecture/flink-sql/#introduction","title":"Introduction","text":"<p>Flink SQL is an ANSI-compliant SQL engine designed for processing both batch and streaming data on distributed computing servers managed by Flink.</p> <p>Built on Apache Calcite, Flink SQL facilitates the implementation of SQL-based streaming logic. It utilizes the Table API, a language-integrated query API for Java, Scala, and Python that enables the composition of queries using relational operators such as selection, filtering, and joining.</p> <p>The Table API efficiently handles both bounded and unbounded streams within a unified and highly optimized ecosystem inspired by traditional databases and SQL.</p> <p>Both the Table API and SQL operate on top of a lower-level stream operator API, which runs on the dataflow runtime:</p> <p></p> <p>The optimizer and planner APIs transform SQL statements for execution across distributed nodes, leveraging the lower-level stream operator API. With Flink SQL, developers work with dynamic tables, a concept similar to materialized views in DB, while abstracting away the stream construct from the developers. Developers may write SQL and use the Table API in Java, Scala, or Python, or leverage the SQL client, an interactive tool for submitting SQL queries to Flink and visualizing the results.</p> <p>Streams or bounded data are mapped to Tables. The following command loads data from a csv file and creates a dynamic table in Flink:</p> <pre><code>CREATE TABLE car_rides (\n    cab_number VARCHAR,\n    plate VARCHAR, \n    cab_type VARCHAR,\n    driver_name VARCHAR, \n    ongoing_trip VARCHAR, \n    pickup_location VARCHAR, \n    destination VARCHAR, \n    passenger_count INT\n) WITH ( \n    'connector' = 'filesystem',\n    'path' = '/home/flink-sql-quarkus/data/cab_rides.txt',\n    'format' = 'csv'\n);\n</code></pre> <p>Show how the table is created:</p> <pre><code>show create table orders;\n</code></pre> <p>See the getting started to run Flink open-source locally with a sql client connected to a Job Manager and Task manager running in container.</p>"},{"location":"architecture/flink-sql/#main-use-cases","title":"Main use cases","text":"<p>Flink and Flink SQL can be used in two main categories of application:</p> <ol> <li>Reactive application, event-driven function</li> <li>Data products with real-time white box ETL pipeline: schematizing, cleaning, enriching for data lake, lake house, feature store or vector store. </li> </ol>"},{"location":"architecture/flink-sql/#parallel-with-database","title":"Parallel with Database","text":"<p>Database applications are typically classified into two domains: Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP), which are used for business reporting.</p> <p>Databases consist of catalogs, databases, tables, views, and materialized views. The most critical component is the query processor, which receives queries, plans their execution using metadata from the catalog (including information about tables and functions), and then executes the query via the storage engine to access the data and generate results.</p> <p>Views are virtual tables derived from the results of SQL queries. Some databases also support materialized views, which cache the results in a physical table. For example, a GROUP BY operation on an aggregate can store the results based on grouping elements and aggregates in a new table. Materialized views can be updated through a full refresh (by re-executing the query) or through incremental updates.</p> <p>Flink SQL utilizes dynamic tables derived from data streams and employs materialized views with incremental updates. While it is not a traditional database, Flink functions as a query processor.The processor runs continuous queries. In Confluent Cloud, the catalog accesses the schema registry for a topic, and query execution occurs on a Flink cluster that retrieves records from topics.</p> <p>Flink can provide either \"exactly once\" or \"at least once\" guarantees (with the possibility of duplicates), depending on the configuration and the external systems used for input and output tables.</p> <p>For effective \"exactly once\" processing, the source must be replayable, and the sink must support transactions. Kafka topics support both of these requirements, and the consumer protocol adheres to the read-committed semantics. However, transaction scope in Kafka is at the single-key level, whereas ACID transactions in databases maintain integrity across multiple keys.</p>"},{"location":"architecture/flink-sql/#changelog-mode","title":"Changelog mode","text":"<p>When mapping table view to stream, a query can generate two different type of messages: 1/ containing insert, update or delete changes or 2/ only insert changes. The changelog normalize function allows Flink to track the precise sequence of changes to data, which is essential for all stateful operations. In Kafka, records provide the current state of the records without information of what changed. In Flink, without this state tracking, any stateful operation would produce incorrect or inconsistent results when source records are updated or deleted, as operators would lack the context of how the data has changed. </p> State size <p>Queries that make update changes, usually have to maintain more state. See Flink table to stream conversion documentation.</p> <p>There are three different modes to persist table rows in a append logm like a Kafka topic: append, retract or upsert. </p> <ul> <li>append is the simplest mode where records are only added to the result stream, never updated or retracted. It means that every insertion can be treated as an independent immutable fact. Records can be distributed using round robin to the different partitions. Do not use primary key with append, as windowing or aggregation will produce undefined, and may be wrong, results.  Temporal join may be possible. Some query will create append output, like window aggregation, or any operations using the watermark.</li> <li>upsert means that all rows with the same primary key are related and must be partitioned together. Events are only Upsert or Delete for a primary key. Upsert mode needs a primary key. </li> <li>retract means a fact can be undone, The stream includes only add (+X) or retract (-X) messages. One update is represented by a -X followed by a +X messages. The combination of +X and -X are related and must be partitioned together. Records are related by all the columns so the entire row is the key.</li> </ul> <p>The <code>change.log</code> property is set up by using the <code>WITH ('changelog.mode' = 'upsert')</code> options when creating the table.</p> <p>Some operations, such as group by, aggregation and deduplication can produce update events. Use the <code>EXPLAIN</code> feature to analyze the physical execution plan of a query to see the changelog mode of each operator.</p> <p>Looking at the physical plan with <code>EXPLAIN create...</code> demonstrates the changelog mode and the state size used per operator.</p> <p>See the concept of changelog and dynamic tables in Confluent's documentation and see this example to study the behavior with a kafka topic as output, and a SQL coding - changelog mode section.</p>"},{"location":"architecture/flink-sql/#sql-programming-basics","title":"SQL Programming Basics","text":"<p>Flink SQL tables are dynamic, meaning they change over time; some tables act more like changelog streams than static tables.</p> <p>The following diagram illustrates the main processing concepts: the <code>shipments</code> table tracks product shipments, while the <code>inventory</code> table maintains the current quantity of each item. The INSERT statement processes streams to update the inventory based on new shipment records. This SQL statement uses the SUM aggregation operation to count each item, with the items shuffled to group them by type.</p> <p></p> <p>SQL is applied directly to the stream of data; data is not stored within Flink. Events can represent INSERT, UPDATE, or DELETE operations in the table. The diagram shows that, at the sink level, the initial events reflect the addition of items to the inventory. When the inventory for the \"Card\" item is updated, a record is first created to remove the current stock of the \"Card\" and then a new message is sent with the updated stock value (2 cards). This behavior arises from the GROUP BY semantics, where the right table is an update-only table while the left is append-only. Dynamic tables can also be persisted in Kafka topics, meaning the table definition includes statements on how to connect to Kafka. In batch processing, the sink can be a database table or a CSV file in the filesystem. </p> <p>Note that the SQL Client executes each INSERT INTO statement as a separate Flink job. The STATEMENT SET command can be used to group multiple insert statements into a single set. As the job manager schedules these jobs to the task managers, SQL statements are executed asynchronously. For batch processing, developers can set the set <code>table.dml-sync</code> option to <code>true</code>.</p> <p>In streaming, the \"ORDER BY\" statement applies only to timestamps in ascending order, while in batch processing, it can be applied to any record field.</p>"},{"location":"architecture/flink-sql/#data-lifecycle","title":"Data lifecycle","text":"<p>In a pure Kafka integration architecture, such as Confluent Cloud, the data lifecycle follows these steps:</p> <ul> <li>Data is read from a Kafka topic to a Flink SQL table.</li> <li>Data is processed using Flink SQL statements.</li> <li>Results are returned as result sets in interactive mode, or to a table (mapped to a topic) in continuous streaming mode.</li> </ul>"},{"location":"architecture/flink-sql/#some-sql-operators","title":"Some SQL operators","text":"Type Operators Comments Stateless SELECT {projection, transformation} WHERE {filter}; UNION ..., CROSS JOIN UNNEST or CROSS JOIN LATERAL Can be distributed Materialized GROUP BY , OVER, JOINS or MATCH_RECOGNIZE Dangerously Stateful, keep an internal copy of the data related to the query Temporal Time windowed operations, interval joins, time-versioned joins Stateful but constrained in size <p>As elements are stored for computing materialized projections, it's crucial to assess the number of elements to retain. Millions to billions of small items are possible. However, if a query runs indefinitely, it may eventually overflow the data store. In such cases, the Flink task will ultimately fail.</p> <p>In a join any previously processed records can be used potentially to process the join operation on new arrived records, which means keeping a lot of records in memory. As memory will be bounded, there are other mechanisms to limit those joins or aggregation, for example using time windows.</p>"},{"location":"architecture/flink-sql/#challenges","title":"Challenges","text":"<ul> <li>We cannot express everything in SQL but we can mix Flink DataStream and Table APIs</li> </ul>"},{"location":"architecture/flink-sql/#flink-sql-high-level-faq","title":"Flink SQL High Level FAQ","text":""},{"location":"architecture/flink-sql/#why-the-watermark-is-set-7-days-in-the-past","title":"Why the watermark is set 7 days in the past?","text":"<p>Creating a table with the $rowtime as watermark like:</p> <p><pre><code>user_id STRING  NULL,\n-- more columns      \n$rowtime    TIMESTAMP_LTZ(3) *ROWTIME*  NOT NULL    METADATA VIRTUAL, WATERMARK AS `SOURCE_WATERMARK`() SYSTEM\n</code></pre> and then checking the watermark value with something like:</p> <p><pre><code>SELECT *, CURRENT_WATERMARK($rowtime) AS current_watermark, $rowtime FROM `table_name`;\n</code></pre> the current_watermark is 7 days behind. </p> <p>Altering the table to use $rowtime as watermark addresses the issues. But the default strategy is to emit the first watermark 7 days in the past, it not enough records (1000 records in each partition) are emitted. </p> <p>Time and Watermarks video</p>"},{"location":"architecture/flink-sql/#how-we-are-going-to-setup-full-load-of-a-table","title":"How we are going to setup full load of a table?","text":"<p>For Flink SQL stream processing, we assume raw-topics are created from a change data capture systems and continuously write records to the topics. When history is important the raw-topic content become the implementation of the event-sourcing patttern and we need to be able to reprocess from the earliest offset. So when defining the table view associated to this topic, using Flink Kafka connector, the CREATE TABLE has, in the WITH section, a parameter to set the kafka reading strategy:      <pre><code>CREATE TABLE raw_user (\n      `user_id` BIGINT,\n      `name` STRING\n)\nWITH (\n      'scan.startup.mode' = 'earliest-offset',\n)\n</code></pre></p> <p>In Confluent Cloud the connector set this property to the earliest by default. Altering the table can change to the latest-offset.</p>"},{"location":"architecture/flink-sql/#how-we-are-going-to-merge-the-reference-data-and-cdc-data","title":"How we are going to merge the reference data and CDC data?","text":"<p>This is a standard left or right joins on the fields needed to identify records on both table. The reference data in the context of Kafka is a topic with retention set to infinite, and will all the records in one partition, sorted or not. In Flink a primary key may be defined in the reference table.</p>"},{"location":"architecture/flink-sql/#deeper-dive","title":"Deeper dive","text":"<ul> <li>SQL and Table API overview</li> <li>Table API</li> <li>Introduction to Apache Flink SQL by Timo Walther</li> <li> <p>Flink API examples presents how the API solves different scenarios:</p> <ul> <li>as a batch processor,</li> <li>a changelog processor,</li> <li>a change data capture (CDC) hub,</li> <li>or a streaming ETL tool</li> </ul> </li> </ul>"},{"location":"architecture/kafka/","title":"Integration with Kafka","text":"<p>Flink has a Kafka connector for consuming and producing messages.  We need the connector jar, define Kafka server properties and then define the source for the stream.</p>"},{"location":"architecture/kafka/#consuming-from-kafka","title":"Consuming from Kafka","text":"<p>So the product documentation is wrong (03/2021): here are some notes and read the code under kafka-flink-demo folder and the TelemetryAggregate class.</p> <p>The major change is to implement a DeserializerSchema to process the event as java bean: (See code)</p> <pre><code>public class TelemetryDeserializationSchema implements DeserializationSchema&lt;TelemetryEvent&gt; {\n\n    private static final long serialVersionUID = -3142470930494715773L;\n    private static final ObjectMapper objectMapper = new ObjectMapper();\n\n    @Override\n    public TelemetryEvent deserialize(byte[] message) throws IOException {\n        return objectMapper.readValue(message, TelemetryEvent.class);\n    }\n\n    @Override\n    public boolean isEndOfStream(TelemetryEvent nextElement) {\n        return false;\n    }\n\n    @Override\n    public TypeInformation&lt;TelemetryEvent&gt; getProducedType() {\n        return TypeInformation.of(TelemetryEvent.class);\n    }\n\n}\n</code></pre> <p>Three serializers exist: TypeInformationSerializationSchema, JsonDeserializationSchema, AvroDeserializationSchema.</p> <p>The creation of the Kafka data source uses the traditional Kafka properties, and the following construct:</p> <pre><code>// better to use a separate class for config to be injected via CDI\nKafkaConfiguration configuration = new KafkaConfiguration();\n\nProperties kafkaProperties = configuration.getConsumerProperties(\"telemetryAggregators\");\n\nFlinkKafkaConsumer&lt;TelemetryEvent&gt; kafkaConsumer = new FlinkKafkaConsumer&lt;TelemetryEvent&gt;(configuration.mainTopicName, new TelemetryDeserializationSchema(), kafkaProperties);\nkafkaConsumer.setStartFromEarliest();\n\nDataStream&lt;TelemetryEvent&gt; telemetryStream = env.addSource(kafkaConsumer);\n</code></pre> <p>If no checkpointing is enabled then the consumer will periodically commit the offsets to Kafka (set <code>enable.auto.commit</code> and <code>auto.commit.interval.ms</code>) . It does not rely on the committed offsets for fault tolerance guarantees.</p> <p>With Flink\u2019s checkpointing enabled, the Flink Kafka Consumer will consume records from a topic and periodically checkpoint all its Kafka offsets, together with the state of other operations.  When the checkpoints are completed then it will commit offsets to kafka. In case of a job failure, Flink will restore the streaming program to the state of the latest  checkpoint and re-consume the records from Kafka, starting from the offsets that were stored  in the checkpoint.</p> <p></p> <p>But when it reprocesses the records again it will generate duplicate at the consumer level. </p> <p></p> <p>Therefore the Sink connector needs to support transactional producer, and uses the producer API to support avoid duplication with transaction id, idempotence and acknowledge on all replicas:</p> <p></p> <p>Partition discover should be enable by properties so Flink job can discover newly added partitions.</p>"},{"location":"architecture/kafka/#timestamp-and-watermark","title":"TimeStamp and Watermark","text":"<p>Timestamp can be in the ConsumerRecord or in the payload itself. so the app needs to specify how to extract the timestamp to be used for time window logic.</p>"},{"location":"coding/cep/","title":"Complex event processing","text":"<p>The goal of CEP is to analyzing pattern relationships between streamed events. Complex processing can be done using Flink using  three capabolities: </p> <ul> <li>Stateful Function</li> <li>Flink CEP is a library to assess event pattern within a data stream.</li> <li>Flink SQL </li> </ul>"},{"location":"coding/cep/#use-cases","title":"Use cases","text":"<ul> <li>Real-time marketing</li> <li>Anomalies detection</li> <li>Financial apps to check the trend in the stock market.</li> <li>Credit card fraud detection.</li> <li>RFID based tracking and monitoring systems</li> </ul>"},{"location":"coding/cep/#concepts","title":"Concepts","text":"<p>It uses the pattern API to define complex pattern sequences  that we want to extract from the input stream.</p> <p>The events in the DataStream to which you want to apply pattern matching must implement proper <code>equals()</code> and <code>hashCode()</code> methods because FlinkCEP uses them for comparing and matching events.</p> <p>The approach is to define simple patterns and then combine them in pattern sequence. </p> <p>A pattern can be either a singleton (accept a single event) or a looping pattern (accept more than one).</p> <p>Here is a generic pattern to illustrate the following sequencing of events like: A B* C</p> <pre><code>Pattern\n        .begin(\"A\").where(/* conditions */)\n        .next(\"B\").oneOrMore().optional().where(/* conditions */)\n        .next(\"C\").where(/* conditions */)\n</code></pre> <p>Quantifier specificies the pattern type.  </p> <pre><code>pattern.oneOrMore()\npattern.times(#ofTimes)\npattern.times(#fromTimes, #toTimes)\n// expecting 1 or more occurrences and repeating as many as possible\npattern.oneOrMore().greedy();\n</code></pre> <p>Each pattern can have one or more conditions based on which it accepts events.</p> <pre><code>pattern.where() \npattern.or()\npattern.until()\n</code></pre> <p>With Iterative condition we can specify a condition that accepts subsequent events  based on properties of the previously accepted events or a statistic over a subset of them.  Iterative conditions can be powerful, especially in combination with looping patterns, e.g. <code>oneOrMore()</code>.</p> <p>We can combine patterns by specifying the desired contiguity conditions between them.</p> <p>FlinkCEP supports the following forms of contiguity between events:</p> <ul> <li>Strict Contiguity: Expects all matching events to appear strictly one after the other, without any non-matching events in-between (use <code>next()</code> function).</li> <li>Relaxed Contiguity: Ignores non-matching events appearing in-between the matching ones (use <code>followedBy()</code>): \"\u201cskip non-matching events till the next matching one\"</li> <li>Non-Deterministic Relaxed Contiguity: Further relaxes contiguity, allowing additional matches that ignore some matching events (use <code>followedByAny()</code>).</li> </ul>"},{"location":"coding/datastream/","title":"DataStreams Programming guidances and examples","text":"<p>This chapter is a set of links to existing examples for Flink DataStream.</p>"},{"location":"coding/datastream/#datastream-deeper-dive","title":"Datastream deeper dive","text":"<ul> <li>Open source documentation with the API.</li> <li>Confluent Flink Cookbook, is a set of recipes around Flink using DataStream. Once clone, load one of the folder as a java project in IDE.</li> </ul>"},{"location":"coding/datastream/#data-set-basic-apps","title":"Data set basic apps","text":"<p>Attention Scala and DataSet aPi do not exist anymore</p> <p>Deprecated from Flink 1.18.</p> <p>The examples directly in the my-flink project under the  jbcodeforce.p1 package:</p> <ul> <li>PersonFiltering.java filter a persons datastream using person's age to create a new \"adult\" output data stream. This example uses test data from a list of person and uses a filtering class which implements the filter method. This code can execute in VSCode or any IDE</li> <li>InnerJoin Proceed two files and do an inner join by using the same key on both files. See next section for details.</li> <li>LeftOuterJoin results will include matching records from both tuples and non matching from left (so person) (<code>personSet.leftOuterJoin(locationSet)</code>).</li> <li>RightOuterJoin matching records present in both data sets and non matching from the right.</li> <li>Full outer join when matching and non matching are present. See fulljoinout.csv output file.</li> <li>Traditional word count from a text uses a filter function to keep line starting by a pattern (letter 'N'), then it uses a tokenizer function to build a tuple for each word with a count of 1. The last step of the flow is to groupBy word and sum the element. Not obvious.</li> </ul>"},{"location":"coding/datastream/#inner-join","title":"Inner join","text":"<p>Need to read from two files and prepare them as tuples. Then process each record of the first tuple with the second one  using field 0 on both tuples as join key. The <code>with()</code> build the new tuple with combined values.  <code>with()</code> need a join function to implement the joining logic and attributes selection.</p> <pre><code> DataSet&lt;Tuple3&lt;Integer,String,String&gt;&gt; joinedSet = \n      personSet.join(locationSet)\n      .where(0) // indice of the field to be used to do join from first tuple\n      .equalTo(0)  // to match the field in idx 0 of the second tuple\n      .with( new JoinFunction&lt;Tuple2&lt;Integer, String&gt;, \n                              Tuple2&lt;Integer, String&gt;, \n                              Tuple3&lt;Integer, String, String&gt;&gt;() {\n\n          public Tuple3&lt;Integer, String, String&gt; join(Tuple2&lt;Integer, String&gt; person,  Tuple2&lt;Integer, String&gt; location)  {\n              return new Tuple3&lt;Integer, String, String&gt;(person.f0,   person.f1,  location.f1);\n          }              \n      });\n</code></pre> <p>Exec within the <code>JobManager</code> container.</p> <pre><code>flink run -d -c jbcodeforce.p1.InnerJoin /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar --persons file:///home/my-flink/data/persons.txt --locations file:///home/my-flink/data/locations.txt --output file:///home/my-flink/data/joinout.csv \n</code></pre>"},{"location":"coding/datastream/#left-outer-join","title":"Left outer join","text":"<p>The construct is the same as above, except the results will include matching records from both tuples and  non matching records coming from the left part of the join:</p> <pre><code> DataSet&lt;Tuple3&lt;Integer,String,String&gt;&gt; joinedSet = \n            personSet.leftOuterJoin(locationSet)\n            ....\n\n      public Tuple3&lt;Integer, String, String&gt; join(\n                        Tuple2&lt;Integer, String&gt; person,  \n                        Tuple2&lt;Integer, String&gt; location)  {\n          if (location == null) {\n              return new Tuple3&lt;Integer, String, String&gt;(person.f0, person.f1, \"NULL\");\n          }\n          return new Tuple3&lt;Integer, String, String&gt;(person.f0,   person.f1,  location.f1);\n      }  \n</code></pre>"},{"location":"coding/datastream/#data-stream-examples","title":"Data Stream examples","text":"<p>Data stream API is used to get real time data. It can come from file with readFile with watching folder for new file to be read, or use <code>socketTextStream</code> or any streaming source (addSource) like Twitter, Kafka...</p> <p>The output can also be a stream (as sink): writeAsText(),.. writeToSocket, addSink...</p> <p>See example in <code>my-flink</code> project source WordCountSocketStream, and to test it, use the <code>nc -l 9999</code> tool to open a socket on port 9999 and send text message.</p> <p>When using docker we need to open a socket in the same network as the Flink task manager, the command looks like:</p> <pre><code>docker run -t --rm --network  flink-studies_default --name ncs -h ncshost subfuzion/netcat -l 9999\n</code></pre>"},{"location":"coding/datastream/#compute-average-profit-per-product","title":"Compute average profit per product","text":"<p>The data set avg.txt represents transactions for a given product with its sale profit. The goal is to compute the average profit per product per month. </p> <p>The solution use Map - Reduce functions.</p> <ul> <li>Input sample:</li> </ul> <pre><code>01-06-2018,June,Category5,Bat,12\n01-06-2108,June,Category4,Perfume,10\n</code></pre> <ul> <li>Output:</li> </ul> <p>In the class datastream.ProfitAverageMR, the DataStream loads the input file as specified in  <code>--input</code> argument and then splits record to get columns as tuple attributes.</p> <pre><code> DataStream&lt;String&gt; saleStream = env.readTextFile(params.get(\"input\"));\n // month, product, category, profit, count\n DataStream&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; mappedSale = saleStream.map(new Splitter()); \n</code></pre> <p>The <code>Splitter</code> class implements a MapFunction which splits the csv string and select the attributes needed to generate the tuple.</p> <p>A first reduce operation is used on the sale tuple where the key is a month (output from GetMonthAsKey) to accumulating profit and the number of record:</p> <pre><code>DataStream&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; reduced = \n  mappedSale.keyBy(new GetMonthAsKey())\n  .reduce(new AccumulateProfitAndRecordCount()); \nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; profitPerMonth = reduced.map(new MapOnMonth());\n</code></pre> <p>here is the main reduce function: the field f3 is the profit, and f4 the number of sale.</p> <pre><code> public static class AccumulateProfitAndRecordCount implements ReduceFunction&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; {\n\n    private static final long serialVersionUID = 1L;\n    @Override\n    public Tuple5&lt;String, String, String, Integer, Integer&gt; reduce(\n            Tuple5&lt;String, String, String, Integer, Integer&gt; current,\n            Tuple5&lt;String, String, String, Integer, Integer&gt; previous) throws Exception {\n\n        return new Tuple5&lt;String, String, String, Integer, Integer&gt;(current.f0,current.f1,current.f2,current.f3 + previous.f3, current.f4 + previous.f4);\n    }\n}\n</code></pre> <p>To run the example once the cluster is started use:</p> <pre><code> docker exec -ti $JMC flink run -d -c jbcodeforce.datastream.ProfitAverageMR /home/my-flink/target/my-flink-1.0.0-runner.jar --input file:///home/my-flink/data/avg.txt \n</code></pre>"},{"location":"coding/datastream/#aggregates","title":"Aggregates","text":"<p>See all the operators examples in this note.</p> <p>Examples of aggregate API to compute min, max... using field at index 3</p> <pre><code>mapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).sum(3).writeAsText(\"/home/my-flink/data/out1\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).min(3).writeAsText(\"/home/my-flink/data/out2\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0) .minBy(3).writeAsText(\"/home/my-flink/data/out3\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).max(3).writeAsText(\"/home/my-flink/data/out4\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).maxBy(3).writeAsText(\"/home/my-flink/data/out5\");\n</code></pre>"},{"location":"coding/datastream/#taxi-rides-examples","title":"Taxi rides examples","text":"<p>This is a more complex solution with a lot of good inspirations for utilities class and way to work on Java Beans.</p> <p>See the flink-training github to access to the source code.</p> <ul> <li>Lab 1- filter non NY taxi rides, the process flow uses the <code>DataStream::filter</code> method. The NYCFilter is a class-filter-function.</li> </ul> <pre><code>DataStream&lt;TaxiRide&gt; filteredRides = rides\n    // keep only those rides and both start and end in NYC\n    .filter(new NYCFilter());\n// ...\n\npublic static class NYCFilter implements FilterFunction&lt;TaxiRide&gt; {\n    @Override\n    public boolean filter(TaxiRide taxiRide) {\n        return GeoUtils.isInNYC(taxiRide.startLon, taxiRide.startLat) &amp;&amp;\n                GeoUtils.isInNYC(taxiRide.endLon, taxiRide.endLat);\n    }\n}\n</code></pre> <p>This exercise uses a lot of utility classes for data and tests which hide the complexity of the data preparation  (see the common folder within the training repository).</p> <ul> <li>Process ride and fare data streams for stateful enrichment.  The result should be a DataStream&gt;, with one record for each distinct rideId.  Each tuple should pair the TaxiRide START event for some rideId with its matching TaxiFare.  There is no control over the order of arrival of the ride and fare records for each rideId. <pre><code>DataStream&lt;TaxiRide&gt; rides = env\n        .addSource(rideSourceOrTest(new TaxiRideGenerator()))\n        .filter((TaxiRide ride) -&gt; ride.isStart)\n        .keyBy((TaxiRide ride) -&gt; ride.rideId);\n\nDataStream&lt;TaxiFare&gt; fares = env\n        .addSource(fareSourceOrTest(new TaxiFareGenerator()))\n        .keyBy((TaxiFare fare) -&gt; fare.rideId);\n\n// Set a UID on the stateful flatmap operator so we can read its state using the State Processor API.\nDataStream&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; enrichedRides = rides\n        .connect(fares)\n        .flatMap(new EnrichmentFunction())\n        .uid(\"enrichment\");\n</code></pre> <p>The join and stateful implementation are done in the EnrichmentFunction as a <code>RichCoFlatMap</code>. A CoFlatMapFunction implements a flat-map transformation over two connected streams. The same instance of the transformation function is used to transform both of the connected streams. That way, the stream transformations can share state.</p> <p>RidesAndFaresSolution.java</p> <p><code>ValueState&lt;TaxiRide&gt; rideState</code> is a partitioned single-value state.</p> <p><code>flatMap1(TaxiRide ride, Collector&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; out)</code> method is called for each element in the first of the connected streams. So here on a ride event, if there is a matching fare already computed then generate the output tuple, if not update keep the ride to be used for the fare event processing.</p> <p><code>flatMap2(TaxiFare fare, Collector&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; out)</code> method is called on the second connected streams. When a fare event arrives, if there is a ride with the same key, join, if not keep the fare for future ride event.</p> <p>So one of the trick is in the ValueState class.</p> <ul> <li>Hourly tips is a time windowed analytics to identify, for each hour, the driver earning the most tips. The approach is to use hour-long windows that compute the total tips for each driver during the hour, and then from that stream of window results, find the driver with the maximum tip total for each hour.</li> </ul> <p>The first data stream below applies a window on a keyed stream. Process is one of the function to use on the window. (reduce and aggregate are the others). </p> <pre><code>    DataStream&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyTips = fares\n            .keyBy((TaxiFare fare) -&gt; fare.driverId)\n            .window(TumblingEventTimeWindows.of(Time.hours(1)))\n            .process(new AddTips());\n\n    DataStream&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyMax = hourlyTips\n            .windowAll(TumblingEventTimeWindows.of(Time.hours(1)))\n            .maxBy(2);\n</code></pre> <p>A process window has an iterable on the collection of events in the window to work with:</p> <pre><code>public static class AddTips extends ProcessWindowFunction&lt;\n            TaxiFare, Tuple3&lt;Long, Long, Float&gt;, Long, TimeWindow&gt; {\n\n    @Override\n    public void process(Long key, Context context, Iterable&lt;TaxiFare&gt; fares, Collector&lt;Tuple3&lt;Long, Long, Float&gt;&gt; out) {\n        float sumOfTips = 0F;\n        for (TaxiFare f : fares) {\n            sumOfTips += f.tip;\n        }\n        out.collect(Tuple3.of(context.window().getEnd(), key, sumOfTips));\n    }\n}\n</code></pre> <p>Time windowing has limitations:</p> <ul> <li>can not correctly process historic data</li> <li>can not correctly handle out-of-order data</li> <li> <p>results will be non-deterministic</p> </li> <li> <p>Long ride alert is an example of Event driven application where alerts are created if a taxi ride started two hours ago is still ongoing. It uses event timestamp and watermarks.</p> </li> </ul> <p>The key is in the MatchFunction process function implementation in which START or END events are kept in a value state, but a timer is set on the context, so the method may get a timer trigger with a processing event that will trigger the onTimer() callback method.</p> <pre><code>context.timerService().registerEventTimeTimer(getTimerTime(ride));\n</code></pre> <p>It generates to the output stream / sink only records from this onTimer.</p>"},{"location":"coding/datastream/#fraud-detection","title":"Fraud detection","text":"<p>This example is based on traditional card transaction fraud detection evaluation. The logic  may need to support:</p> <ul> <li>verify card is not already reported as lost or stolen</li> <li>verify the customer is not already alerted (avoid over alerting)</li> <li>multiple transactions in short time period</li> <li>duplicate transactions from a specific merchant type</li> <li>online transaction and in-person transaction followed in short time period</li> <li>transaction location too far to be feasible in the time period</li> </ul> <p>For lost cards and customer alerted the lists are sent to each node processing the flow so the lookup / join is local and based on the card_id and customer_id. In an EDA implementation the sources will come from Kafka Topics. </p> <p>To support the search for transaction.card_number being lost or not, we use the concepts of broadcast state and stream. The BroadcastState  the same elements are sent to all instances of an operator.</p>"},{"location":"coding/datastream/#interesting-articles","title":"Interesting articles","text":"<ul> <li>Event Driven File Ingestion using Flink Source API</li> </ul>"},{"location":"coding/firstapp/","title":"First Java Applications","text":"Update <ul> <li>Created 2018 </li> <li>Updated 10/2024 Flink open source 1.20.x supports Java 17 Use jbang or sdkman (sdk cli) to install jdk 11 or 17.</li> </ul> <p>In this chapter, I will address how to develop basic Flink java application with Table API, DataStream API or SQL in java in the context of three deployment: open-source, Confluent Cloud and Confluent Platform. The chapter is also addressing CI/CD practices.</p>"},{"location":"coding/firstapp/#introduction","title":"Introduction","text":"<p>Each Java Flink app is a Java main function which defines the data flow to execute on one or more data streams. The flow will be \"compiled\" during deployment, by the flink cli client, as a directed acyclic graph where each node may run in a task slot, within a distributed cluster of Task Manager nodes.</p> <p>The code structure follows the following standard steps:</p> <ol> <li>Obtain a Flink execution environment</li> <li> <p>Define the data pipeline graph (as a separate method to simplify unit testing)</p> <ol> <li>Load/create the initial data from the stream</li> <li>Specify transformations on the data</li> <li>Specify where to put the results of the computations</li> </ol> </li> <li> <p>Trigger the program execution</p> </li> </ol> <p>Once developers build the application jar file, they use Flink CLI to send the jar as a job to the Job manager server which will schedule and assign the job to task managers. </p>"},{"location":"coding/firstapp/#create-a-java-project-with-maven","title":"Create a Java project with maven","text":"<p>See the product documentation which can be summarized as:</p> <ol> <li>Be sure t have maven and JAVA_HOME setup</li> <li> <p>Create a project template, named quickstart, using the Flink quickstart shell and specifying the Flink version (e.g. 1.20.2)</p> <pre><code>curl https://flink.apache.org/q/quickstart.sh | bash -s 1.20.2\n</code></pre> </li> <li> <p>Add the following maven dependencies into the <code>pom.xml</code>:</p> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n    &lt;artifactId&gt;flink-java&lt;/artifactId&gt;\n    &lt;version&gt;${flink-version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> </li> <li> <p>When using Kafka, add kafka connector dependencies in pom.xml</p> <pre><code>  &lt;dependency&gt;\n      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n      &lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;\n      &lt;version&gt;1.20.2&lt;/version&gt;\n  &lt;/dependency&gt;\n</code></pre> </li> <li> <p>Create a Java Class with a main function and the following code structure (See 01-word-count example):</p> <ul> <li>get Flink execution context</li> <li>defined process flow to apply to the data stream</li> <li>start the execution</li> </ul> <pre><code>// Get execution context\n  ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n  // use file as input so use program arguments to get file name\n  ParameterTool params = ParameterTool.fromArgs(args);\n  env.getConfig().setGlobalJobParameters(params);\n  defineWorkflow(env)\n  env.execute();\n</code></pre> <p>The code above uses the ParameterTool  class to process the program arguments.  So most of the basic examples use <code>--input filename</code> and <code>--output filename</code> as java arguments. So <code>params</code> will have those arguments in a Map. </p> </li> <li> <p>Define event structure as POJO, as a separate Java Bean in the <code>event</code> folder.</p> </li> <li>Implement the process logic and the event mapping, filtering logic... We will see more examples later.</li> <li>Package with <code>mvn package</code>. Create a UBER jar if using external system, for reading from a filesystem, Flink has the predefined connectors. The <code>maven-shade-plugin</code> maven plugin creates such UBER jar.</li> <li>Access to a Flink compute pool, locally using local installation, docker, Kubernetes or Confluent Cloud for Flink. Use the <code>flink cli</code> to submit the job for a local cluster, use FlinkDeployment(KFF) or FlinkApplication (CMF)    <pre><code>flink run -c j9r.flink.MyJob myapp.jar\n</code></pre></li> </ol> <p>With the open source version, we have access to a lot of different connectors, for example to load data from csv file. This is convenient to do local testing. or batch processing.</p>"},{"location":"coding/firstapp/#an-example-of-batch-processing","title":"An example of Batch processing","text":"<p>The goals is to read a csv file and compute aggregates in Java DataStream, and then TableAPI. The code is in code/table-api/loan-batch-processing</p>"},{"location":"coding/firstapp/#create-a-quarkus-java-app","title":"Create a Quarkus java app","text":"<ul> <li>Create a Quarkus app: <code>quarkus create app -DprojectGroupId=jbcodeforce -DprojectArtifactId=my-flink</code>. See code examples under <code>flink-java/my-flink</code> folder and <code>jbcodeforce.p1</code> package.</li> <li>Do the same steps as above for the main class.</li> <li>Be sure to set quarkus uber-jar generation (<code>quarkus.package.type=uber-jar</code>) in the <code>application.properties</code> to get all the dependencies in a unique jar: Flink needs all dependencies in the classpath.</li> <li>Package the jar with <code>mvn package</code></li> <li>Every Flink application needs a reference to the execution environment (variable <code>env</code> in previous example). </li> </ul>"},{"location":"coding/firstapp/#submit-job-to-flink","title":"Submit job to Flink","text":"<ul> <li>Start a job manager and task manager with the docker compose under <code>deployment/docker</code> folder or <code>flink-1.19.1/bin/start_cluster.sh</code> for local installation.</li> <li>To submit a job to a Session cluster, use the following command which uses the <code>flink</code> cli. This can be done usin the flink cli on the local install or inside the running <code>JobManager</code> container when using docker or k8s:</li> </ul> <pre><code># One way with mounted files to task manager and job manager containers.\nCNAME=\"jbcodeforce.p1.WordCountMain\"\nJMC=$(docker ps --filter name=jobmanager --format={{.ID}})\ndocker exec -ti $JMC flink run -d -c $CNAME /home/my-flink/target/my-flink-1.0.0-runner.jar --input file:///home/my-flink/data/wc.txt --output file:///home/my-flink/data/out.csv \n\n# inside the jobmanager\nflink run -d -c jbcodeforce.p1.WordCountMain /home/my-flink/target/my-flink-\n1.0.0-runner.jar --input file:///home/my-flink/data/wc.txt --output file:///home/my-flink/data/out.csv\n</code></pre> <p>See the coding practice summary for more datastream examples.</p> <p>And the official operators documentation to understand how to transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated data flow topologies.</p> <p>TO REWORK</p>"},{"location":"coding/firstapp/#unit-testing","title":"Unit testing","text":"<p>There are three type of function to test:</p> <ul> <li>Stateless</li> <li>Stateful</li> <li>Timed process</li> </ul>"},{"location":"coding/firstapp/#stateless","title":"Stateless","text":"<p>For stateless, the data flow can be isolated in static method within the main class, or defined within a separate class. The test instantiates the class and provides the data.</p> <p>For example testing a string to a tuple mapping (MapTrip() is a MapFunction(...) extension):</p> <pre><code> public void testMapToTuple() throws Exception {\n        MapTrip mapFunction = new MapTrip();\n        Tuple5&lt;String,String,String, Boolean, Integer&gt; t = mapFunction.map(\"id_4214,PB7526,Sedan,Wanda,yes,Sector 19,Sector 10,5\");\n        assertEquals(\"Wanda\",t.f0);\n        assertEquals(\"Sector 19\",t.f1);\n        assertEquals(\"Sector 10\",t.f2);\n        assertTrue(t.f3);\n        assertEquals(5,t.f4);\n    }\n</code></pre>"},{"location":"coding/firstapp/#stateful","title":"Stateful","text":"<p>The test needs to check whether the operator state is updated correctly and if it is cleaned up properly, along with the output of the operator. Flink provides TestHarness classes so that we don\u2019t have to create the mock objects.</p>"},{"location":"coding/firstapp/#example-of-standalone-job-docker-compose-file","title":"Example of standalone job docker-compose file","text":"<p>Change the <code>--job-classname</code> parameter of the standalone-job command within the docker-compose file:</p> <pre><code>version: \"2.2\"\nservices:\n  jobmanager:\n    image: flink:latest\n    ports:\n      - \"8081:8081\"\n    command: standalone-job --job-classname com.job.ClassName [--job-id &lt;job id&gt;] [--fromSavepoint /path/to/savepoint [--allowNonRestoredState]] [job arguments]\n    volumes:\n      - /host/path/to/job/artifacts:/opt/flink/usrlib\n    environment:\n      - |\n        FLINK_PROPERTIES=\n        jobmanager.rpc.address: jobmanager\n        parallelism.default: 2\n\n  taskmanager:\n    image: flink:latest\n    depends_on:\n      - jobmanager\n    command: taskmanager\n    scale: 1\n    volumes:\n      - /host/path/to/job/artifacts:/opt/flink/usrlib\n    environment:\n      - |\n        FLINK_PROPERTIES=\n        jobmanager.rpc.address: jobmanager\n        taskmanager.numberOfTaskSlots: 2\n        parallelism.default: 2\n</code></pre>"},{"location":"coding/flink-sql/","title":"Flink SQL","text":"Updates <p>Created 10/24, Updated 12/20/24 Revised 12/06/24</p> <p>This chapter offers a compilation of best practices for implementing Flink SQL solutions, applicable to local Flink open-source, the Confluent Platform for Flink or the Confluent Cloud for Flink.</p>"},{"location":"coding/flink-sql/#getting-started-with-a-sql-client","title":"Getting started with a SQL client","text":"<p>Confluent Cloud enables users to write Flink SQL statements through the web console or a CLI shell, while Flink Open Source features a <code>sql-client</code> shell that communicates with an existing job manager. Currently, the Flink Kubernetes operator does not support the SQL client for Session Clusters. When using Kubernetes deployment, any SQL script must be packaged with a Java program called SQL Runner and deployed as a Flink Application using a FlinkDeployment descriptor.\"</p> <p>Use one of the following approaches:</p> <ul> <li>Once Flink project is downloaded locally, and the cluster started, use <code>sql-client.sh</code> to connect to the cluster.</li> <li>When using Flink with docker compose: the SQL client in the docker container runs against local Flink cluster (see deployment/custom-flink-image folder to build a custom image using the dockerfile with the <code>sql-client</code> service and any specific connector jars).</li> <li>Use Confluent Cloud Flink console to write SQL Statements in a Workspace, and run them directly from there: statements may run on a compute pool and may run forever.</li> <li>Use Confluent cli connected to a compute pool defined in a Confluent Cloud environment. (To create a new environment using Terraform see this note)</li> </ul> Local SQL client <p>The SQL Client aims to provide an easy way to write, debug, and submit table programs to a Flink cluster without a single line of code in any programming language. To interact with Flink using the SQL client, open a bash in the running container, or in the flink bin folder:</p> <pre><code># be sure to mount the folder with sql scripts into the container\n\n# Using running job manager running within docker\ndocker exec -ti sql-client bash\n# in kubernetes pod\nkubectl exec -ti pod_name -n namespace -- bash\n# in the shell /opt/flink/bin \n./sql-client.sh\n</code></pre> SQL client with Confluent Cloud cli <p>See quick start note which is summarized as:</p> <ul> <li>Connect to Confluent Cloud with CLI, then get environment and compute pool identifiers</li> </ul> <pre><code>confluent login --save\nexport ENV_ID=$(confluent environment list -o json | jq -r '.[] | select(.name == \"aws-west\") | .id')\nexport COMPUTE_POOL_ID=$(confluent flink compute-pool list -o json | jq -r '.[0].id')\n</code></pre> <ul> <li>Start local SQL client - using the <code>aws-west</code> environment.</li> </ul> <pre><code>confluent flink shell --compute-pool $COMPUTE_POOL_ID --environment $ENV_ID\n</code></pre> <ul> <li>Write SQL statements, results are visible in the active session.</li> </ul> Run SQL in Kubernetes application <p>Write SQL statements and test them with Java SQL runner. The Class is in flink-studies/code/flink-java/sql-runner folder. Then package the java app and sql script into a docker image then use a FlinkDeployment  descriptor; (see this git doc).</p> <p>See the Flink SQL CLI commands documentation.</p> <p>See the flink-sql/00-basic-sql folder to get some getting started with Flink SQL examples.</p>"},{"location":"coding/flink-sql/#basic-commands","title":"Basic commands","text":"<ul> <li> <p>Show catalogs, tables... By default there is a default catalog and database withour any table.</p> <pre><code>SHOW CATALOGS;\nUSE CATALOG `examples`;\nSHOW DATABASES;\nUSE `marketplace`;\nSHOW TABLES;\nSHOW TABLES LIKE '*_raw'\nSHOW JOBS;\nDESCRIBE tablename;\nDESCRIBE EXTENDED table_name;\n</code></pre> </li> </ul> Understand a type of attribute or get table structure with metadata <pre><code>show create table 'tablename';\n-- for a specific attribute\nselect typeof(column_name) from table_name limit 1;\n</code></pre> <p>Flink SQL planner performs type checking. Assessing type of inferred table is helpful specially around timestamp. See Data type mapping documentation.</p> Understand the physical execution plan for a SQL query <p>See the explain keyword or Confluent Flink documentation for the output explanations.</p> <pre><code>explain select ...\n</code></pre> <p>Indentation indicates data flow, with each operator passing results to its parent. </p> <p>Review the state size, the changelog mode, the upsert key... Operators change changelog modes when different update patterns are needed, such as when moving from streaming reads to aggregations.</p> <p>Pay special attention to data skew when designing your queries. If a particular key value appears much more frequently than others, it can lead to uneven processing where a single parallel instance becomes overwhelmed handling that key\u2019s data. Consider strategies like adding additional dimensions to your keys or pre-aggregating hot keys to distribute the workload more evenly. Whenever possible, configure the primary key to be identical to the upsert key.</p>"},{"location":"coding/flink-sql/#ddl-statements","title":"DDL statements","text":"<p>Data Definition Language (DDL) are statements to define metadata in Flink SQL by creating, updating, or deleting tables. See the Flink SQL Examples in Confluent Cloud for Apache Flink documentation.</p> <p>A table registered with the CREATE TABLE statement can be used as a table source or a table sink.</p>"},{"location":"coding/flink-sql/#table-creation-how-tos","title":"Table creation how-tos","text":"Primary key and partition by considerations <ul> <li>Primary key can have one or more columns, all of them should be not null, and only being <code>NOT ENFORCED</code></li> <li>The primary key declaration, partitions the table implicitly by the key column(s)</li> <li>Flink uses the primary key for state management and deduplication with upsert, if the sink connector supports it. While the partition key is what determines which Kafka partition a message will be written to. This is a Kafka-level concept.</li> <li>For upsert mode, the bucket key must be equal to primary key. While for append/retract mode, the bucket key can be a subset of the primary key.</li> <li>In Confluent Cloud, partition key will generate a key schema, except if using the option (<code>'key.format' = 'raw'</code>)</li> <li>If the destination topic doesn't define partitioning key, then CC Flink SQL will write the records in whatever partitioning that was used at the end of the query. if last operation in the query is <code>GROUP BY foo</code> or <code>JOIN ON A.foo = B.foo</code>, then output records would be partitioned on <code>foo</code> values, and they wouldn't be re-partitioned before writing them into Kafka. The <code>foo</code> partitioning is preserved. </li> <li>If you have parallel queries like without any data shuffling, like <code>INSERT INTO Table_A SELECT * FROM Table_B</code>, then any skew from Table_B would be repeated in Table_A. Otherwise, if partitioning key is defined (like <code>DISTRIBUTED BY HASH(metric)</code>), any writes into that topic would be shuffled by that new key.</li> <li>In case of key skew, add more fields in the distribution to partition not just in one key. That would allow Flink to read data in more parallel fashion, improving the problem with readings from Kafka's bottleneck of 18MBs/connection (partition)</li> <li>An interesting metric, 5 partitions feeds 1 CFU. </li> <li>The performance bottleneck will be actually records serialization and deserialisation, avro is slower than protobuf.</li> </ul> <pre><code>-- simplest table\nCREATE TABLE humans (race STRING, origin STRING);\n-- with primary key \nCREATE TABLE manufactures (m_id INT PRIMARY KEY NOT ENFORCED, site_name STRING);\n-- with hash distribution to 2 partitions that match the primary key\nCREATE TABLE humans (hid INT PRIMARY KEY NOT ENFORCED, race STRING, gender INT) DISTRIBUTED BY (hid) INTO 2 BUCKETS;\n</code></pre> Create a table with csv file as persistence - Flink OSS <p>We need to use the file system connector.</p> <pre><code>create table user (\n    'user_id' VARCHAR(250),\n    'name' VARCHAR(50)\n) partitioned by ('used-id')\nWITH (\n    'format' = 'json', -- other format are: csv, parquet\n    'connector' = 'filesystem',\n    'path' = '/tmp/users'\n);\n</code></pre> CREATE TABLE in Confluent Cloud for Flink <p>The table creation creates topic and -key, -value schemas in the Schema Registry in the same environment as the compute pool in which the query is run. The <code>connector</code> is automatically set to <code>confluent</code>.  The non null and nullable columns are translate as the following avro fields: <pre><code>  \"fields\": [\n        {\n        \"name\": \"customer_id\",\n        \"type\": \"string\"\n        },\n        {\n        \"default\": null,\n        \"name\": \"first_name\",\n        \"type\": [\n            \"null\",\n            \"string\"\n        ]\n        }\n  ]\n</code></pre></p> <p>DATE as:</p> <pre><code>{\n  \"default\": null,\n  \"name\": \"registration_date\",\n  \"type\": [\n    \"null\",\n    {\n      \"logicalType\": \"local-timestamp-millis\",\n      \"type\": \"long\"\n    }\n  ]\n}\n</code></pre> How to consume from a Kafka topic to a SQL table? -- Flink OSS <p>On Confluent Cloud for flink, there are already tables created for each topic. For local Flink we need to create table with column definitions that maps to attributes of the record. The <code>From</code> right operand proposes the list of topic/table for the catalog and database selected. For Flink OSS or Confluent Platform for Flink the <code>WITH</code> statement helps to specify the source topic.</p> <pre><code>select .... from TableName \nWITH (\n    'connector' = 'kafka',\n    'topic' = 'flight_schedules',\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'properties.group.id' = 'fs_grp',\n    'scan.startup.mode' = 'earliest-offset',\n)\n</code></pre> <p>For Avro and schema registry with open source Flink. See the tool extract_sql_from_avro.py to query Confluent Schema Registry and build the matching SQL to create a table connected to the topic using this schema.</p> <pre><code>CREATE TABLE shoe_customers (\n    id STRING,\n    first_name STRING,\n    last_name STRING,\n    email STRING,\n    phone STRING,\n    street_address STRING,\n    state STRING,\n    zip_code STRING,\n    country STRING,\n    country_code STRING\n) WITH (\n    'connector' = 'kafka',\n    'topic' = 'shoe_customers',\n    'properties.bootstrap.servers' = 'broker:29092',\n    'scan.startup.mode' = 'earliest-offset',\n    'key.format' = 'raw',\n    'key.fields' = 'id',\n    'value.format' = 'avro-confluent',\n    'properties.group.id' = 'flink-sql-consumer',\n    'value.fields-include' = 'ALL',\n    'value.avro-confluent.url' = 'http://schema-registry:8081'\n);\n</code></pre> How to load data from a csv file using filesystem connector using SQL - Flink OSS <p>Enter the following statement in a SQL client session:</p> <p><pre><code>SET execution.runtime-mode=BATCH;\n</code></pre> Create a table from the content of the file, mounted inside the container or accessible on local file system:</p> <pre><code>CREATE TABLE employee_info (\n    emp_id INT,\n    name VARCHAR,\n    dept_id INT\n) WITH ( \n    'connector' = 'filesystem',\n    'path' = '/home/flink-sql-demos/00-basic-sql/data/employees.csv',\n    'format' = 'csv'\n);\n</code></pre> <p>Show tables and list some elements within the table.</p> <pre><code>SHOW TABLES;\n\nSELECT * from employee_info WHERE dept_id = 101;\n</code></pre> <p>Show how the table is created:</p> <pre><code>show create table orders;\n</code></pre> <p>See complete example in the readme</p> How to add a metadata field in a table? <p>Use ALTER TABLE</p> <pre><code>alter table flight_schedules add(dt string metadata virtual);\n</code></pre> Create a table as another table by inserting all records (CTAS create table as select) <p>CREATE TABLE AS SELECT is used to create table and insert values in the same statement. It derives the physical column data types and names (from aliased columns), the changelog.mode (from involved tables, operations, and upsert keys), and the primary key.</p> <p>By using a primary key:</p> <pre><code>create table shoe_customer_keyed(\n    primary key(id) not enforced\n) distributed by(id) into 1 buckets\nas select id, first_name, last_name, email from shoe_customers;\n</code></pre> Combine deduplication with create table as select <p>Attention the '`' is important. Ordering with DESC means takes the earliest record</p> <pre><code>CREATE TABLE tenant_dedup (\n    PRIMARY KEY (`tenantId`) NOT ENFORCED\n) DISTRIBUTED BY HASH(`tenantId`) into 1 buckets \n    WITH (\n        'changelog.mode' = 'upsert',\n        'value.fields-include' = 'all'\n    ) AS SELECT \n    `tenantId`,\n    `hostname`,\n    `ts`\n    FROM (\n        SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY `tenantId`\n            ORDER\n            BY $rowtime DESC\n        ) AS row_num\n        FROM tenants\n    ) WHERE row_num = 1;\n</code></pre> How to generate data using Flink Faker? (Flink OSS) <p>Create at table with records generated with Flink faker connector using the DataFaker expressions.. Valid only on OSS Flink or Confluent platform for Flink.</p> <p><pre><code>CREATE TABLE `bounded_pageviews` (\n  `url` STRING,\n  `user_id` STRING,\n  `browser` STRING,\n  `ts` TIMESTAMP(3)\n)\nWITH (\n  'connector' = 'faker',\n  'number-of-rows' = '500',\n  'rows-per-second' = '100',\n  'fields.url.expression' = '/#{GreekPhilosopher.name}.html',\n  'fields.user_id.expression' = '#{numerify ''user_##''}',\n  'fields.browser.expression' = '#{Options.option ''chrome'', ''firefox'', ''safari'')}',\n  'fields.ts.expression' =  '#{date.past ''5'',''1'',''SECONDS''}'\n);\n</code></pre> This will only work in customized Flink client with the jar from Flink faker.</p> Generate data with DataGen for Flink OSS <p>Use DataGen to do in-memory data generation</p> How to generate test data to Confluent Cloud Flink? <p>Use Kafka Connector with DataGen. Those connector exists with a lot of different pre-defined model. Also it is possible to define custom Avro schema and then use predicates to generate data. There is a Produce sample data quick start tutorial from the Confluent Cloud home page. See also this readme.</p> How to transfer the source timestamp to another table <p>As $rowtime is the timestamp of the record in Kafka, it may be interesting to keep the source timestamp to the downstream topic.</p> <pre><code>create table `some_clicks` (\n      `order_id` STRING NOT NULL,\n      ...\n      `event_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp')\ndistributed.... \n</code></pre> <p>Then the statement to insert record to the new table:</p> <pre><code>insert into `some_clicks`\nselect\n    order_id, \n    user_id,\n    $rowtime as event_time\nfrom  `src_table`;\n</code></pre> Dealing with late event <p>Any streams mapped to a table have records arriving more-or-less in order, according to the <code>$rowtime</code>, and the watermarks let the Flink SQL runtime know how much buffering of the incoming stream is needed to iron out any out-of-order-ness before emitting the sorted output stream.</p> <p>We need to  specify the watermark strategy: for example within 30 second of the event time:</p> <pre><code>create table new_table (\n    ....\n    `event_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp',\n watermark for `event_time` as `event_time` - INTERVAL '30' SECOND\n);\n</code></pre> <p>On CCF the watermark is on the <code>$rowtime</code> by default.</p> How to change system watermark? <pre><code>ALTER TABLE table_name MODIFY WATERMARK FOR $rowtime AS $rowtime - INTERVAL '1' SECOND;\n-- in case we need to reverse back\nALTER TABLE table_name DROP WATERMARK;\n</code></pre> <p>This can be used when doing enrichment join on reference table. We do not want to wait for watermark arriving on the reference table, so set the watermark of this reference table to the max INT using <code>ALTER TABLE table_name SET</code>$rowtime<code>TO_TIMESTAMP(,0)</code></p> Create a table with topic as one day persistence <p>See the WITH options.</p> <p><pre><code>create table `small-orders` (\n    `order_id` STRING NOT NULL,\n    `customer_id` INT NOT NULL,\n    `product_id` STRING NOT NULL,\n    `price` DOUBLE NOT NULL\n) distributed by hash(order_id) into 1 buckets\nwith (\n    'kafka.retention.time' = '1 d'\n);\n\ninsert into `small-orders` select * from `examples`.`marketplace`.`orders` where price &lt; 20;\n</code></pre> <code>distributed by hash(order_id)</code> and <code>into 1 buckets</code> specify that the table is backed by a Kafka topic with 1 partitions, and the order_id field will be used as the partition key. </p> Table with Kafka Topic metadata <p>The headers and timestamp are the only options not read-only, all are VIRTUAL. Virtual columns are by default excluded from a SELECT * similar to the system column like <code>$rowtime</code>. </p> <pre><code>ALTER TABLE &lt;table_name&gt; ADD (\n    `headers` MAP&lt;STRING,STRING&gt; METADATA,\n    `leader-epoch`INT METADATA VIRTUAL,\n    `offset` BIGINT METADATA VIRTUAL,\n    `partition` BIGINT METADATA VIRTUAL,\n    `timestamp` TIMESTAMP_LTZ(3) METADATA,\n    `timestamp-type` STRING METADATA VIRTUAL,\n    `topic` STRING METADATA VIRTUAL\n);\n</code></pre> <p>The headers can be updated within SQL statements, some values may be static or coming from the value of one of the selected field. The timestamp can also being updated and for example in most time window queries will be set to the <code>window_time</code>.</p> <pre><code>CREATE TABLE clicks_per_seconds (\n    events_per_second BIGINT,\n    window_time TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'\n    )\n\nINSERT INTO clicks_per_seconds\nSELECT\n    COUNT(*) AS events_per_second,\n    window_time\nFROM TABLE(TUMBLE(TABLE clicks, DESCRIPTOR(`$rowtime`), INTERVAL '1' SECOND))\nGROUP BY window_time, window_end, window_start\n</code></pre> How to support nested rows? <p>Avro, Protobuf or Json schemas are very often hierarchical per design. It is possible to use CAST to name a column within a sub-schema: <pre><code>-- DDL\ncreate table t (\n    StatesTable ROW&lt; states ARRAY&lt;ROW&lt;name STRING, city STRING, lg DOUBLE, lat DOUBLE&gt;&gt;&gt;,\n    creationDate STRING\n)\n\ninsert into t(StatesTable,creationDate)\nvalues( \n    (\n    ARRAY[  row('California', 'San Francisco', -122.4194, 37.7749),\n        row('New York', 'New York City', -74.0060, 40.7128),\n        row('Texas', 'Austin', -97.7431, 30.2672)\n    ]\n    ), \n    '2020-10-10'\n)\n</code></pre></p> <pre><code>-- example of defining attribute from the idx element of an array from a nested json schema:\n  CAST(StatesTable.states[3] AS DECIMAL(10, 4)) AS longitude,\n  CAST(StatesTable.states[4] AS DECIMAL(10, 4)) AS latitude,\n</code></pre> <p>See also the CROSS JOIN UNNEST keywords.</p> <p>See also running demo in flink-sql/03-nested-row</p>"},{"location":"coding/flink-sql/#changelog-mode","title":"Changelog mode","text":"<p>In traditional DB, changelog, or transaction log, records all modification operations in the database. Flink SQL also generates changelog data: changelogs that contain only INSERT-type events is an append streams, with UPDATE events it is an update stream.</p> <p>Some operations (stateful ones) in Flink such as group aggregation and deduplication can produce update events. With retract mode, an update event is split into delete and insert events.</p> <p>The append mode is the simplest mode where records are only added to the result stream, never updated or retracted. Like a write-once data. With append mode, it is possible to create a table with a primary key, and inserting duplicate records with the same key, will be insert events. </p> <pre><code>create table if not exists orders (\n        order_id STRING primary key not enforced,\n        product_id STRING,\n        quantity INT\n    ) DISTRIBUTED into '1' BUCKETS \n    with (\n        'changelog.mode' = 'append',\n        'value.fields-include' = 'all'\n    );\n</code></pre> <p>The outcome includes records in topics that are insert records:</p> Changelog mode: insert events <p>while running the following statement, in a session job, returns the last records per key: (ORD_1, BANANA, 13), (ORD_2, APPLE, 23).</p> <pre><code>SELECT * FROM `orders` LIMIT 10;\n</code></pre> <p>For retract mode, Flink emits pairs of retraction and addition records. When updating a value, it first sends a retraction of the old record (negative record) followed by the addition of the new record (positive record). It means, a fact can be undone, and the combination of +X and -X are related and must be partitioned together. With retract mode a consumer outside of Flink need to interpret the header. </p> <p>Any stateful aggregation, group by, joins, over, match_recognize ... enforces using a key and so upsert or retract changelog mode.</p> <p>See this code examples for the different changelog mode study.</p> <p>Also be sure to get the key as part of the values, using the <code>'value.fields-include' = 'all'</code> option, if not it will not be possible to group by the key.</p>"},{"location":"coding/flink-sql/#applying-to-a-medallion-architecture","title":"Applying to a Medallion Architecture","text":"<p>The current approach can be used for Flink pipeline processing:</p> Table type Goals Parameters Raw table  non debezium format Get the raw data from CDC or outbox pattern cleanup.policy = 'delete', changelog.mode = 'append', value.format = 'avro-registry'... Raw table debezium format Same goals cleanup.policy = 'delete', changelog.mode = 'retract' (retract is the default for Debezium connector),  value.format = 'avro-debezium-registry' Sources Deduplicate and keep last record per key cleanup.policy = 'delete', changelog.mode = 'upsert' Intermediates Enrichment, transformation cleanup.policy = 'delete',  changelog.mode = 'upsert' Sink tables: Facts, Dimensions, Views Create star schema elements cleanup.policy = 'compact', changelog.mode = 'retract' or 'upsert'"},{"location":"coding/flink-sql/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Confluent product document - changelog </li> <li>Flink SQL Secrets: Mastering the Art of Changelog Event Out-of-Orderness</li> <li>Resolving: Primary key differs from derived upsert key</li> </ul>"},{"location":"coding/flink-sql/#sinkupsertmaterializer","title":"SinkUpsertMaterializer","text":"<p>When operating in upsert mode and processing two update events, a potential issue arises. If input operators for two tables in upsert mode are followed by a join and then a sink operator, update events might arrive at the sink out of order. If the downstream operator's implementation doesn't account for this out-of-order delivery, it can lead to incorrect results.</p> <p>Flink typically determines the ordering of update history based on the primary key (or upsert keys) through a global analysis in the Flink planner. However, a mismatch can occur between the upsert keys of the join output and the primary key of the sink table. The <code>SinkUpsertMaterializer</code> operator addresses this mapping discrepancy.</p> <p>This operator maintains a complete list of RowData in its state to correctly process any deletion events originating from the source table. However, this approach can lead to a significant state size, resulting in increased state access I/O overhead and reduced job throughput. Also the output value for each primary key is always the last (tail) element in the maintained list. It is generally advisable to avoid using <code>SinkUpsertMaterializer</code> whenever possible. </p> <p>Consider a scenario where 1 million records need to be processed across a small set of 1,000 keys. In this case, <code>SinkUpsertMaterializer</code> would need to store a potentially long list, averaging approximately 1,000 records per key. </p> <p>To mitigate the usage of <code>SinkUpsertMaterializer</code>:</p> <ul> <li>Ensure that the partition keys used for deduplication, group aggregation, etc., are identical to the sink table's primary keys.</li> <li><code>SinkUpsertMaterializer</code> is unnecessary if retractions are generated using the same key as the sink table's primary key. If a large number of records are processed but most are subsequently retracted, SinkUpsertMaterializer can significantly reduce its state size.</li> <li>Utilize Time-To-Live (TTL) to limit the state size based on time.</li> <li>A higher number of distinct values per primary key directly increases the state size of the SinkUpsertMaterializer.</li> </ul>"},{"location":"coding/flink-sql/#confluent-cloud-flink-table-creation","title":"Confluent Cloud Flink table creation","text":"<p>See the product documentation with some specificities, like source and sink tables are mapped to Kafka Topics. The <code>$rowtime</code> TIMESTAMP_LTZ(3) NOT NULL is provided as a system column.</p> <ul> <li> <p>For each topic there is an inferred table created. The catalog is the Confluent environment and the Kafka cluster is the database. We can use the ALTER TABLE statement to evolve schemas for those inferred tables.</p> </li> <li> <p>A table by default is mapped to a topic with 6 partitions, and the changelog being append. Primary key leads to an implicit DISTRIBUTED BY(k), and value and key schemas are created in Schema Registry. It is possible to create table with primary key and append mode, while by default it is a upsert mode. </p> <pre><code>CREATE TABLE telemetries (\n    device_id INT PRIMARY KEY NOT ENFORCED, \n    geolocation STRING, metric BIGINT,\n    ts TIMESTAMP_LTZ(3) NOT NULL METADATA FROM 'timestamp')\nDISTRIBUTED INTO 4 BUCKETS\nWITH ('changelog.mode' = 'append');\n</code></pre> </li> </ul> <p>The statement above also creates a metadata column for writing a Kafka message timestamp. This timestamp will not be defined in the schema registry. Compared to <code>$rowtime</code> which is declared as a <code>METADATA VIRTUAL</code> column, <code>ts</code> is selected in a <code>SELECT *</code> and is writable.</p> <ul> <li> <p>When the primary key is specified, then it will not be part of the value schema, except if we specify (using <code>value.fields-include' = 'all'</code>) that the value contains the full table schema. The payload of k is stored twice in Kafka message:</p> <pre><code>CREATE TABLE telemetries (k INT, v STRING)\nDISTRIBUTED BY (k)\nWITH ('value.fields-include' = 'all');\n</code></pre> </li> <li> <p>If the key is a string, it may make sense to do not have a schema for the key in this case declare (the key columns are determined by the DISTRIBUTED BY clause): This does not work if the key name is not <code>key</code>.</p> <pre><code>CREATE TABLE telemetries (device_id STRING, metric BIGINT)\nDISTRIBUTED BY (key)\nWITH ('key.format' = 'raw');\n</code></pre> </li> <li> <p>To keep the record in the topic forever add this <code>kafka.retention.time' = '0'</code> as options in the WITH. The supported units are:</p> </li> </ul> <pre><code>\"d\", \"day\", \"h\", \"hour\", \"m\", \"min\", \"minute\", \"ms\", \"milli\", \"millisecond\",\n\"micro\", \"microsecond\", \"ns\", \"nano\", \"nanosecond\"\n</code></pre>"},{"location":"coding/flink-sql/#dml-statements","title":"DML statements","text":"<p>Data modification language, is used to define statements which modify the data and don\u2019t change the metadata.</p>"},{"location":"coding/flink-sql/#common-patterns","title":"Common patterns","text":"<p>This is important to recall that a select applies to a stream of record so the results will change at each new record. A query as below will show the last top 10 orders, and when a new record arrives this list is updated. </p> <pre><code>select * from `examples`.`marketplace`.`orders` order by $rowtime limit 10;\n</code></pre> How to filter out records? <p>using the WHERE clause</p> <pre><code>select * from flight_events where status = 'cancelled';\n</code></pre> <p>Count the number of events related to a cancelled flight (need to use one of the selected field as grouping key):</p> <pre><code>select fight_id, count(*) as cancelled_fl from FlightEvents where status = 'cancelled' group by flight_id;\n</code></pre> <p>Recall that this results produces a dynamic table.</p> How to combine records from multiple tables (UNION)? <p>When the two tables has the same number of columns of the same type, then we can combine them:</p> <pre><code>SELECT * FROM T1\nUNION ALL\nSELECT * FROM T2;\n</code></pre> <p>See product documentation on Union. Remember that UNION will apply distinct, and avoid duplicate, while UNION ALL will generate duplicates. </p> OVER aggregations <p>OVER aggregations compute an aggregated value for every input row over a range of ordered rows. It does not reduce the number of resulting rows, as GROUP BY does, but produces one result for every input row. This is helpful when we need to act on each input row, but consider some time interval. A classical example is to get the number of orders in the last 10 seconds:</p> <pre><code>SELECT \n    order_id,\n    customer_id,\n    `$rowtime`,\n    SUM(price) OVER w AS total_price_ten_secs, \n    COUNT(*) OVER w AS total_orders_ten_secs\nFROM `examples`.`marketplace`.`orders`\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    RANGE BETWEEN INTERVAL '10' SECONDS PRECEDING AND CURRENT ROW\n)\n</code></pre> <p>To get the order exceeding some limits for the first time and then when the computed aggregates go below other limits. LAG</p> <pre><code>-- compute the total price and # of orders for a period of 10s for each customer\nWITH orders_ten_secs AS ( \nSELECT \n    order_id,\n    customer_id,\n    `$rowtime`,\n    SUM(price) OVER w AS total_price_ten_secs, \n    COUNT(*) OVER w AS total_orders_ten_secs\nFROM `examples`.`marketplace`.`orders`\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    RANGE BETWEEN INTERVAL '10' SECONDS PRECEDING AND CURRENT ROW\n    )\n),\n-- get previous orders and current order per customer\norders_ten_secs_with_lag AS (\nSELECT \n    *,\n    LAG(total_price_ten_secs, 1) OVER w AS total_price_ten_secs_lag, \n    LAG(total_orders_ten_secs, 1) OVER w AS total_orders_ten_secs_lag\nFROM orders_ten_secs\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    )\n-- Filter orders when the order price and number of orders were above some limits for previous or current order aggregates\n)\nSELECT customer_id, 'BLOCK' AS action, `$rowtime` AS updated_at \nFROM orders_ten_secs_with_lag \nWHERE \n    (total_price_ten_secs &gt; 300 AND total_price_ten_secs_lag &lt;= 300) OR\n    (total_orders_ten_secs &gt; 5 AND total_orders_ten_secs_lag &lt;= 5)\nUNION ALL \nSELECT customer_id, 'UNBLOCK' AS action, `$rowtime` AS updated_at \nFROM orders_ten_secs_with_lag \nWHERE \n    (total_price_ten_secs &lt;= 300 AND total_price_ten_secs_lag &gt; 300) OR\n    (total_orders_ten_secs &lt;= 5 AND total_orders_ten_secs_lag &gt; 5);\n</code></pre> How to access element of an array of rows? <p>The table has a column that is an array of rows.  <pre><code> CREATE TABLE my_table (\n    key_col INT,\n    nested_data ARRAY&lt;ROW&lt;id INT, name STRING&gt;&gt;\n) WITH (...)\n</code></pre></p> <p>To create one record per row, so exploding the array, use CROSS JOIN UNNEST: <pre><code> SELECT\n    t.key_col,\n    unnested_row.id,\n    unnested_row.name\nFROM\n    my_table AS t\nCROSS JOIN UNNEST(t.nested_data) AS unnested_row;\n</code></pre> So each row in the nested_data arrary will be row in the output table with the matching key_col.</p> How to Aggregate a field into an ARRAY? <p>Let start by a simple array indexing (the index is between 1 to n_element). Below, the <code>values array</code> creates test data into a memory table aliased a <code>T</code> with a column named <code>array_field</code>:</p> <pre><code>SELECT array_field[4] FROM ((VALUES ARRAY[5,4,3,2,1])) AS T(array_field)\n</code></pre> <p>The following code, is creating a view with an array of aggregates, which in this case, is concatenating the urls over a 1 minute tumble window.</p> <pre><code>CREATE VIEW visited_pages_per_minute AS \nSELECT \nwindow_time,\nuser_id, \nARRAY_AGG(url) AS urls\nFROM TABLE(TUMBLE(TABLE examples.marketplace.clicks, DESCRIPTOR(`$rowtime`), INTERVAL '1' MINUTE))\nGROUP BY window_start, window_end, window_time, user_id;\n-- once the view is created\nSELECT * from visited_pages_per_minute;\n-- it is possible to expand an array into multiple rows\nSELECT v.window_time, v.user_id, u.url FROM visited_pages_per_minute AS v\nCROSS JOIN UNNEST(v.urls) AS u(url)\n</code></pre> Navigate a hierarchical structure in a table <p>The unique table has node and ancestors representation. Suppose the graph represents a Procedure at the highest level, then an Operation, then a Phase and a Phase Step at the level 4. In the Procedures table we can have rows like:</p> <pre><code>id, parent_ids, depth, information\n'id_1', [], 0 , 'procedure 1'\n'id_2', ['id_1'], 1 , 'operation 1'\n'id_3', ['id_1','id_2'], 2 , 'phase 1'\n'id_4', ['id_1','id_2','id_3'], 3 , 'phase_step 1'\n'id_5', ['id_1','id_2','id_3'], 3 , 'phase_step 2'\n</code></pre> <p>Suppose we want to extract the matching  procedure_id, operation_id, phase_id, phase_step_id like <pre><code>id, procedure_id, operation_id, phase_id, phase_step_id, information\n'id_1', 'id_1', NULL, NULL, NULL, 'procedure 1'\n'id_2', 'id_1', 'id_2', NULL, NULL, 'operation 1'\n'id_3', 'id_1', 'id_2', 'id_3', NULL, 'phase 1'\n'id_4', 'id_1', 'id_2', 'id_3', 'id_4', 'phase_step 1'\n'id_5', 'id_1', 'id_2', 'id_3', 'id_5', 'phase_step 2'\n</code></pre></p> <p>if the depth is 3, then the response should have all ids populated, if 0 only the top level is returned.</p> <pre><code>with `procedures` as (\n    select 'id_1' as id, array[''] as parentIds, 0 as `depth` , 'procedure 1' as info\n    UNION ALL\n    select 'id_2' as id, array['id_1'] as parentIds, 1 as `depth` , 'operation 1' as info\n    UNION ALL\n    select 'id_3' as id, array['id_1','id_2'] as parentIds, 2 as `depth`, 'phase 1' as info\n    UNION ALL\n    select 'id_4' as id, array['id_1','id_2','id_3'] as parentIds, 3 as `depth`, 'phase_step 1' as info\n    UNION ALL\n    select 'id_5' as id, array['id_1','id_2','id_3'] as parentIds, 3 as `depth`, 'phase_step 2' as info\n    )\nselect \n    id, \n    parent_id, \n    case when `depth` = 3 then id end as phase_step_id,\n    case when `depth` = 2 then id end as phase_id,\n    case when `depth` = 1 then id end as operation_id,\n    case when `depth` = 0 then id end as procedure_id,\n    info from `procedures` cross join unnest(parentIds) as ids(parent_id)\n</code></pre> How to transform a field representing epoch to a timestamp? <p>epoch is a BIGINT.</p> <pre><code> TO_TIMESTAMP(FROM_UNIXTIME(click_ts_epoch)) as click_ts\n</code></pre> How to change a date string to a timestamp? <pre><code>TO_TIMESTAMP('2024-11-20 12:34:568Z'),\n</code></pre> <pre><code>to_timestamp(transaction_date, 'yyyy-MM-dd HH:mm:ss') as tx_date, -- bigint\n</code></pre> <p>See all the date and time functions.</p> How to compare a date field with current system time? <pre><code>WHEN TIMESTAMPDIFF(day, event.event_launch_date, now()) &gt; 120 THEN ...\n</code></pre> <p>The table used as target to this processing, if new records are added to it, then needs to be append log, as if it is upsert then the now() time is not determenistic for each row to process.</p> How to mask a field? <p>Create a new table from the existing one, and then use REGEXP_REPLACE to mask an existing attribute</p> <pre><code>create table users_msk like users;\nINSERT INTO users_msk SELECT ..., REGEXP_REPLACE(credit_card,'(\\w)','*') as credit_card FROM users;\n</code></pre> How to filter row that has column content not matching a regular expression? <p>Use REGEX</p> <pre><code>WITH filtered_data AS (\nSELECT *,\n    CASE \n        WHEN NOT REGEXP(user_agent, '.*Opera/.*') \n        THEN TRUE  -- Keep this row\n        ELSE FALSE -- Filter out rows that match \"Opera/\"\n    END AS keep_row\nFROM examples.marketplace.clicks\n)\n\nSELECT * \nFROM filtered_data\nWHERE keep_row = TRUE;\n</code></pre> What are the different SQL execution modes? <p>Using previous table it is possible to count the elements in the table using:</p> <pre><code>select count(*) AS `count` from pageviews;\n</code></pre> <p>and we get different behaviors depending of the execution mode:</p> <pre><code>set 'execution.runtime-mode' = 'batch';\n# default one\nset 'execution.runtime-mode' = 'streaming';\n\nset 'sql-client.execution.result-mode' = 'table';\n</code></pre> <p>In changelog mode, the SQL Client doesn't just update the count in place, but instead displays each message in the stream of updates it's receiving from the Flink SQL runtime.</p> <pre><code>set 'sql-client.execution.result-mode' = 'changelog';\n</code></pre> <p></p> How to access json data from a string column being a json object? <p>Use json_query function in the select.</p> <pre><code> json_query(task.object_state, '$.dueDate') AS due_date,\n</code></pre> <p>Use json_value() instead if the column content is a dict or json {}.</p> How to transform a json array column (named data) into an array to then generate n rows? <p>Returning an array from a json string: <pre><code>json_query(`data`, '$' RETURNING ARRAY&lt;STRING&gt;) as anewcolumn\n</code></pre></p> <p>To create as many rows as there are elements in the nested array: <pre><code>SELECT existing_column, anewcolumn from table_name\ncross join unnest (json_query(`data`, '$' RETURNING ARRAY&lt;STRING&gt;)) as t(anewcolumn)\n</code></pre></p> <p>UNNEST returns a new row for each element in the array See multiset expansion doc</p> How to implement the equivalent of SQL explode? <p>SQL EXPLODE creates a row for each element in the array or map, and ignore null or empty values in array. <pre><code>SELECT explode(col1) from values (array(10,20)), (null)\n</code></pre> SQL has also EXPLODE_OUTER, which returns all values in array including null or empty. To translate this to Flink SQL we can use MAP_ENTRIES and MAP_FROM_ARRAYS. MAP_ENTRIES returns an array of all entries in the given map. While MAP_FROM_ARRAYS returns a map created from an arrays of keys and values. <pre><code>select map_entries(map_from_arrays())\n</code></pre></p> How to use conditional functions? <p>Flink has built-in conditional functions (See also Confluent support) and specially the CASE WHEN:</p> <pre><code>SELECT \n    *\n    FROM `stocks`\n    WHERE  \n    CASE \n        WHEN price &gt; 200 THEN 'high'\n        WHEN price &lt;=200 AND price &gt; 150 THEN 'medium'\n        ELSE 'low'\n    END;\n</code></pre> When and how to use custom watermark? <p>Developer should use their own watermark strategy when there are not a lot of records per topic/partition, there is a need for a large watermark delay, and need to use another timestamp.  The default watermark strategy in SOUCE_WATERMARK(), a watermark defined by the source. The common strategy used is the <code>maximim-out-of-orderness</code> to allow messages arriving later to be part of the window, to ensure more accurate results, as a tradeoff of latency. It can be defined using:</p> <pre><code>ALTER TABLE &lt;table_name&gt; MODIFY WATERMARK for `$rowtime` as `$rowtime` - INTERVAL '20' SECONDS\n</code></pre> <p>The minimum out-of-orderness is 50ms and can be set up to 7 days. See Confluent documentation. </p> Deduplication example <pre><code>SELECT ip_address, url, TO_TIMESTAMP(FROM_UNIXTIME(click_ts_raw)) as click_timestamp\nFROM (\n    SELECT *,\n    ROW_NUMBER() OVER ( PARTITION BY ip_address ORDER BY TO_TIMESTAMP(FROM_UNIXTIME(click_ts_raw)) DESC) as rownum FROM clicks\n    )\nWHERE rownum = 1;\n</code></pre> <p>See this example.</p> How to manage late message to be sent to a DLQ? <p>First create a DLQ table like late_orders based on the order table:</p> <pre><code>    create table late_orders\n    with (\n        'connector'= ''\n    ) \n    LIKE orders (EXCLUDING OPTIONS)\n</code></pre> <p>Groups the main stream processing and late arrival in a statement set:</p> <pre><code>EXECUTE STATEMENT SET\nBEGIN\n    INSERT INTO late_orders SELECT from orders WHERE `$rowtime` &lt; CURRENT_WATERMARK(`$rowtime`);\n    INSERT INTO order_counts -- the sink table\n    SELECT window_time, COUNT(*) as cnt\n    FROM TABLE(TUMBLE(TABLE orders DESCRIPTOR(`$rowtime`), INTERVAL '1' MINUTE))\n    GROUP BY window_start, window_end, window_time\nEND\n</code></pre> <p>The Flink built-in system functions.</p>"},{"location":"coding/flink-sql/#joins","title":"Joins","text":"<p>When doing a join, Flink needs to materialize both the right and left of the join tables fully in state, which can cost a lot of memory, because if a row in the left-hand table (LHT), also named the probe side, is updated, the operator needs to emit an updated match for all matching rows in the right-hand table (RHT) or build side. The cardinality of right side will be mostly bounded at a given point of time, but the left side may vary a lot. A join emits matching rows to downstream processing.</p> <p>Here is a list of important tutorials on Joins:</p> <ul> <li>Confluent Cloud: video on joins.</li> <li>Confluent -developer: How to join streams. The matching content is in flink-sql/04-joins folder for Confluent Cloud or Platform for Flink. This folder also includes more SQL exercises.</li> <li>Confluent temporal join</li> <li>Window Join Queries in Confluent Cloud for Apache Flink</li> </ul> Inner knowledge on temporal join <p>Event-time temporal joins are used to join two or more tables based on a common event time (in one of the record table or the kafka record: <code>$rowtime</code> system column). With an event-time attribute, the operator can retrieve the value of a key as it was at some point in the past. The right-side, versioned table, stores all versions, identified by time, since the last watermark.</p> <p>The temporal Flink sql looks like:</p> <pre><code>SELECT [column_list]\nFROM table1 [AS &lt;alias1&gt;]\n[LEFT] JOIN table2 FOR SYSTEM_TIME AS OF table1.{ rowtime } [AS &lt;alias2&gt;]\nON table1.column-name1 = table2.column-name1\n</code></pre> <p>When enriching a particular <code>table1</code>, an event-time temporal join waits until the watermark on the table2 stream reaches the timestamp of that <code>table1</code> row, because only then is it reasonable to be confident that the result of the join is being produced with complete knowledge of the relevant <code>table2</code> data. This table2 record can be old as the watermark on that table being late.</p> How to join two tables on a key within a time window using event column as timestamp and store results in a target table? <p>Full example:</p> <pre><code>-- use separate statements to create the tables\ncreate table Transactions (ts TIMESTAMP(3), tid BIGINT, amount INT);\ncreate table Payments (ts TIMESTAMP(3), tid BIGINT, type STRING);\ncreate table Matched (tid BIGINT, amount INT, type STRING);\n\nexecute statement set\nbegin\ninsert into Transactions values(now(), 10,20),(now(),11,25),(now(),12,34);\ninsert into Payments values(now(), 10, 'debit'),(now(),11,'debit'),(now(),12,'credit');\ninsert into Matched \n    select T.tid, T.amount, P.type\n    from Transactions T join Payments P ON T.tid = P.tid \n    where P.ts between T.ts and T.ts + interval '1' minutes;\nend\n</code></pre> Join on 1x1 relationship <p>In current Flink SQL it is not possible to efficiently join elements from two tables when we know the relation is 1 to 1: one transaction to one account, one shipment to one order. As soon as there is a match, normally we want to emit the result and clear the state. This is possible to do so with the DataStream API, not SQL.</p>"},{"location":"coding/flink-sql/#windowing-table-value-functions","title":"Windowing / Table Value Functions","text":"<p>Windowing Table-Valued Functions groups the Tumble, Hop, Cumulate, and Session Windows. Windows split the stream into \u201cbuckets\u201d of finite size, over which we can implement logic. The return value adds three additional columns named \u201cwindow_start\u201d, \u201cwindow_end\u201d, \u201cwindow_time\u201d to indicate the assigned window.</p> <ul> <li>The TUMBLE function assigns each element to a window of specified window size. Tumbling windows have a fixed size and do not overlap.</li> </ul> Count the number of different product type per 10 minutes (TUMBLE window) <p>Aggregate a Stream in a Tumbling Window documentation..  The following query counts the number of different product types arriving from the event stream by interval of 10 minutes.</p> <p><pre><code>SELECT window_start, product_type, count(product_type) as num_ptype\n    FROM TABLE(\n        TUMBLE(\n            TABLE events,\n            DESCRIPTOR(`$rowtime`),\n            INTERVAL '10' MINUTES\n        )\n    )\n    GROUP BY window_start, window_end, ;\n</code></pre> DESCRIPTOR indicates which time attributes column should be mapped to tumbling windows (here the kafka record ingestion timestamp). </p> <p>When the internal time has expired the results will be published. This puts an upper bound on how much state Flink needs to keep to handle a query, which in this case is related to the number of different product type. </p> <p>It is possible to use another timestamp from the input table. For example the <code>transaction_ts TIMESTAMP(3),</code>  then we need to declare a watermark on this ts:</p> <p><code>WATERMARK FOR transaction_ts AS transaction_ts - INTERVAL '5' SECOND,</code> so it can be used in the descriptor function.</p> <pre><code>INSERT INTO app_orders\nselect \n    window_start, \n    window_end, \n    customer_id, sum(order_amount) \nfrom table(tumble(table `daily_spend`, DESCRIPTOR(transaction_ts), interval '24' hours)) \ngroup by window_start, window_end, customer_id \n</code></pre> Aggregation over a window <p>Windows over approach is to end with the current row, and stretches backwards through the history of the stream for a specific interval, either measured in time, or by some number of rows. For example counting the umber of flight_schedule events of the same key over the last 100 events:</p> <pre><code>select\n    flight_id,\n    evt_type,\n    count(evt_type) OVER w as number_evt,\nfrom flight_events\nwindow w as( partition by flight_id order by $rowtime rows between 100 preceding and current row);\n</code></pre> <p>The results are updated for every input row. The partition is by flight_id. Order by $rowtime is necessary.</p> Find the number of elements in x minutes intervals advanced by 5 minutes? (HOP) <p>Confluent documentation on window integration.. For HOP wuindow, there is the slide parameter to control how frequently a hopping window is started:</p> <pre><code>    SELECT\n        window_start, window_end,\n        COUNT(DISTINCT order_id) AS num_orders\n    FROM TABLE(\n        HOP(TABLE shoe_orders, DESCRIPTOR(`$rowtime`), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES))\n    GROUP BY window_start, window_end;\n</code></pre> How to compute the accumulate price over time in a day (CUMULATE) <p>Needs to use the cumulate window, which adds up records to the window until max size, but emits results at each window steps.  The is image summarizes well the behavior: </p> <pre><code>SELECT window_start, window_end, SUM(price) as `sum`\n    FROM TABLE(\n        CUMULATE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '30' SECONDES, INTERVAL '3' MINUTES))\n    GROUP BY window_start, window_end;\n</code></pre>"},{"location":"coding/flink-sql/#row-pattern-recognition","title":"Row pattern recognition","text":"Find the longest period of time for which the average price of a stock did not go below a value <p>Create a Datagen to publish StockTicker to a Kafka topic. See product documentation on CEP pattern with SQL</p> <pre><code>create table StockTicker(symbol string, price int tax int) with ('connector' = 'kafka',...)\nSELECT * From StockTicker \nMATCH_RECOGNIZE ( \n    partition by symbol \n    order by rowtime\n    measures\n        FIRST(A.rowtime) as start_tstamp,\n        LAST(A.rowtime) as last_tstamp,\n        AVG(A.price) as avgPrice\n    ONE ROW PER MATCH\n    AFTER MATCH SKIP PAST LAST ROW\n    PATTERN (A+ B)\n    DEFINE\n        A as AVG(A.price) &lt; 15\n);\n</code></pre> <p>MATCH_RECOGNIZE helps to logically partition and order the data that is used with the PARTITION BY and ORDER BY clauses, then defines patterns of rows to seek using the PATTERN clause. The logical components of the row pattern variables are specified in the DEFINE clause. B is defined implicitly as not being A.</p>"},{"location":"coding/flink-sql/#confluent-cloud-specific","title":"Confluent Cloud Specific","text":"<p>See Flink Confluent Cloud queries documentation.</p> <p>Each topic is automatically mapped to a table with some metadata fields added, like the watermark in the form of <code>$rowtime</code> field, which is mapped to the Kafka record timestamp. To see it, run <code>describe extended table_name;</code> With watermarking. arriving event records will be ingested roughly in order with  respect to the <code>$rowtime</code> time attribute field.</p> Mapping from Kafka record timestamp and table $rowtime <p>The Kafka record timestamp is automatically mapped to the <code>$rowtime</code> attribute, which is a read only field. Using this field we can order the record by arrival time:</p> <pre><code>select 'flight_id', 'aircraft_id', 'status', $rowtime\nfrom Aircrafts\norder by $rowtime;\n</code></pre> How to run Confluent Cloud for Flink? <p>See the note, but can be summarized as: 1/ create a stream processing compute pool in the same environment and region as the Kafka cluster, 2/ use Console or CLI (flink shell) to interact with topics.</p> <p></p> <pre><code>confluent flink quickstart --name my-flink-sql --max-cfu 10 --region us-west-2 --cloud aws\n</code></pre> Running Confluent Cloud Kafka with local Flink <p>The goal is to demonstrate how to get a cluster created in an existing Confluent Cloud environment and then send message via FlinkFaker using local table to Kafka topic:</p> <p></p> <p>The scripts and readme .</p> Reading from a topic specific offsets <pre><code>ALTER TABLE table_name SET (\n    'scan.startup.mode' = 'specific-offsets',\n    'scan.startup.specific-offsets' = 'partition:0,offset:25; partition:1,offset:10'\n);\n-- Returns from offsets 26 and 11\nSELECT * FROM table_name;\n</code></pre> create a long running SQL with cli <p>Get or create a service account.</p> <pre><code>confluent iam service-account create my-service-account --description \"new description\"\nconfluent iam service-account list\nconfluent iam service-account describe &lt;id_of_the_sa&gt;\n</code></pre> <pre><code>confluent flink statement create my-statement --sql \"SELECT * FROM my-topic;\" --compute-pool &lt;compute_pool_id&gt; --service-account sa-123456 --database my-cluster\n</code></pre> Assess the current flink statement running in Confluent Cloud <p>To assess which jobs are still running, which jobs failed, and which stopped, we can use the user interface, go to the Flink console &gt; . Or the <code>confluent</code> CLI:</p> <pre><code>confluent environment list\nconfluent flink compute-pool list\nconfluent flink statement list --cloud aws --region us-west-2 --environment &lt;your env-id&gt; --compute-pool &lt;your pool id&gt;\n</code></pre>"},{"location":"coding/flink-sql/#from-batch-to-real-time","title":"From batch to real-time","text":"How to support Type 2 slowly changing dimension (SCD) table? <p>Type 2 SCDs are designed to maintain a complete history of all changes to dimension data. When a change occurs, a new row is inserted into the table, representing the updated record, while the original record remains untouched. Each record in the table is typically assigned a unique identifier (often a surrogate key) to distinguish between different versions of the same dimension member. </p>"},{"location":"coding/flink-sql/#user-defined-function","title":"User Defined Function","text":"<p>Flink User Defined Function allows developers to upload custom logic, complex calculation, data manipulation (XML, custom AVRO), within a SQL or TableAPI queries. Developers can leverage existing libraries like Geospatial calculation, Math computation...  At the deployment level it can be considered as FaaS. Not optimized for cold start, it could be P90 at 25s. </p> <p>For developer the steps are:</p> <ol> <li>Develop a functin to extends a <code>org.apache.flink.table.functions.ScalarFunction</code></li> <li>Build a uber jar</li> <li>Register the UDF as a Flink artifact. On Confluent Cloud, artifacts are environment scoped.</li> <li>Configure SQL to use this function using: `CREATE FUNCTION fct_name as 'fully.qualified.classname' USING JAR 'confluent-artifact://....'</li> </ol>"},{"location":"coding/flink-sql/#troubleshooting-sql-statement-running-slow","title":"Troubleshooting SQL statement running slow","text":"How to search for hot key? <pre><code>SELECT \n    id, \n    tenant_id, \n    count(*) as record_count,\nFROM table_name \nGROUP BY id, tenant_id\n</code></pre> <p>A more advanced statistical query ( TO BE TESTED) <pre><code>WITH key_stats AS (\n    SELECT \n        id,\n        tenant_id,\n        count(*) as record_count\n    FROM src_aqem_tag_tag \n    GROUP BY id, tenant_id\n),\ndistribution_stats AS (\n    SELECT \n        AVG(record_count) as mean_count,\n        STDDEV(record_count) as stddev_count,\n        PERCENTILE_APPROX(record_count, 0.75) as q3,\n        PERCENTILE_APPROX(record_count, 0.95) as p95,\n        PERCENTILE_APPROX(record_count, 0.99) as p99\n    FROM key_stats\n)\nSELECT \n    ks.*,\n    ds.mean_count,\n    ds.stddev_count,\n    -- Z-score calculation for outlier detection\n    CASE \n        WHEN ds.stddev_count &gt; 0 \n        THEN (ks.record_count - ds.mean_count) / ds.stddev_count\n        ELSE 0\n    END as z_score,\n    -- Hot key classification\n    CASE \n        WHEN ks.record_count &gt; ds.p99 THEN 'EXTREME_HOT'\n        WHEN ks.record_count &gt; ds.p95 THEN 'VERY_HOT'\n        WHEN ks.record_count &gt; ds.q3 * 1.5 THEN 'HOT'\n        ELSE 'NORMAL'\n    END as hot_key_category\nFROM key_stats ks\nCROSS JOIN distribution_stats ds\nWHERE ks.record_count &gt; ds.mean_count \n</code></pre></p>"},{"location":"coding/flink-sql/#metric-to-consider","title":"Metric to consider","text":""},{"location":"coding/flink-sql/#explain-statement","title":"Explain statement","text":""},{"location":"coding/getting-started/","title":"Flink Getting Started Guide","text":"Updates <ul> <li>Created 2018 </li> <li>Updated 2/14/2025 - improve note, on k8s deployment and get simple demo reference, review done. </li> <li>03/30/25: converged the notes and update referenced links</li> <li>09/25: Review - simplications.</li> </ul> <p>There are four different approaches to deploy and run Apache Flink / Confluent Flink:</p> <ol> <li>Local Binary Installation</li> <li>Docker-based local deployment</li> <li>Kubernetes Deployment Colima, AKS, EKS, GKS, King or minicube</li> <li>Confluent Cloud Managed Service</li> </ol>"},{"location":"coding/getting-started/#pre-requisites","title":"Pre-requisites","text":"<p>Before getting started, ensure you have some of the following dependant components:</p> <ol> <li>Java 11 or higher installed (OpenJDK) for developing Java based solution.</li> <li>Docker Engine and Docker CLI (for Docker and Kubernetes deployments)</li> <li>kubectl and Helm (for Kubernetes deployment)</li> <li>Confluent Cloud account (for Confluent Cloud deployment, and Flink SQL development)</li> <li>Confluent cli installed for both Confluent Cloud and Confluent Platform</li> <li>Git clone this repository, to access code and deployment manifests and scripts</li> </ol>"},{"location":"coding/getting-started/#1-open-source-apache-flink-local-binary-installation","title":"1. Open Source Apache Flink Local Binary Installation","text":"<p>This approach is ideal for development and testing on a single machine.</p>"},{"location":"coding/getting-started/#installation-steps","title":"Installation Steps","text":"<ol> <li>Download and extract Flink binary    <pre><code># Using the provided script\n./deployment/product-tar/install-local.sh\n\n# Or manually\ncurl https://dlcdn.apache.org/flink/flink-1.20.1/flink-1.20.1-bin-scala_2.12.tgz --output flink-1.20.1-bin-scala_2.12.tgz\ntar -xzf flink-1.20.1-bin-scala_2.12.tgz\n</code></pre></li> <li> <p>Download and extract Kafka binary See download versions <pre><code>curl https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz  --output kafka_2.13-3.9.0.tgz \ntar -xvf kafka_2.13-3.9.0.tgz\ncd kafka_2.13-3.9.0\nKAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\nbin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\n</code></pre></p> </li> <li> <p>Set environment variables:    <pre><code>export FLINK_HOME=$(pwd)/flink-1.20.1\nexport KAFKA_HOME=$(pwd)/kafka_2.13-3.9.0\nexport PATH=$PATH:$FLINK_HOME/bin:$KAFKA_HOME/bin\n</code></pre></p> </li> <li> <p>Start the Flink cluster:    <pre><code>$FLINK_HOME/bin/start-cluster.sh\n</code></pre></p> </li> <li> <p>Access the Web UI at http://localhost:8081</p> </li> <li> <p>Submit a job (Java application)    <pre><code>./bin/flink run ./examples/streaming/TopSpeedWindowing.jar\n./bin/flink list\n./bin/flink cancel &lt;id&gt; \n</code></pre></p> </li> <li> <p>Download needed SQL connector for Kafka    <pre><code>cd flink-1.20.1\nmkdir sql-lib\ncd sql-lib\ncurl https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.4.0-1.20/flink-sql-connector-kafka-3.4.0-1.20.jar --output flink-sql-connector-kafka-3.4.0-1.20.jar\n</code></pre></p> </li> <li> <p>Stop the cluster:     <pre><code>$FLINK_HOME/bin/stop-cluster.sh\n</code></pre></p> </li> </ol>"},{"location":"coding/getting-started/#running-a-simple-sql-application","title":"Running a simple SQL application","text":"<ol> <li> <p>Start Kafka cluster and create topics    <pre><code>$KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/kraft/server.properties\n$KAFKA_HOME/bin/kafka-topics.sh --create --topic flink-input --bootstrap-server localhost:9092\n$KAFKA_HOME/bin/kafka-topics.sh --create --topic message-count --bootstrap-server localhost:9092\n</code></pre></p> </li> <li> <p>Use the Flink SQL Shell:    <pre><code>$FLINK_HOME/bin/sql-client.sh --library $FLINK_HOME/sql-lib\n</code></pre></p> <ul> <li>Create a table using Kafka connector:   <pre><code>CREATE TABLE flinkInput (\n   `raw` STRING,\n   `ts` TIMESTAMP(3) METADATA FROM 'timestamp'\n) WITH (\n   'connector' = 'kafka',\n   'topic' = 'flink-input',\n   'properties.bootstrap.servers' = 'localhost:9092',\n   'properties.group.id' = 'j9rGroup',\n   'scan.startup.mode' = 'earliest-offset',\n   'format' = 'raw'\n);\n</code></pre></li> <li>Create an output table in a debezium format so we can see the before and after data:   <pre><code>CREATE TABLE msgCount (\n   `count` BIGINT NOT NULL\n) WITH (\n   'connector' = 'kafka',\n   'topic' = 'message-count',\n   'properties.bootstrap.servers' = 'localhost:9092',\n   'properties.group.id' = 'j9rGroup',\n   'scan.startup.mode' = 'earliest-offset',\n   'format' = 'debezium-json'\n);\n</code></pre></li> <li>Make the simplest flink processing by counting the messages:   <pre><code>INSERT INTO msgCount SELECT COUNT(*) as `count` FROM flinkInput;\n</code></pre>   The result will look like:   <pre><code>[INFO] Submitting SQL update statement to the cluster...\n[INFO] SQL update statement has been successfully submitted to the cluster:\nJob ID: 2be58d7f7f67c5362618b607da8265d7\n</code></pre></li> <li>Start a producer in one terminal   <pre><code> bin/kafka-console-producer.sh --topic flink-input --bootstrap-server localhost:9092\n</code></pre></li> <li>Verify result in a second terminal   <pre><code>bin/kafka-console-consumer.sh -topic message-count  --bootstrap-server localhost:9092\n</code></pre>   The results will be a list of debezium records like   <pre><code>{\"before\":null,\"after\":{\"count\":1},\"op\":\"c\"}\n{\"before\":{\"count\":1},\"after\":null,\"op\":\"d\"}\n{\"before\":null,\"after\":{\"count\":2},\"op\":\"c\"}\n{\"before\":{\"count\":2},\"after\":null,\"op\":\"d\"}\n{\"before\":null,\"after\":{\"count\":3},\"op\":\"c\"}\n{\"before\":{\"count\":3},\"after\":null,\"op\":\"d\"}\n{\"before\":null,\"after\":{\"count\":4},\"op\":\"c\"}\n</code></pre></li> </ul> </li> <li> <p>[Optional] Start SQL Gateway  for concurrent SQL queries:    <pre><code>$FLINK_HOME/bin/sql-gateway.sh start -Dsql-gateway.endpoint.rest.address=localhost\n</code></pre></p> </li> </ol> <p>See product documentation for different examples. To do some  Python Table API demonstrations see this chapter.</p>"},{"location":"coding/getting-started/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If port 8081 is already in use, modify <code>$FLINK_HOME/conf/flink-conf.yaml</code></li> <li>Check logs in <code>$FLINK_HOME/log</code> directory</li> <li>Ensure Java 11+ is installed and JAVA_HOME is set correctly</li> </ul>"},{"location":"coding/getting-started/#2-docker-based-deployment","title":"2. Docker-based Deployment","text":"<p>This approach provides containerized deployment using Docker Compose.</p>"},{"location":"coding/getting-started/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Build a custom Apache Flink image with your own connectors. Verify current docker image tag then use the Dockerfile:    <pre><code>cd deployment/custom-flink-image\ndocker build -t jbcodeforce/myflink .\n</code></pre></p> </li> <li> <p>Start Flink session cluster:    <pre><code>cd deployment/docker\ndocker compose up -d\n</code></pre></p> </li> </ol>"},{"location":"coding/getting-started/#docker-compose-with-kafka","title":"Docker Compose with Kafka","text":"<p>To run Flink with Kafka:    <pre><code>cd deployment/docker\ndocker compose -f kafka-docker-compose.yaml up -d\n</code></pre></p>"},{"location":"coding/getting-started/#customization","title":"Customization","text":"<ul> <li>Modify <code>deployment/custom-flink-image/Dockerfile</code> to add required connectors</li> <li>Update <code>deployment/docker/flink-oss-docker-compose.yaml</code> for configuration changes</li> </ul> <p>During development, we can use docker-compose to start a simple <code>Flink session</code> cluster or a standalone job manager to execute one unique job, which has  the application jar mounted inside the docker image. We can use this same environment to do SQL based Flink apps. </p> <p>As Task manager will execute the job, it is important that the container running the flink code has access to the jars needed to connect to external sources like Kafka or other tools like FlinkFaker. Therefore, in <code>deployment/custom-flink-image</code>, there is a Dockerfile to get the needed jars to build a custom Flink image that may be used for Taskmanager  and SQL client. Always update the jar version with new Flink version.</p> Docker hub and maven links <ul> <li>Docker Hub Flink link</li> <li>Maven Flink links</li> </ul>"},{"location":"coding/getting-started/#3-kubernetes-deployment","title":"3. Kubernetes Deployment","text":"<p>This approach provides scalable, production-ready deployment using Kubernetes. </p>"},{"location":"coding/getting-started/#apache-flink","title":"Apache Flink","text":"<p>See the K8S deployment deeper dive chapter and the lab readme for details.</p>"},{"location":"coding/getting-started/#confluent-platform-manager-for-flink-on-kubernetes","title":"Confluent Platform Manager for Flink on Kubernetes","text":"<p>See Kubernetes deployment chapter for detailed instructions, and the deployment folder and readme. See also the Confluent operator documentation, submit Flink SQL Statement with Confluent Manager for Apache Flink:</p>"},{"location":"coding/getting-started/#4-confluent-cloud-deployment","title":"4. Confluent Cloud Deployment","text":"<p>This approach provides a fully managed Flink service and very easy to get started quickly without managing Flink clusters or Kafka Clusters.</p> <ol> <li> <p>Create a Flink compute pool:    <pre><code>confluent flink compute-pool create my-pool --cloud aws --region us-west-2\n</code></pre></p> </li> <li> <p>Start SQL client:    <pre><code>confluent flink shell\n</code></pre></p> </li> <li> <p>Submit SQL statements:    <pre><code>CREATE TABLE my_table (\n  id INT,\n  name STRING\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'my-topic',\n  'properties.bootstrap.servers' = 'pkc-xxxxx.region.provider.confluent.cloud:9092',\n  'properties.security.protocol' = 'SASL_SSL',\n  'properties.sasl.mechanism' = 'PLAIN',\n  'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"&lt;API_KEY&gt;\" password=\"&lt;API_SECRET&gt;\";',\n  'format' = 'json'\n);\n</code></pre></p> </li> </ol> <p>See Confluent Cloud Flink documentation for more details.</p> <p>Explore the Shift Left project, your dedicated CLI for scaling and organizing Confluent\u202fCloud\u202fFlink projects with an opinionated, streamlined approach.</p>"},{"location":"coding/getting-started/#choosing-the-right-deployment-approach","title":"Choosing the Right Deployment Approach","text":"Approach Use Case Pros Cons Local Binary Development, Testing Simple setup, Fast iteration Limited scalability, or manual configuration and maintenance on distributed computers. Docker Development, Testing Containerized, Reproducible Manual orchestration Kubernetes Production Scalable, Production-ready Complex setup Confluent Cloud Production Fully managed, No ops Vendor Control Plane"},{"location":"coding/getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Apache Flink Documentation</li> <li>Confluent Cloud Documentation</li> <li>Flink Kubernetes Operator</li> <li>Docker Hub Flink Images</li> <li>Shift Left project to manage Flink project at scale.</li> </ul> &lt;&lt; PREVIOUS: Key Concepts &gt;&gt; NEXT: Flink SQL &gt;&gt;"},{"location":"coding/k8s-deploy/","title":"Flink Kubernetes Deployment","text":"This chapter updates <ul> <li>Created 10/2024</li> <li>12/24: move some content to hands-on readme, clean content</li> <li>01/25: sql processing section</li> <li>05/25: merge content, simplify, add some details on deployment - fully test k8s deployment on Colima</li> <li>07/25: Update for Confluent Platform v8</li> <li>09/29: Update to diagrams and doc structure.</li> <li>10/12: update to Minio and snapshot / checkpoint configuration</li> <li>10/20: Reorganize content</li> </ul> <p>Apache Flink has defined a Kubernetes Operator (FKO) to deploy and manage custom resources for Flink deployments. Confluent Platform Manager for Flink (CMF) is also deployed on Kubernetes with its own operator, leveraging the FKO. Also as part of the Confluent Platform it is integrated with Confluent Kubernetes Operator (CKO).</p> <p>We assume reader has good understanding and knowledge of Kubernetes, and kubectl.</p> <p>Let start to review the Apache Flink Kubernetes Operator concepts.</p>"},{"location":"coding/k8s-deploy/#apache-flink-kubernetes-operator-concepts","title":"Apache Flink Kubernetes Operator Concepts","text":"<p>Apache Flink Kubernetes Operator(FKO) acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. This note summarizes how to use this operator. </p> <p>The operator takes care of submitting, savepointing, upgrading and generally managing Flink jobs using the built-in Flink Kubernetes integration. The operator fully automates the entire lifecycle of the job manager, the task managers, and the applications. A FlinkDeployment is a manifest to define what needs to be deployed, and then the FKO manages the deployment by using Kubernetes deployments, pods... It supports query on the custom resources it manages. </p> Figure 1: Apache Flink Kubernetes Operator to manage Flink Job and Task managers <p>Failures of Job Manager pod are handled by the Deployment Controller which takes care of spawning a new Job Manager.</p> <p>As any Kubernetes operators, FKO can run namespace-scoped, to get multiple versions of the operator in the same Kubernetes cluster, or cluster-scoped for highly distributed  deployment. The operator maps its custom resources to existing Kubernetes resources of deployments, replica sets, config maps, secrets, service accounts...</p> <p>The following figure represents a simple deployment view of a Flink Cluster, in parallel of a Kafka cluster running on a Kubernetes platform:</p> Figure 2: Flink and Kafka OSS - K8S deployment <p>The FKO may have two instances running in parallel. A Flink application may run on its own namespace and will be one jb manager and n task managers.  The Kafka cluster runs in its own namespace. File services are needed for Flink to persist checkpoints and savepointes. </p>"},{"location":"coding/k8s-deploy/#confluent-for-kubernetes-operator-cfk","title":"Confluent for Kubernetes Operator (CFK)","text":"<p>The deployments for Confluent Platform and Confluent Manager for Flink, may look like in the following figure:</p> Figure 3: K8S deployment <p>CFK Operator is the control plane for deploying and managing Confluent in your Kubernetes private cloud environment. It defines custom resource definitions to support Kafka based resources like topics, brokers, connectors. It also defines how to deploy Schema Registry.</p> <p>Once you have a Kubernetes cluster the approach is to first deploy the CKO and then define Kafka resources using manifests:</p> <ul> <li>See This makefile and documentation.</li> <li>See the KraftCluster manifest used for all demonstrations in this repository.</li> </ul>"},{"location":"coding/k8s-deploy/#confluent-manager-for-flink-cmf","title":"Confluent Manager for Flink (CMF)","text":"<p>Confluent Manager for Flink (CMF) is a Kubernetes operator, to manage Flink Applications, Environments, Compute pools, Catalogs, we will detail those in a later section.</p> <p>The following figure illustrates the relationships between those operators:</p> Figure 4: The operators playing together <p>It is important to note that all CR deployments via <code>kubectl</code> go to the CFK with the <code>apiVersion: platform.confluent.io/v1beta1</code>. CRs touching Flink resources are delegated to the CMF operator. While deploying Flink components via the Confluent CLI, the CMF CRDs use different apiVersion. Therefore it is possible to run CMF without CFK. Any CR with <code>cmf.confluent.io/v1</code> as apiVersion needs to be created with confluent CLI, as using <code>kubectl</code> will not work because the CRDs are not known by Kubernetes.</p> <ul> <li>Helpful commands to work on CRDs:</li> </ul> <pre><code>kubectl get crds | grep confluent\nkubectl describe crd kafkatopics.platform.confluent.io  \n</code></pre> <p>The examples in Confluent github provides scenario workflows to deploy and manage Confluent on Kubernetes including Flink and this article: How to Use Confluent for Kubernetes to Manage Resources Outside of Kubernetes covers part of the deployment. </p>"},{"location":"coding/k8s-deploy/#custom-resources","title":"Custom Resources","text":""},{"location":"coding/k8s-deploy/#apache-flink-specific-custom-resources","title":"Apache Flink specific custom resources","text":"<p>A Flink Application is any user's program that spawns one or multiple Flink jobs from its <code>main()</code> method and is deploying a JobManager and n Task managers. They may run in their own namespace. </p> <p>The Flink Kubernetes Operator is looking at different <code>Flink Deployment</code>, so it can be isolated within its own namespace. When deploying the FKO it is important to specify the namespaces to watch for future deployments. The following command modify this list: <pre><code>helm upgrade --install cp-flink-Kubernetes-operator --version \"~1.120.0\"  confluentinc/flink-Kubernetes-operator --set watchNamespace=\"{flink, confluent, el-demo, rental}\" -n flink\n</code></pre></p> <p>It is important to delete the operator pod and let Kubernetes restarts the FKO pod with the new config.</p> <p>The custom resource definition that describes the schema of a FlinkDeployment is a cluster wide resource. The Operator continuously tracks cluster events relating to the <code>FlinkDeployment</code> and <code>FlinkSessionJob</code> custom resources. The operator control flow is described in this note.. The important points to remember are:</p> <ul> <li>The operator control flow is: 1/ Observes the status of the currently deployed resource. 2/ Validates the new resource spec, 3/ Reconciles any required changes based on the new spec and the observed status.</li> <li>The Observer module assesses the current stateus of any deployed Flink resources. </li> <li>Observer is responsible for application upgrade.</li> <li>The job manager is validated via a call to its REST api and the status is recorded in the <code>jobManagerDeploymentStatus</code></li> <li>A Job cannot be in running state without a healthy jobmanager.</li> </ul>"},{"location":"coding/k8s-deploy/#confluent-manager-for-flink-and-fko","title":"Confluent Manager for Flink and FKO","text":"<p>Confluent Manager for Apache Flink\u00ae (CMF) manages a fleet of Flink Applications (cluster) across multiple <code>Environments</code>. CP Console is integrated with CMF. CMF exposes a REST API and cli integration for managing Flink statements.</p> <p>CMF integrates with FKO to support Flink native custom resources. The following figure illustrates the current (Oct 2025) configuration of Flink solution deployment using the different CRs apiVersion.</p> Figure 5: CFK, CMF and CKO <ul> <li>Flink Environments can be created with Manifests or using the Confluent CLI. The metadata is persisted in an embedded database. The 'Environment' concept is to group multiple Flink applications together. This is an isolation layer for RBAC, and to define Flink Configuration cross compute pools and applications deployed within an environment. Flink Configuration may include common observability and checkpointing storage (HDFS or S3) definitions. See one definition of FlinkEnvironment.</li> <li>a REST API supports all the external integration to the operator. Confluent Control Center and  the <code>confluent</code> cli are using this end point.</li> <li>CMF manages FlinkDeployment resources internally</li> <li>Confluent For Kubernetes is the Confluent operator to manage Kafka resources, but it supports the deployment of FlinkEnvironment and an \"adapted\" FlinkApplication (to reference an Environment) so kubectl command can access this operator for confluent platform CRs ( <code>apiVersion: platform.confluent.io/v1beta1</code>).</li> </ul> <p>It is still possible to do pure OSS FlinkDeployment CRs but this strongly not recommended to leverage the full power of Confluent Platform and get Confluent Support.</p> <p>Let review the Kubernetes custom resources for Flink.</p>"},{"location":"coding/k8s-deploy/#flink-custom-resources","title":"Flink Custom Resources","text":"<p>Once the Flink for Kubernetes Operator is running, we can submit jobs using  <code>FlinkDeployment</code> (for Flink Application or for Job manager and task manager for session cluster) and <code>FlinkSessionJob</code> Custom Resources for Session. The following figure represents those concepts: </p> FKO main Custom Resources Definitions <p>On the left, a <code>FlinkSessionJob</code> references an existing FlinkDeployment as multiple session jobs can run into the same Flink cluster. The <code>job</code> declaration specifies the code to run with its specific configuration. While on the right, the application mode, has the job definition as part of the FlinkDeployment, as the JobManager and TaskManager mininum resource requirements.</p> <p>The Apache Flink FlinkDeployment spec is here and is used to define Flink application (will have a job section) or session cluster (only job and task managers configuration).</p> <p>It is important to note that <code>FlinkDeployment</code> and <code>FlinkApplication</code> CRDs have a podTemplate, so ConfigMap(s) and Secret(s) can be used to configure environment variables for the flink app. (Be sure to keep the container name as <code>flink-main-container</code>)</p> <pre><code>spec:\n  podTemplate:\n    spec:\n      containers:\n        - name: flink-main-container\n          envFrom:\n            - configMapRef:\n                name: flink-app-cm\n</code></pre>"},{"location":"coding/k8s-deploy/#confluent-specific-crs","title":"Confluent Specific CRs","text":"<p>First an important document to read: The Operator API references.</p> <p>Confluent Managed for Flink manages Flink application mode only and is using its own CRDs to define <code>FlinkEnvironment</code> and <code>FlinkApplication</code>. The CRDs are defined here. To be complete, it also define KafkaCatalog and ComputePool CRDs to map the same concepts introduced by Confluent Cloud.</p> <ul> <li>The new CRs for Environment, Application,  Compute pool, and Flink Catalog:</li> </ul> Confluent Manager for Flink - Custom Resources Definitions <ul> <li>An FlinkEnvironment define access control to flink resources and may define FlinkConfigurations cross applications. Environment level has precedence over Flink configuration for individual Flink applications. See one example in deployment/k8s/cmf.    <pre><code>apiVersion: platform.confluent.io/v1beta1\nkind: FlinkEnvironment\nmetadata:\n  name: dev-env\n  namespace: confluent\nspec:\n  kubernetesNamespace: el-demo\n  flinkApplicationDefaults: \n    metadata:\n      labels:\n        env: dev-env\n    spec:\n      flinkConfiguration:\n        taskmanager.numberOfTaskSlots: '1'\n        state.backend.type: rocksdb\n        state.checkpoints.dir: 's3a://flink/checkpoints'\n        state.savepoints.dir: 's3a://flink/savepoints'\n        state.backend.incremental: 'true'\n        state.backend.rocksdb.use-bloom-filter: 'true'\n        state.checkpoints.num-retained: '3'\n        ...\n      podTemplate:\n        metadata:\n          name: flink-pod-template\n        spec:\n          containers:\n          - name: flink-main-container\n            env:\n            - name: S3_ENDPOINT\n              valueFrom:\n                secretKeyRef:\n                  name: minio-s3-credentials\n                  key: s3.endpoint\n          ...\n  cmfRestClassRef:\n    name: default\n    namespace: confluent\n</code></pre></li> </ul> <p>Some important elements to consider are: </p> <ul> <li> <p><code>kubernetesNamespace</code> is the namespace where the Flink deployment(s) will be deployed. So one environment establishes foundations for those Flink applications. It can define default Flink configuration for all applications and add common labels, like specifying the environment name they run in. <code>FlinkApplication</code> is referencing back the Flink Environment which is not what Flink OSS Application does. The last piece is the <code>cmfRestClassRef</code> to reference the Kubernetes object/resource used to define access point to the CMF REST api.</p> </li> <li> <p><code>CMFRestClass</code> defines the client configuration to access CMF Rest APIs. This resource is referenced by other CFK resources (ex FlinkEnvironment, FlinkApplication) to access CMF Rest APIs. In a more advance configuration, this CR defines security like the authentication mechanism and mTLS to access the REST api.   <pre><code>apiVersion: platform.confluent.io/v1beta1\nkind: CMFRestClass\nmetadata:\n  name: default\n  namespace: confluent\nspec:\n  cmfRest:\n    endpoint: http://cmf-service.confluent.svc.cluster.local\n</code></pre></p> </li> <li> <p><code>FlinkApplication</code> in the context of Confluent Manager for Flink is the same as Flink OSS but it supports references to Environment and to the CMFRestClass. Every application runs on its own cluster, providing isolation between all applications.</p> </li> <li>Service Account: Service accounts provide a secure way for applications (like Flink jobs deployed via CMF) to interact with Confluent Cloud resources (e.g., Kafka clusters, Schema Registry) without relying on individual user credentials. Service accounts are central to the RBAC system. Need one service account per application or most likely per environment. The SA, cluster role, role and the role bindings need to be defined in the target namespace where the Flink app will be deployed. See this example for one application or this one based on Table API</li> <li>KafkaCatalog is used to expose Kafka Topics as Tables for Flink. This CRD defines a Kafka Catalog object to connect to Kafka Cluster(s) and Schema Registry. Each referenced Kafka Cluster is mapped as a Database. See the kafka catalog example in external lookup demo or rental demo</li> <li>ComputePools are used in the context of Flink SQL to execute SQL queries or statements.  The ComputePool will only be used when the statement is deployed which happens after the compilation. It is a second level of Flink configuration for Flink cluster settings. See the kafka catalog example in external lookup demo. One important element is to specify the <code>image</code> attribute to referent a flink with SQL like <code>confluentinc/cp-flink-sql:1.19-cp1</code>. See docker hub for last tags.</li> </ul> <p>The configuration flexibility:</p> <ul> <li>FlinkConfiguration defined  at the environment level can apply to all compute pools of this environment, and applications</li> <li>Compute pool configuration can apply to all SQL statements executed within the compute pool</li> <li>Flink Application has its own configuration, knowing that an application can be done with DataStream, or TableAPI.</li> </ul>"},{"location":"coding/k8s-deploy/#installation","title":"Installation","text":"<p>The Components to install for each deployment approach:</p> Confluent PlatformOpen Source Approach <p>In the context of a Confluent Platform deployment, the components to install are represented in the following figure from bottom to higher layer:</p> <p></p> <p>For an equivalent open source the components are:</p> <p></p>"},{"location":"coding/k8s-deploy/#prerequisites","title":"Prerequisites","text":"<p>Any Kubernetes deployment should include the following pre-requisites:</p> <ul> <li>kubectl </li> <li>A Kubernetes cluster. Colima for local Kubernetes. See start colima with deployment/k8s/start_colima.sh or <code>make start_colima</code> under deployment/k8s folder.</li> <li> <p>Be sure to have helm cli installed: (see installation instructions)   <pre><code># for mac\nbrew install helm\n# or \nbrew upgrade helm\n# for WSL2 - ubuntu\nsudo apt-get install helm\n</code></pre></p> </li> <li> <p>Install Confluent CLI or update existing CLI with:    <pre><code>confluent update\n</code></pre></p> </li> </ul>"},{"location":"coding/k8s-deploy/#colima-playground","title":"Colima playground","text":"<p>See the Colima installation instructions.</p> <ul> <li>Start a Kubernetes cluster, using one of the following options:   <pre><code>colima start --Kubernetes\n# or under deployment/k8s folder\n./start_colima.sh\n# or using make under deployment/k8s\nmake start_colima\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#external-components","title":"External Components","text":"<p>The certificate manager and minio operator may be deploy in one command under <code>deployment/k8s</code>:    <pre><code>make deploy\n</code></pre></p>"},{"location":"coding/k8s-deploy/#certificate-manager","title":"Certificate manager","text":"<ul> <li> <p>See certificate manager current releases, and update the CERT_MGR_VERSION=v1.18.1 in the Makefile, then run the command:   <pre><code>make deploy_cert_manager\n</code></pre></p> </li> <li> <p>Which is doing:   <pre><code>kubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.18.1/cert-manager.yaml\n</code></pre></p> </li> <li> <p>Verify deployment with   <code>sh     kubeclt get pods -n cert-manager     # or     make verify_cert_manager</code></p> </li> </ul>"},{"location":"coding/k8s-deploy/#using-minio","title":"Using MinIO","text":"<p>MinIO is an object storage solution that provides an Amazon Web Services S3-compatible API and supports all core S3 features, on k8s. It may be used for Flink checkpoint and snapshot persistenace, or when deploying application jar file to Flink, as a file storage.</p> <ul> <li> <p>First be sure to have the MinIO CLI installed.      <pre><code>brew install minio/stable/mc\n# or to upgrade to a new version\nbrew upgrade minio/stable/mc\n# Verify installation\nmc --help\n</code></pre></p> <p>mc cli command summary</p> </li> <li> <p>Deploy Minio operator under <code>minio-dev</code> namespace, using `Make     <pre><code>make deploy_minio\nmake verify_minio\n</code></pre></p> </li> <li> <p>Access MinIO S3 API and Console     <pre><code>kubectl port-forward pod/minio 9000 9090 -n minio-dev\n# or\nmake port_forward_minio_console\n</code></pre></p> </li> <li> <p>Log in to the Console with the credentials <code>minioadmin | minioadmin</code></p> </li> <li>Setup a minio client with credential saved to  $HOME/.mc/config.json     <pre><code>mc alias set dev-minio http://localhost:9000 minioadmin minioadmin\n# make a bucket\nmc mb dev-minio/flink\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#cfk-installation","title":"CFK installation","text":"<p>See this Makefile and Readme</p> <p>See the Confluent Platform product installation documentation for details. We can summarize as: </p> <ul> <li>The deployment leverages Kubernetes native API to configure, deploy, and manage Kafka cluster, Connect workers, Schema Registry, Confluent Control Center, Confluent REST Proxy and application resources such as topics.</li> <li> <p>The following diagram illustrates those components in one namespace.      Confluent Platform Components - k8s deployment </p> </li> <li> <p>Under the deployment/k8s/cfk folder, run <code>make deploy</code> which will do the following operations:</p> <ul> <li>Create a namespace for Confluent products deployment. By default, it deploys Confluent Platform in the namespaced deployment, and it manages Confluent Platform components in this namespace . </li> <li>Add Confluent Platform Helm repositories</li> <li>Deploy Confluent Kafka Broker using one Kraft controller, one to three brokers, the new Confluent Center Console, with REST api and Schema Registry.     <pre><code>      NAME                                  READY   STATUS      RESTARTS          AGE\nconfluent-operator-764dbdf6f9-6f7gx   1/1     Running     158 (5h41m ago)   89d\ncontrolcenter-ng-0                    3/3     Running     13 (5h41m ago)    32d\nkafka-0                               1/1     Running     4 (5h41m ago)     32d\nkraftcontroller-0                     1/1     Running     4 (5h41m ago)     32d\nschemaregistry-0                      1/1     Running     7 (5h41m ago)     32d\n</code></pre></li> <li> <p>The console may be accessed via port-forwarding:     <pre><code>kubectl -n confluent port-forward svc/controlcenter-ng 9021:9021 \nchrome localhost:9021\n</code></pre></p> <p></p> </li> </ul> </li> <li> <p>See Confluent Platform releases information.</p> </li> </ul>"},{"location":"coding/k8s-deploy/#confluent-manager-for-flink-cmf_1","title":"Confluent Manager for Flink (CMF)","text":"<p>Updated 10.18.2025: For CFK version 2.0.3 and CP v8.0.2</p> <p>See the Makefile under deployment/k8s/cmf which includes a set of targets to simplify the deployment. See Confluent Manager for Flink product documentation for deeper information. The following steps are a summary of what should be done.</p> <ul> <li>Install Confluent Manager for Flink operator, under <code>deployment/k8s/cmf</code> <pre><code>make help\nmake deploy\nmake status\n</code></pre></li> </ul> <p>See deploy application section for SQL or Java app deployment</p>"},{"location":"coding/k8s-deploy/#ha-configuration","title":"HA configuration","text":"<p>Within Kubernetes, we can enable Flink HA in the ConfigMap of the cluster configuration that will be shared with deployments:</p> <pre><code>  flinkConfiguration:\n    taskmanager.numberOfTaskSlots: \"2\"\n    state.backend: rockdb\n    state.savepoints.dir: file:///flink-data/savepoints\n    state.checkpoints.dir: file:///flink-data/checkpoints\n    high-availability.type: Kubernetes\n    high-availability.storageDir: file:///flink-data/ha\n    job.autoscaler.enabled: true\n</code></pre> <p>This configuration settings is supported via FKO.  See product documentation, and the autoscaler section for deeper parameter explanations. The Flink autoscaler monitors the number of unprocessed records in the input (pending records), and will allocate more resources to absorb the lag. It adjusts parallelism at the flink operator level within the DAG. </p> <p>JobManager metadata is persisted in the file system specified by <code>high-availability.storageDir</code> . This <code>storageDir</code> stores all metadata needed to recover a JobManager failure.</p> <p>JobManager Pods, that crashed, are restarted automatically by the Kubernetes scheduler, and as Flink persists metadata and the job artifacts, it is important to mount pv to the expected paths.</p> <pre><code>podTemplate:\n  spec:\n    containers:\n      - name: flink-main-container\n        volumeMounts:\n        - mountPath: /flink-data\n          name: flink-volume\n    volumes:\n    - name: flink-volume\n      hostPath:\n        # directory location on host\n        path: /tmp/flink\n        # this field is optional\n        type: Directory\n</code></pre> <p>Recall that <code>podTemplate</code> is a base declaration common for job and task manager pods. Can be overridden by the jobManager and taskManager pod template sub-elements (spec.taskManager.podTemplate). The previous declaration will work for local k8s with hostPath access, for Kubernetes cluster with separate storage class then the volume declaration is:</p> <pre><code>volumes:\n  - name: flink-volume\n    persistenceVolumeClaim:\n      claimName: flink-pvc\n</code></pre> <p>podTemplate can include nodeAffinity to allocate taskManager to different node characteristics:</p> <pre><code>  podTemplate:\n      spec:\n        affinity:\n          nodeAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n              nodeSelectorTerms:\n                - matchExpressions:\n                    - key: cfk-cr\n                      operator: In\n                      values:\n                        - flink\n        tolerations:\n          - key: cfk-cr\n            operator: Equal\n            value: flink\n            effect: NoSchedule\n</code></pre>"},{"location":"coding/k8s-deploy/#documentations","title":"Documentations","text":"<ul> <li>Confluent Platform for Flink has another operator integrated with FKO. See my CP Flink summary.</li> <li>Confluent Flink operator documentation</li> <li>Getting started with Flink OSS Standalone Kubernetes Setup.</li> <li>Apache Flink Native Kubernetes deployment.</li> <li>A Confluent Platform demonstration git repo: confluentinc/confluent-demo</li> </ul> <ul> <li>Next steps is to upload jar files for the different applications to deploy, or data sets for SQL table. See application section.</li> </ul> <p>TO UPDATE </p>"},{"location":"coding/k8s-deploy/#durable-storage","title":"Durable Storage","text":"<p>Durable storage is used to store consistent checkpoints of the Flink state. Review the state management section in the concept chapter. The checkpoints are saved to object storage compatible with S3, or HDFS protocol. The FlinkConfiguration can be set at the Application, ComputePool or Environment level.</p> <p>Two important elements to configure:  1. the environment variable ENABLE_BUILT_IN_PLUGINS 1. The <code>state.checkpoints.dir</code> to the location of S3 bucket.</p> <p>The following is a configuration using minio and the presto S3FileSystem which is a specific implementation (created by Presto) of the file system interface within Apache Flink. (See the S3FileSystemFactory class). </p> <pre><code>\"flinkConfiguration\": {\n        \"pipeline.operator-chaining.enabled\": \"false\",\n        \"execution.checkpointing.interval\": \"10s\",\n        \"taskmanager.numberOfTaskSlots\": \"4\",\n        \"fs.s3.impl\": \"org.apache.flink.fs.s3presto.S3FileSystem\",\n        \"presto.s3.endpoint\": \"http://minio.minio-dev.svc.cluster.local:9000\",\n        \"presto.s3.path.style.access\": \"true\",\n        \"presto.s3.connection.ssl.enabled\": \"false\",\n        \"presto.s3.access-key\": \"admin\",\n        \"presto.s3.secret-key\": \"admin123\",\n        \"state.checkpoints.dir\": \"s3://flink/stateful-flink/checkpoints\",\n        \"state.savepoints.dir\": \"s3://flink/stateful-flink/savepoints\",\n        \"state.checkpoints.interval\": \"10000\",\n        \"state.checkpoints.timeout\": \"600000\"\n\n      },\n</code></pre> <p>For Minio settings:</p> <pre><code>  s3.endpoint: http://minio.minio-dev.svc.cluster.local:9000\n  s3.path.style.access: \"true\"\n  s3.connection.ssl.enabled: \"false\"\n  s3.access-key: minioadmin\n  s3.secret-key: minioadmin\n  state.checkpoints.dir: s3://flink/stateful-flink/checkpoints\n  state.savepoints.dir: s3://flink/stateful-flink/savepoints\n  state.checkpoints.interval: \"10000\"\n  state.checkpoints.timeout: \"600000\"\n</code></pre> <p>TO BE CONTINUED</p> <p>A RWX, shared PersistentVolumeClaim (PVC) for the Flink JobManagers and TaskManagers provides persistence for stateful checkpoint and savepoint of Flink jobs. </p> <p>A flow is a packaged as a jar, so developers need to define a docker image with the Flink API and any connector jars. Example of Dockerfile and FlinkApplication manifest.</p> <p>Also one solution includes using MinIO to persist application jars.</p>"},{"location":"coding/k8s-deploy/#apache-flink-oss","title":"Apache Flink OSS","text":"<ul> <li> <p>Add the Apache Flink Helm repositories:      <pre><code>helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-Kubernetes-operator-1.11.0\n# Verify help repo entries exist\nhelm repo list\n# Be sure to change the repo as the URL may not be valid anymore\nhelm repo remove  flink-operator-repo\n# try to update repo content\nhelm repo update\n</code></pre></p> <p>See Apache Flink Operator documentation </p> </li> </ul>"},{"location":"coding/k8s-deploy/#deploy-apache-flink-kubernetes-operator","title":"Deploy Apache Flink Kubernetes Operator","text":"<p>The Apache flink Kubernetes operator product documentation lists the setup steps.</p> <ul> <li>Get the list of Apache Flink releases and tags here </li> <li>To get access to k8s deployment manifests and a Makefile to simplify deployment, of Apache Flink, or Confluent Platform on k8s (local colima or minikube) see the deployment/k8s/flink-oss folder.    <pre><code>make prepare\nmake verify_flink\nmake deploy_basic_flink_deployment  \n</code></pre></li> <li>Access Flink UI</li> <li>Mount a host folder as a PV to access data or SQL scripts, using hostPath.</li> </ul>"},{"location":"coding/k8s-deploy/#flink-config-update","title":"Flink Config Update","text":"<ul> <li> <p>If a write operation fails when the pod creates a folder or updates the Flink config, verify the following:</p> <ul> <li>Assess PVC and R/W access. Verify PVC configuration. Some storage classes or persistent volume types may have restrictions on directory creation</li> <li>Verify security context for the pod. Modify the pod's security context to allow necessary permissions.</li> <li>The podTemplate can be configured at the same level as the task and job managers so any mounted volumes will be available to those pods. See basic-reactive.yaml from Flink Operator examples.</li> </ul> </li> </ul> <p>See PVC and PV declarations</p>"},{"location":"coding/k8s-deploy/#flink-session-cluster","title":"Flink Session Cluster","text":"<p>For Session cluster, there is no jobSpec. See this deployment definition. Once a cluster is defined, it has a name and can be referenced to submit SessionJobs.</p> <p>A SessionJob is executed as a long-running Kubernetes Deployment. We may run multiple Flink jobs on a Session cluster. Each job needs to be submitted to the cluster after the cluster has been deployed. To deploy a job, we need at least three components:</p> <ul> <li>a Deployment which runs a JobManager</li> <li>a Deployment for a pool of TaskManagers</li> <li>a Service exposing the JobManager\u2019s REST and UI ports</li> </ul> <p>For a deployment select the execution mode: <code>application, or session</code>. For production it is recommended to deploy in <code>application</code> mode for better isolation, and using a cloud native approach. We can just build a dockerfile for our application using the Flink jars.</p>"},{"location":"coding/k8s-deploy/#session-deployment","title":"Session Deployment","text":"<p>Flink has a set of examples like the Car top speed computation with simulated record. As this code is packaged in a jar available in maven repository, we can declare a job session.</p> <p>Deploy a config map to define the <code>log4j-console.properties</code> and other parameters for Flink (<code>flink-conf.yaml</code>)</p> <p>The diagram below illustrates the standard deployment of a job on k8s with session mode:</p> <p></p> <p>src: apache Flink site</p> <pre><code>apiVersion: flink.apache.org/v1beta1\nkind: FlinkSessionJob\nmetadata:\n  name: car-top-speed-job\nspec:\n  deploymentName: flink-session-cluster\n  job:\n    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.17.2/flink-examples-streaming_2.12-1.17.2-TopSpeedWindowing.jar\n    parallelism: 4\n    upgradeMode: stateless\n</code></pre> <p>Before deploying this job, be sure to deploy a session cluster using the following command:</p> <pre><code># under deployment/k8s\nkubectl apply -f basic-job-task-mgrs.yaml \n</code></pre> <p>Once the job is deployed we can see the pod and then using the user interface the job continuously running:</p> <ul> <li> <p>Example of deploying Java based SQL Runner to interpret a Flink SQL script: package it as docker images, and deploy it with a Session Job. There is a equivalent for Python using Pyflink.</p> <ul> <li>See the ported code for Java</li> <li>And for the Python implementation</li> </ul> </li> </ul>"},{"location":"coding/k8s-deploy/#flink-state-snapshot","title":"Flink State Snapshot","text":"<p>To help managing snapshots, there is another CR called FlinkStateSnapshot</p>"},{"location":"coding/k8s-deploy/#flink-application-deployment","title":"Flink Application Deployment","text":"<p>There two types of Flink application: the java packaging or the SQL client with open session to the cluster.</p>"},{"location":"coding/k8s-deploy/#flink-sql-processing","title":"Flink SQL processing","text":"<p>There are multiple choices to run Flink SQL: using the SQL client, or package the SQL scripts in a docker container with the java SQL runner executing the SQL statements from a file, or use the Table API. The application deployment is Java based even if SQL scripts are used for stream processing.</p> <p>With Apache Flink OSS, Flink Session Cluster is the most suitable deployment mode for the SQL Client. This is a long-running Flink cluster (JobManager and TaskManagers) on which you can submit multiple jobs to. The sql client is a long-running, interactive application that submits jobs to an existing cluster.</p> <p>For Confluent Manager For Flink the recommended approach is to use a Flink Application, which per design, is one Job manager with multiple Task managers or use the Flink SQL Shell.</p>"},{"location":"coding/k8s-deploy/#confluent-manager-for-flink","title":"Confluent Manager for Flink","text":"<p>As seen previously in Confluent Manager for Flink the method is to create an Environment and Compute pool to run the SQL statements in a pool. Those concepts and components are the same as the Confluent Cloud for Flink.</p> <ul> <li> <p>Be sure that the port-forward to the svc/cmf-service is active.</p> </li> <li> <p>Define an environment:    <pre><code>export CONFLUENT_CMF_URL=http://localhost:8084\n# be sure to not be connected to confluent cloud, if not do:\nconfluent logout\n# Look at current environment\nconfluent flink environment  list\n# Create new env\nconfluent flink environment create dev --kubernetes-namespace el-demo\n# or under deployment/k8s/cmf\nmake create_flink_env\n</code></pre></p> </li> <li> <p>Define a compute pool (verify current docker image tag) and see the compute_pool.json <pre><code>make create_compute_pool\n</code></pre></p> </li> <li> <p>Flink SQL uses the concept of Catalogs to connect to external storage systems. CMF features built-in KafkaCatalogs to connect to Kafka and Schema Registry.  A <code>KafkaCatalog</code> exposes Kafka topics as tables and derives their schema from Schema Registry. Define a Flink Catalog as json file: (see cmf/kafka_catalog.json). The catalog is configured with connection properties for the Kafka and Schema Registry clients.   <pre><code>make create_kafka_catalog\n</code></pre></p> </li> <li> <p>Define secret to access Kafka Cluster See this secret and the mapping <pre><code>make create_kafka_secret\nmake create_env_secret_mapping\n</code></pre></p> </li> <li> <p>Use the confluent cli to start a Flink  SQL shell   <pre><code>confluent --environment dev --compute-pool pool1 flink shell --url http://localhost:8084\n</code></pre></p> </li> </ul>"},{"location":"coding/k8s-deploy/#apache-flink-oss_1","title":"Apache Flink (OSS)","text":"<p>You can run the SQL Client in a couple of ways:</p> <ul> <li>As a separate Docker container: The Flink Docker images include the SQL Client. You can run a container and connect to the JobManager. You will need to mount a volume to persist SQL scripts and other data.   <pre><code>kubectl exec -it &lt;sql-client-pod-name&gt; -- /opt/flink/bin/sql-client.sh\n</code></pre></li> </ul> <p>When running the SQL Client as a pod within the same Kubernetes cluster, you can use the internal DNS name of the JobManager service to connect. The format is typically ..svc.cluster.local <ul> <li>Locally: Download the Flink distribution, extract it, and run the SQL Client from your local machine.   <pre><code># port forwarding\nkubectl port-forward svc/&lt;jobmanager-service-name&gt; 8081:8081\n\n./bin/sql-client.sh -s &lt;jobmanager-service-name&gt;:8081\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#flink-application","title":"Flink Application","text":"<p>An application deployment must define the job (JobSpec) field with the <code>jarURI</code>, <code>parallelism</code>, <code>upgradeMode</code> one of (stateless/savepoint/last-state) and the desired <code>state</code> of the job (running/suspended). See this sample app or the cmf_app_deployment.yaml in the e-com-sale demonstration.</p> <p>Here is an example of FlinkApplication, the CRD managed by the CMF operator:</p> <pre><code>apiVersion: \"cmf.confluent.io/v1\nkind: FlinkApplication\"\nspec:\n  flinkVersion: v1_19\n  image: confluentinc/cp-flink:1.19.1-cp2  # or a custom image based on this one.\n  job:\n      jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar\n      # For your own deployment, use your own jar\n      jarURI: local:///opt/flink/usrlib/yourapp01.0.0.jar\n      parallelism: 2\n      upgradeMode: stateless\n      state: running\n  jobManager: \n    resource: \n      cpu: 1\n      memory: 1048m\n  taskManager: \n    resource: \n      cpu: 1\n      memory: 1048m\n</code></pre> <ul> <li>See manage Flink app using Confluent for Flink</li> </ul>"},{"location":"coding/k8s-deploy/#fault-tolerance","title":"Fault tolerance","text":"<p>For Flink job or application that are stateful and for fault tolerance, it is important to enable checkpointing and savepointing:</p> <pre><code>job:\n  jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar\n\n  parallelism: 2\n  upgradeMode: savepoint\n  #savepointTriggerNonce: 0\n  # initialSavepointPath: file:///\n</code></pre> <p>The other upgradeMode is ``</p> How to validate checkpointing? <p>Checkpointing let Flink to periodically save the state of a job into local storage.  Look at the pod name of the task manager and stop it with <code>kubectl delete pod/....</code> Flink should automatically restart the job and recover from the latest checkpoint. Use the Flink UI or CLI to see the job status.</p> How to validate savepointing? <p>Savepoints are manually triggered snapshots of the job state, which can be used to upgrade a job or to perform manual recovery. To trigger a savepoint we need to set a value into <code>savepointTriggerNonce</code> in the FlinkDeployment descriptor and then apply the changes.  Get the location of the save point and then add to the yaml <code>initialSavepointPath</code> to redeploy the applicationL: it will reload its state from the savepoint. There is a custom resource definition (FlinkStateSnapshotSpec) to trigger savepoints. </p> <p><code>flinkConfiguration</code> is a hash map used to define the Flink configuration, such as the task slot, HA and checkpointing parameters.</p> <pre><code>  flinkConfiguration:\n    high-availability.type: org.apache.flink.Kubernetes.highavailability.KubernetesHaServicesFactory\n    high-availability.storageDir: 'file:///opt/flink/volume/flink-ha'\n    restart-strategy: failure-rate\n    restart-strategy.failure-rate.max-failures-per-interval: '10'\n    restart-strategy.failure-rate.failure-rate-interval: '10 min'\n    restart-strategy.failure-rate.delay: '30 s'\n    execution.checkpointing.interval: '5000'\n    execution.checkpointing.unaligned: 'false'\n    state.backend.type: rocksdb\n    state.backend.incremental: 'true'\n    state.backend.rocksdb.use-bloom-filter: 'true'\n    state.checkpoints.dir: 'file:///opt/flink/volume/flink-cp'\n    state.checkpoints.num-retained: '3'\n    state.savepoints.dir: 'file:///opt/flink/volume/flink-sp'\n    taskmanager.numberOfTaskSlots: '10'\n    table.exec.source.idle-timeout: '30 s'\n</code></pre> <p>The application jar needs to be in a custom Flink docker image built using the Dockerfile as in e-com-sale-demo, or uploaded to a MinIO bucket. </p> <p>The following Dockerfile is used for deploying a solution in application mode, which packages the Java Flink jars with the app, and any connector jars needed for the integration and starts the <code>main()</code> function.</p> <pre><code>FROM confluentinc/cp-flink:1.19.1-cp2\nRUN mkdir -p $FLINK_HOME/usrlib\nCOPY /path/of/my-flink-job-*.jar $FLINK_HOME/usrlib/my-flink-job.jar\n</code></pre> <ul> <li> <p>With Confluent Platform for Flink:</p> <pre><code>  # First be sure the service is expose\n  kubectl port-forward svc/cmf-service 8080:80 -n flink\n  # Deploy the app given its deployment\n  confluent flink application create k8s/cmf_app_deployment.yaml  --environment $(ENV_NAME) --url http://localhost:8080 \n</code></pre> </li> </ul> Access to user interface <p>To forward your jobmanager\u2019s web ui port to local 8081.</p> <pre><code>kubectl port-forward ${flink-jobmanager-pod} 8081:8081 \n# Or using confluent cli CP for Flink command:\nconfluent flink application web-ui-forward $(APP_NAME) --environment $(ENV_NAME) --port 8081 --url http://localhost:8080\n</code></pre> <p>And navigate to http://localhost:8081.</p>"},{"location":"coding/k8s-deploy/#using-minio-for-app-deployment","title":"Using MinIO for app deployment","text":"<ul> <li> <p>Upload an application to minio bucket:</p> <pre><code>mc cp ./target/flink-app-0.1.0.jar dev-minio/flink/flink-app-0.1.0.jar\nmc ls dev-minio/flink\n</code></pre> </li> <li> <p>Start the application using confluent cli:</p> <pre><code>confluent flink application create --environment env1 --url http://localhost:8080 app-deployment.json\n</code></pre> </li> <li> <p>Open Flink UI:</p> <pre><code>confluent flink application web-ui-forward --environment env1 flink-app --url http://localhost:8080\n</code></pre> </li> <li> <p>Produce messages to kafka topic</p> <pre><code>echo 'message1' | kubectl exec -i -n confluent kafka-0 -- /bin/kafka-console-producer --bootstrap-server kafka.confluent.svc.cluster.local:9092 --topic in\n</code></pre> </li> <li> <p>Cleanup</p> <pre><code># the Flink app\nconfluent flink application delete kafka-reader-writer-example --environment development --url http://localhost:8080\n# the Kafka cluster\n# the operators\n</code></pre> </li> </ul>"},{"location":"coding/k8s-deploy/#practices","title":"Practices","text":"<ul> <li>It is not recommended to host a Flink Cluster across multiple Kubernetes clusters. Flink node exchanges data between task managers and so better to run in same region, and within same k8s. </li> </ul>"},{"location":"coding/stateful-func/","title":"Stateful function","text":"<p>Update 1/2025: it seems the OSS project has less traction.</p> <p>Stateful Functions is an open source framework that reduces the complexity of building and orchestrating distributed stateful applications at scale. It brings together the benefits of stream processing with Apache Flink\u00ae and Function-as-a-Service (FaaS) to provide a powerful abstraction for the next generation of event-driven architectures.</p> <p></p> <p>The Flink worker processes (TaskManagers) receive the events from the ingress systems (Kafka, Kinesis, etc.) and route them to the target functions. They invoke the functions and route the resulting messages to the next respective target functions. Messages designated for egress are written to an egress system.</p>"},{"location":"coding/table-api/","title":"Table API","text":"Update <p>Created 10/2024 - Updated 11/03/24. Reorganize in improve documentation 10/2025</p>"},{"location":"coding/table-api/#concepts","title":"Concepts","text":"<p>The TableAPI serves as the lower-level API for executing Flink SQL, allowing for stream processing implementations in Java and Python. The Table API encapsulates a stream or a physical table, enabling developers to implement streaming processing by programming against these tables.</p> <p>See the main concepts and APIs. The structure of a program looks mostly the same:</p> <ol> <li>Create a TableEnvironment for batch or streaming execution     <pre><code>import org.apache.flink.table.api.EnvironmentSettings;\nimport org.apache.flink.table.api.TableEnvironment;\n\nEnvironmentSettings settings = EnvironmentSettings\n    .newInstance()\n    .inStreamingMode()\n    //.inBatchMode()\n    .build();\n\nTableEnvironment tEnv = TableEnvironment.create(settings);\n</code></pre></li> <li>Create one or more source table(s)</li> <li>Create one or more sink Tables(s) or use the print sink</li> <li>Create processing logic using SQL string or Table API functions</li> </ol> <p>Summary of important concepts:</p> <ul> <li>The main function is a Flink client, that will compiles the code into a dataflow graph and submots to the JobManager.</li> <li>A TableEnvironment maintains a map of catalogs of tables </li> <li>Tables can be either virtual (VIEWS) or regular TABLES which describe external data.</li> <li>Tables may be temporary (tied to the lifecycle of a single Flink session), or permanent ( visible across multiple Flink sessions and clusters).</li> <li>Temportary table may shadow a permanent table.</li> <li>Tables are always registered with a 3-part identifier consisting of catalog, database, and table name.</li> <li>TableSink is a generic interface to to write results to. A batch Table can only be written to a <code>BatchTableSink</code>, while a streaming Table requires either an <code>AppendStreamTableSink</code>, a <code>RetractStreamTableSink</code>, or an <code>UpsertStreamTableSink</code>.</li> <li>A pipeline can be explained with <code>TablePipeline.explain()</code> and executed invoking <code>TablePipeline.execute()</code>.</li> <li>Recall that High-Availability in Application Mode is only supported for single-execute() applications.</li> </ul> <p>It is important to note that Table API and SQL queries can be easily integrated with and embedded into DataStream programs.</p>"},{"location":"coding/table-api/#packaging","title":"Packaging","text":"<p>TBD</p>"},{"location":"coding/table-api/#confluent-specifics","title":"Confluent Specifics","text":"<p>In Confluent Manager for Flink deployment, only Flink Application mode is supported. A Flink Application is any user's program that spawns one or multiple Flink jobs from its <code>main()</code> method. The execution of these jobs can happen in a local JVM (LocalEnvironment) or on a remote setup of clusters with multiple machines (kubernetes).</p> <p>In the context of Confluent Cloud, the Table API program acts as a client-side library for interacting with the Flink engine hosted in the cloud. It enables the submission of  <code>Statements</code> and retrieval of <code>StatementResults</code>. The provided Confluent plugin integrates specific components for configuring the TableEnvironment, eliminating the need for a local Flink cluster. By including the <code>confluent-flink-table-api-java-plugin</code> dependency, Flink's internal components\u2014such as CatalogStore, Catalog, Planner, Executor, and configuration, are managed by the plugin and fully integrated with Confluent Cloud. This integration is via the REST API, so Confluent Table API plugin is an higher emcapsulation of the CC REST API. </p>"},{"location":"coding/table-api/#getting-started","title":"Getting Started","text":"<p>The development approach includes at least the following steps:</p> <ol> <li> <p>Create a maven project with a command like:     <pre><code>mvn archetype:generate -DgroupId=j9r.flink -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.5 -DinteractiveMode=false\n</code></pre></p> </li> <li> <p>Add the flink table api, and Kafka client dependencies:</p> Open Source LibrariesConfluent Cloud for FlinkConfluent Platform for Flink <p><pre><code>&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n\n&lt;artifactId&gt;flink-java&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-clients&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-connector-base&lt;/artifactId&gt;\n&lt;!-- Depending of the serialization needs --&gt;\n&lt;artifactId&gt;flink-json&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-avro&lt;/artifactId&gt;\n&lt;!-- when using schema registry --&gt;\n&lt;artifactId&gt;flink-avro-confluent-registry&lt;/artifactId&gt;\n</code></pre> (see an example of pom.xml).  Use <code>provided</code> dependencies to get the Flink jars from the deployed product. </p> <p><pre><code> &lt;dependency&gt;\n    &lt;groupId&gt;io.confluent.flink&lt;/groupId&gt;\n    &lt;artifactId&gt;confluent-flink-table-api-java-plugin&lt;/artifactId&gt;\n    &lt;version&gt;${confluent-plugin.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> For Confluent Cloud for Flink deployment see the pom.xml in table-api/ccf-table-api folder.</p> <pre><code>\n</code></pre> </li> <li> <p>Implement and unit test the flow. See best practices for code structure.</p> </li> <li> <p>Depending of the target Flink runtime, there will be different steps: </p> For Confluent Platform for Flink:For Confluent Cloud for Flink: <ol> <li>Define a FlinkApplication CR</li> <li>Package the jar, and build a docker image using Confluent Platform for Flink base image with a copy of the app jar. (See example of Dockerfile)</li> <li>Deploy to Kubernetes using the Flink kubernetes operator</li> <li>Monitor with the web ui.</li> </ol> <ol> <li> <p>Add the  <code>io.confluent.flink.confluent-flink-table-api-java-plugin</code> into the maven dependencies and use the following Environment settings: </p> Confluent Cloud access via environment variables<pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport org.apache.flink.table.api.EnvironmentSettings;\nEnvironmentSettings settings = ConfluentSettings.fromGlobalVariables();\nTableEnvironment env = TableEnvironment.create(settings);\n</code></pre> Confluent Cloud access via properties file<pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport org.apache.flink.table.api.EnvironmentSettings;\nEnvironmentSettings settings = ConfluentSettings.fromResource(\"/cloud.properties\");\nTableEnvironment env = TableEnvironment.create(settings);\n</code></pre> <p>When using private endpoint, there is a need to be at the 1.20.50 version of the plugin (<code>confluent_flink_table_api_python_plugin=\"&gt;=1.20.50\"</code>) and use code (python example) like:</p> <pre><code>settings = ConfluentSettings.new_builder_from_file(CLOUD_PROPERTIES_PATH) \n.set_context_name(\"my-context\") \n.set_rest_endpoint(\"private.confluent.cloud\") \n.build()\n</code></pre> </li> <li> <p>Set environment variables to specify API key and secrets, Confluent Cloud environment, compute pool... </p> Confluent Cloud related  environment variables<pre><code>export CLOUD_PROVIDER=\"aws\"\nexport CLOUD_REGION=\"us-west-2\"\nexport FLINK_API_KEY=\"&lt;your-flink-api-key&gt;\"\nexport FLINK_API_SECRET=\"&lt;your-flink-api-secret&gt;\"\nexport ORG_ID=\"&lt;your-organization-id&gt;\"\nexport ENV_ID=\"&lt;your-environment-id&gt;\"\nexport COMPUTE_POOL_ID=\"&lt;your-compute-pool-id&gt;\"\n</code></pre> </li> <li> <p>Execute the java program</p> </li> </ol> </li> </ol> <p>See the Confluent Flink cookbook for more Table API and DataStream examples.</p>"},{"location":"coding/table-api/#java","title":"Java","text":"<p>Any main function needs to connect to the Flink environment. Confluent API offers a way to read cloud client properties so the running Flink application can access the Job and Task managers running in the Confluent Cloud compute pools as a service (see code example above).</p> <p>A table environment is the base class, entry point, and central context for creating Table and SQL API programs. </p> <p>TableEnvironment uses an <code>EnvironmentSettings</code> that define the execution mode. The following is a template code to run Table API program submitted to a Job Manager deployed locally or on Kubernetes (OSS Flink or CP for Flink):</p> Deployment for Kubernetes or local<pre><code>settings= EnvironmentSettings.newInstance()\n   .inStreamingMode()\n   .withBuiltInCatalogName(\"default_catalog\")\n   .withBuiltInDatabaseName(\"default_database\")\n   .build();\n\nTableEnvironment tableEnv = TableEnvironment.create(settings);\n        tableEnv.get_config().set(\"parallelism.default\", \"4\");\n</code></pre> <p>Once packaged with maven as a uber-jar the application may be executed locally to send the dataflow to the Confluent Cloud for Flink JobManager or can be deployed as a <code>FlinkApplication</code> within a k8s cluster. </p>"},{"location":"coding/table-api/#confluent-cloud-for-flink-execution","title":"Confluent Cloud for Flink execution","text":"<p>When using remote Confluent Cloud for Flink, it is possible to directly execute the java jar and it will send the DAG to the remote job manager:</p> <pre><code># set all environment variable or use /cloud.properties in resource folder\njava -jar target/flink-app-ecom-0.1.0.jar \n</code></pre> <p>See code sample: Main_00_JoinOrderCustomer.java</p> <p>Get the catalog and databases, and use the environment to get the list of tables. In Confluent Cloud, there is a predefined catalog with some table samples: <code>examples.marketplace</code>.</p> <pre><code># using a sql string\nenv.executeSql(\"SHOW TABLES IN `examples`.`marketplace`\").print();\n# or using the api\nenv.useCatalog(\"examples\");\nenv.useDatabase(\"marketplace\");\nArrays.stream(env.listTables()).forEach(System.out::println);\n# work on one table\nenv.from(\"`customers`\").printSchema();\n</code></pre>"},{"location":"coding/table-api/#python","title":"Python","text":""},{"location":"coding/table-api/#code-samples","title":"Code Samples","text":"<p>The important classes are:</p> <ul> <li>TableEnvironment</li> <li>Table</li> <li>Row</li> <li>Expressions contains static methods for referencing table columns, creating literals, and building more complex Expression chains. See below.</li> <li>Confluent Table API Tutorial</li> </ul>"},{"location":"coding/table-api/#code-structure","title":"Code structure","text":"<p>Clearly separate the creation of sources, sinks, workflow in different methods. References those methods in the main().</p>"},{"location":"coding/table-api/#create-some-test-data","title":"Create some test data","text":"<p>Use one of the TableEnvironment fromValues() methods,</p> From collectionWith Schema and list of row <pre><code>env.fromValues(\"Paul\", \"Jerome\", \"Peter\", \"Robert\")\n                .as(\"name\")\n                .filter($(\"name\").like(\"%e%\"))\n                .execute()\n                .print();\n</code></pre> <pre><code>import org.apache.flink.table.api.DataTypes;\nTable customers = env.fromValues(\n                        DataTypes.ROW(\n                                DataTypes.FIELD(\"customer_id\", DataTypes.INT()),\n                                DataTypes.FIELD(\"name\", DataTypes.STRING()),\n                                DataTypes.FIELD(\"email\", DataTypes.STRING())),\n                        row(3160, \"Bob\", \"bob@corp.com\"),\n                        row(3107, \"Alice\", \"alice.smith@example.org\"),\n                        row(3248, \"Robert\", \"robert@someinc.com\"));\n</code></pre>"},{"location":"coding/table-api/#joining-two-tables","title":"Joining two tables","text":"<p>See the example in 00_join_order_customer.java. The statements may run forever. </p>"},{"location":"coding/table-api/#a-deduplication-example","title":"A deduplication example","text":"<p>The deduplication of record over a time window is a classical pattern. See this SQL query with the following Table API implementation</p> <pre><code>Table source;\n</code></pre>"},{"location":"coding/table-api/#confluent-tools-for-printing-and-stop-statement","title":"Confluent tools for printing and stop statement","text":"<p>See this git repository</p>"},{"location":"coding/table-api/#define-data-flows","title":"Define data flows","text":"<p>A TablePipeline describes a flow of data from source(s) to sink. We can also use </p> <p>The pipeline flow can use different services, defined in separate Java classes. Those classes may be reusable. The environment needs to be passed to each service, as this is the environment which includes all the Table API functions.</p> <p>Some code is in this folder.</p>"},{"location":"coding/table-api/#how-to","title":"How to","text":"Create a data generator <p>There is the FlinkFaker tool that seems to be very efficient to send different types of data. It is using Datafaker Java library, which can be extended to add our own data provider. FlinkFaker jar is added to the custom flink image in the Dockerfile. The challenges will be to remote connect to the compute-pool as defined in Confluent Cloud.</p> Connect to Confluent Cloud remotely <p>Define the cloud.properties and then use the Confluent API</p> <pre><code>    import io.confluent.flink.plugin.ConfluentSettings;\n    import org.apache.flink.table.api.EnvironmentSettings;\n    import org.apache.flink.table.api.TableEnvironment;\n\n    public static void main(String[] args) {\n        EnvironmentSettings settings = ConfluentSettings.fromResource(\"/cloud.properties\");\n        TableEnvironment env = TableEnvironment.create(settings);\n</code></pre> Create a table with Kafka topic as persistence in Confluent Cloud? <pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport io.confluent.flink.plugin.ConfluentTableDescriptor;\n//...\nenv.createTable(\n        TARGET_TABLE1,\n        ConfluentTableDescriptor.forManaged()\n            .schema(\n                    Schema.newBuilder()\n                            .column(\"user_id\", DataTypes.STRING())\n                            .column(\"name\", DataTypes.STRING())\n                            .column(\"email\", DataTypes.STRING())\n                            .build())\n            .distributedBy(4, \"user_id\")\n            .option(\"kafka.retention.time\", \"0\")\n            .option(\"key.format\", \"json-registry\")\n            .option(\"value.format\", \"json-registry\")\n            .build());\n</code></pre> Access to the schema of an existing topic / table? <pre><code>import org.apache.flink.table.api.DataTypes;\n//...\nDataType productsRow = env.from(\"examples.marketplace.products\")\n                .getResolvedSchema()\n                .toPhysicalRowDataType();\nList&lt;String&gt; columnNames = DataType.getFieldNames(productsRow);\nList&lt;DataType&gt; columnTypes = DataType.getFieldDataTypes(productsRow);\n// use in the schema function to create a new topic ...\n        Schema.newBuilder()\n                .fromFields(columnNames, columnTypes)\n                .column(\"additionalColumn\", DataTypes.STRING())\n                .build()\n</code></pre> How to split records to two topic, using StatementSet? <pre><code>StatementSet statementSet = env.createStatementSet()\n                    .add(\n                        env.from(\"`examples`.`marketplace`.`orders`\")\n                           .select($(\"product_id\"), $(\"price\"))\n                           .insertInto(\"PricePerProduct\"))\n                    .add(\n                        env.from(\"`examples`.`marketplace`.`orders`\")\n                           .select($(\"customer_id\"), $(\"price\"))\n                           .insertInto(\"PricePerCustomer\"));\n</code></pre>"},{"location":"coding/table-api/#deeper-dive","title":"Deeper dive","text":"<ul> <li>See this git repo: Learn-apache-flink-table-api-for-java-exercises. </li> <li>See the Table API in Java documentation.</li> <li>Connecting the Apache Flink Table API to Confluent Cloud with matching github which part of this code was ported into flink-sql-demos/02-table-api-java</li> </ul> <p>TO WORK ON</p>"},{"location":"coding/table-api/#lower-level-java-based-programming-model","title":"Lower level Java based programming model","text":"<ul> <li>Start Flink server using docker (start with docker compose or on Kubernetes). </li> <li>Start by creating a java application (quarkus create app for example or using maven) and a Main class. See code in flink-sql-quarkus folder.</li> <li>Add dependencies in the pom</li> </ul> <pre><code>      &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n      &lt;/dependency&gt;\n        &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n        &lt;scope&gt;provided&lt;/scope&gt;\n      &lt;/dependency&gt;\n      &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-planner-loader&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n        &lt;scope&gt;provided&lt;/scope&gt;\n      &lt;/dependency&gt;\n</code></pre> <pre><code>public class FirstSQLApp {\n public static void main(String[] args) {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);\n</code></pre> <p>The <code>TableEnvironment</code> is the entrypoint for Table API and SQL integration. See Create Table environment</p> <p>A TableEnvironment maintains a map of catalogs of tables which are created with an identifier. Each identifier consists of 3 parts: catalog name, database name and object name.</p> <pre><code>    // build a dynamic view from a stream and specifies the fields. here one field only\n    Table inputTable = tableEnv.fromDataStream(dataStream).as(\"name\");\n\n    // register the Table object in default catalog and database, as a view and query it\n    tableEnv.createTemporaryView(\"clickStreamsView\", inputTable);\n</code></pre> <p></p> <p>Tables can be either temporary, tied to the lifecycle of a single Flink session, or permanent, making them visible across multiple Flink sessions and clusters.</p> <p>Queries such as SELECT ... FROM ... WHERE which only consist of field projections or filters are usually stateless pipelines. However, operations such as joins, aggregations, or deduplications require keeping intermediate results in a fault-tolerant storage for which Flink\u2019s state abstractions are used.</p>"},{"location":"coding/table-api/#etl-with-table-api","title":"ETL with Table API","text":"<p>See code: TableToJson</p> <pre><code>public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv);\n\n        final Table t = tableEnv.fromValues(\n\n            row(12L, \"Alice\", LocalDate.of(1984, 3, 12)),\n            row(32L, \"Bob\", LocalDate.of(1990, 10, 14)),\n            row(7L, \"Kyle\", LocalDate.of(1979, 2, 23)))\n        .as(\"c_id\", \"c_name\", \"c_birthday\")\n        .select(\n                jsonObject(\n                JsonOnNull.NULL,\n                    \"name\",\n                    $(\"c_name\"),\n                    \"age\",\n                    timestampDiff(TimePointUnit.YEAR, $(\"c_birthday\"), currentDate())\n                )\n        );\n\n        tableEnv.toChangelogStream(t).print();\n        streamEnv.execute();\n    }\n</code></pre>"},{"location":"coding/table-api/#join-with-a-kafka-streams","title":"Join with a kafka streams","text":"<p>Join transactions coming from Kafka topic with customer information.</p> <pre><code>    // customers is reference data loaded from file or DB connector\n    tableEnv.createTemporaryView(\"Customers\", customerStream);\n    // transactions come from kafka\n    DataStream&lt;Transaction&gt; transactionStream =\n        env.fromSource(transactionSource, WatermarkStrategy.noWatermarks(), \"Transactions\");\n    tableEnv.createTemporaryView(\"Transactions\", transactionStream\n    tableEnv\n        .executeSql(\n            \"SELECT c_name, CAST(t_amount AS DECIMAL(5, 2))\\n\"\n                + \"FROM Customers\\n\"\n                + \"JOIN (SELECT DISTINCT * FROM Transactions) ON c_id = t_customer_id\")\n        .print();\n</code></pre>"},{"location":"coding/terraform/","title":"Using Terraform to deploy Flink App or Statement","text":"<p>Source of major information can be found in the Terraform Confluent provider formal documentation, and in the examples of deployment.</p> <p>There are two approaches to manage Confluent Platform Kafka cluster in the same Terraform workspace:</p> <ol> <li>Manage multiple clusters </li> <li>Manage a single Kafka cluster</li> </ol> <p>But Terraform plugin also support Flink statement deployments on Confluent Cloud.</p>"},{"location":"coding/terraform/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>If not done create Confluent Cloud API key with secret for your user or for a service account (See confluent.cloud/settings/api-keys). Your user needs <code>OrganizationAdmin</code> role. For production do not use, user keys but prefer service account keys. </li> <li> <p>If not already done create a service account for terraform runner. Assign the OrganizationAdmin role to this service account by following this guide.</p> </li> <li> <p>To get the visibility of the existing keys use the command:</p> <pre><code>confluent api-key list | grep &lt;cc_userid&gt;&gt;\n</code></pre> </li> <li> <p>Export as environment variables:</p> <pre><code>export CONFLUENT_CLOUD_API_KEY=\nexport CONFLUENT_CLOUD_API_SECRET=\n</code></pre> </li> </ul>"},{"location":"coding/terraform/#infrastructure","title":"Infrastructure","text":""},{"location":"coding/terraform/#kafka","title":"Kafka","text":"<p>See the Product documentation to create kafka cluster with Terraform. The basic cluster sample project describes the needed steps, but it is recommended to use standard kafka cluster with RBAC access control.</p> <p>This repository includes a demo with IaC definition in the deployment cc-terraform folder which defines the following components:</p> <ul> <li>Confluent Cloud Environment</li> <li>A Service account to manage the environment: <code>env_manager</code> with the role of <code>EnvironmentAdmin</code> and API keys</li> <li>A Kafka cluster in a single AZ (for demo), with service accounts for app-manager, producer and consumer apps</li> <li>A schema registry with API keys to access the registry at runtime</li> </ul>"},{"location":"coding/terraform/#compute-pool","title":"Compute pool","text":"<p>Flink Compute pool can also be configured with Terraform, and see this example in the Terraform Confluent quickstart repository.</p> <p>The <code>flink.tf</code> in deployment cc-terraform:  defines the following components:</p> <ul> <li>A flink pool, with 2 service accounts, one for flink app management and one for developing flink statements.</li> </ul>"},{"location":"coding/terraform/#deploy-the-configuration","title":"Deploy the configuration","text":"<ul> <li> <p>Use the classical tf commands:</p> <pre><code>terraform init\nterraform plan\nterraform apply --auto-approve\n</code></pre> </li> <li> <p>If there is a 401 error on accessing Confluent, it is a problem of api_key within the environment variables.</p> </li> <li> <p>The output of this configuration needs to be used by other deployment like the Flink statement ones. It can be retrieved at any time with</p> <pre><code>terraform output\n</code></pre> </li> </ul>"},{"location":"coding/terraform/#deploy-a-flink-statement","title":"Deploy a Flink statement","text":"<p>Flink tatement can be deployed using Terraform: DDL is the easier part. Automation of DML statement is more complex: the first deployment on a fresh environment is simple, the challenges come when doing cannary deployment on existing running statements for logic upgrade. For that see the statement life cycle chapter in the cookbook chapter.</p> <pre><code>resource \"confluent_flink_statement\" \"ddl-dim_tenant-v1\" {\nproperties = {\n    \"sql.current-catalog\"  = var.current_catalog\n    \"sql.current-database\" = var.current_database\n}\n\nstatement  = file(\"${path.module}/sql-scripts/ddl.dim_tenant.sql\")\nstatement_name = \"dml-dim-tenant_v1\"\n\nstopped = false  # change when updating the statement\n}\n</code></pre> <p>statement can be a reference to a file, or a string including the statement content.</p> <pre><code>resource \"confluent_flink_statement\" \"create-table\" {\nstatement  = \"CREATE TABLE ${local.table_name}(ts TIMESTAMP_LTZ(3), random_value INT);\"\n</code></pre>"},{"location":"coding/udf_sql/","title":"User Defined Functions","text":"<p>User-defined functions (UDFs) are extension to SQL for frequently used logic and custom program and integration. It can be done in Java or PyFlink.</p> <p>Confluent documentation on UDF and a Confluent git repo with a sample UDF, and my new repository for UDFs catalog.</p>"},{"location":"coding/udf_sql/#udf-catalog","title":"UDF Catalog","text":"<p>This repository includes the following UDFs:</p> <ul> <li>Geo Distance using the Haversine formula to compute distance between two points on earth. It requires the latitude and longitude of the two points.</li> </ul>"},{"location":"coding/udf_sql/#deploying-to-confluent-cloud","title":"Deploying to Confluent Cloud","text":"<ul> <li>Get FlinkDeveloper RBAC to be able to manage workspaces and artifacts</li> <li> <p>Use the Confluent CLI to upload the jar file. Example from GEO_DISTANCE     <pre><code>confluent environment list\n# then in your environment\nconfluent flink artifact create geo_distance --artifact-file target/geo-distance-udf-1.0-0.jar --cloud aws --region us-west-2 --environment env-nk...\n</code></pre></p> <pre><code>+--------------------+--------------+\n| ID                 | cfa-nx6wjz   |\n| Name               | geo_distance |\n| Version            | ver-nxnnnd   |\n| Cloud              | aws          |\n| Region             | us-west-2    |\n| Environment        | env-nknqp3   |\n| Content Format     | JAR          |\n| Description        |              |\n| Documentation Link |              |\n+--------------------+--------------+\n</code></pre> <p>Also visible in the Artifacts menu </p> </li> <li> <p>UDFs are registered inside a Flink database     <pre><code>CREATE FUNCTION GEO_DISTANCE\nAS\n'io.confluent.udf.GeoDistanceFunction'\nUSING JAR 'confluent-artifact://cfa-...';\n</code></pre></p> </li> <li>Use the function to compute distance between Paris and London:     </li> </ul>"},{"location":"concepts/","title":"Apache Flink - Core Concepts","text":"Version <p>Update 07/2025 - Review done with simplification and avoid redundancies.</p>"},{"location":"concepts/#quick-reference","title":"Quick Reference","text":"<ul> <li>Core Concepts</li> <li>Stream Processing</li> <li>State Management</li> <li>Time Handling</li> </ul>"},{"location":"concepts/#why-flink","title":"Why Flink?","text":"<p>Traditional data processing faces key challenges: - Transactional Systems: Monolithic applications with shared databases create scaling challenges - Analytics Systems: ETL pipelines create stale data and require massive storage and often duplicate data across systems.  ETLs extract data from a  transactional database, transform it into a common representation (including validation, normalization, encoding, deduplication, and schema transformation), and then load the new records into the target analytical database. These processes are run periodically in batches.</p> <p>Flink enables real-time stream processing with three application patterns:</p> <ol> <li>Event-Driven Applications: Reactive systems using messaging</li> <li>Data Pipelines: Low-latency transformation and enrichment  </li> <li>Real-Time Analytics: Immediate computation and action on streaming data</li> </ol>"},{"location":"concepts/#overview-of-apache-flink","title":"Overview of Apache Flink","text":"<p>Apache Flink is a distributed stream processing engine for stateful computations over bounded and unbounded data streams. It's  become an industry standard due to its performance and comprehensive feature set.</p> <p>Key Features:</p> <ul> <li>Low Latency Processing: Offers event time semantics for consistent and accurate results, even with out-of-order events.</li> <li>Exactly-Once Consistency: Ensures reliable state management to avoid duplicates and not loosing message.</li> <li>High Throughput: Achieves millisecond latencies while processing millions of events per second.</li> <li>Powerful APIs: Provides APIs for operations such as map, reduce, join, window, split, and connect.</li> <li>Fault Tolerance and High Availability: Supports failover for task manager nodes, eliminating single points of failure.</li> <li>Multilingual Support: Enables streaming logic implementation in Java, Scala, Python, and SQL.</li> <li>Extensive Connectors: Integrates seamlessly with various systems, including Kafka, Cassandra, Pulsar, Elasticsearch, File system, JDBC complain  Database, HDFS and S3.</li> <li>Kubernetes Native: Supports containerization and deployment on Kubernetes with dedicated k8s operator to manage session job or application as  well as job and task managers.</li> <li>Dynamic Code Updates: Allows for application code updates and job migrations across different Flink clusters without losing application state.</li> <li>Batch Processing: Also transparently support traditional batch processing workloads as reading at rest table becomes a stream in Flink</li> </ul>"},{"location":"concepts/#stream-processing-concepts","title":"Stream Processing Concepts","text":"<p>A Flink application runs as a job - a processing pipeline structured as a directed acyclic graph (DAG) with:</p> <ul> <li>Sources: Read from streams (Kafka, Kinesis, Queue, CDC etc.)</li> <li>Operators: Transform, filter, enrich data</li> <li>Sinks: Write results to external systems</li> </ul> Data flow as directed acyclic graph <p>Operations can run in parallel across partitions. Some operators (like Group By) require data reshuffling or repartitioning.</p>"},{"location":"concepts/#bounded-and-unbounded-data","title":"Bounded and unbounded data","text":"<p>A Stream is a sequence of events, bounded or unbounded:</p> Bounded and unbounded event sequence"},{"location":"concepts/#dataflow","title":"Dataflow","text":"<p>In Flink 1.20.x, applications are composed of streaming dataflows. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below:</p> A Flink application -  src: apache Flink product doc <p>Stream processing includes a set of functions to transform data, and to produce a new output stream. Intermediate steps compute rolling aggregations like min, max, mean, or collect  and buffer records in time window to compute metrics on a finite set of events. </p> Streaming Dataflow  src: apache Flink product doc <p>Data is partitioned for parallel processing. Each stream has multiple partitions, and each operator has multiple subtasks for scalability.</p> Distributed processing  src: apache Flink product doc <p>Operations like GROUP BY require data reshuffling across network, which can be costly but enables distributed aggregation.</p> <pre><code>INSERT INTO results\nSELECT key, COUNT(*) FROM events\nWHERE color &lt;&gt; blue\nGROUP BY key;\n</code></pre>"},{"location":"concepts/#state-management","title":"State Management","text":"<p>Flink keep state of its processing for Fault tolerance. State can grow over time. Local state persistence improves latency while remote checkpointing ensures fault tolerance.</p> Flink and Kafka integration with state management <p>We can dissociate different type of operations:</p> <p>Stateless Operations process each event independently without retaining information: - Basic operations: <code>INSERT</code>, <code>SELECT</code>, <code>WHERE</code>, <code>FROM</code>  - Scalar/table functions, projections, filters</p> <p>Stateful Operations maintain state across events for complex processing: - <code>JOIN</code> operations (except <code>CROSS JOIN UNNEST</code>) - <code>GROUP BY</code> aggregations (windowed/non-windowed) - <code>OVER</code> aggregations and <code>MATCH_RECOGNIZE</code> patterns</p> <p>Flink ensures fault tolerance through checkpoints and savepoints that persistently store application state.</p>"},{"location":"concepts/#windowing","title":"Windowing","text":"<p>Windows group stream events into finite buckets for processing. Flink provides window table-valued functions (TVF): Tumbling, Hop, Cumulate, Session.</p> <p>Window Types:</p> <ul> <li> <p>Tumbling window assigns events to non-overlapping buckets of fixed size. Records are assigned to the window based on an event-time attribute field, specified by the DESCRIPTOR() function. Once the window boundary is crossed, all events within that window are sent to an evaluation function for processing. </p> <ul> <li>Count-based tumbling windows define how many events are collected before triggering evaluation. </li> <li>Time-based tumbling windows define time interval (e.g., n seconds) during which events are collected. The amount of data within a window can vary depending on the incoming event rate. </li> </ul> <pre><code>.keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2)))\n</code></pre> <p>in SQL:</p> <pre><code>-- computes the sum of the price in the orders table within 10-minute tumbling windows\nSELECT window_start, window_end, SUM(price) as `sum`\nFROM TABLE(\n    TUMBLE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '10' MINUTES))\nGROUP BY window_start, window_end;\n</code></pre> </li> </ul> Tumbling window concept <ul> <li> <p>Sliding windows allows for overlapping periods, meaning an event can belong to multiple buckets. This is particularly useful for capturing trends over time. The window sliding time parameter defines the duration of the window and the interval at which new windows are created. For example, in the following code snippet defines a new 2-second window is created every 1 second:</p> <pre><code>.keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1)))\n</code></pre> <p>As a result, each event that arrives during this period will be included in multiple overlapping windows, enabling more granular analysis of the data stream.</p> </li> </ul> Sliding window concept <ul> <li> <p>Session window begins when the data stream processes records and ends when there is a defined period of inactivity. The inactivity threshold is set using a timer, which determines how long to wait before closing the window.</p> <pre><code>.keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))\n</code></pre> <p>The operator creates one window for each data element received.  If there is a gap of 5 seconds without new events, the window will close. This makes session windows particularly useful for scenarios where you want to group events based on user activity or sessions of interaction, capturing the dynamics of intermittent data streams effectively.</p> </li> </ul> Session window concept <ul> <li>Global: One window per key, requires explicit triggers</li> </ul> <p>See Windowing TVF documentation.</p> <pre><code>```java\n.keyBy(0)\n.window(GlobalWindows.create())\n.trigger(CountTrigger.of(5))\n```\n</code></pre> <p>See Windowing Table-Valued Functions details in Confluent documentation.</p>"},{"location":"concepts/#event-time","title":"Event time","text":"<p>Time is a central concept in stream processing and can have different interpretations based on the context of the flow or environment:</p> <ul> <li>Processing Time refers to the system time of the machine executing the task. It offers the best performance and lowest latency since it relies on the local clock. But it may lead to no deterministic results due to factors like ingestion delays, parallel execution, clock synch, backpressure...</li> <li>Event Time is the timestamp embedded in the record at the event source level. Using event-time ensures consistent and deterministic results, regardless of the order in which events are processed. This is crucial for accurately reflecting the actual timing of events.</li> <li>Ingestion Time denotes the time when an event enters the Flink system. It captures the latency introduced during the event's journey into the processing framework.</li> </ul> <p>In any time window, the order of arrival may not be guarantee, and some events with an older timestamp may fall outside of the time window boundaries. To address this challenge, particularly when computing aggregates, it's essential to ensure that all relevant events have arrived within the intended time frame.</p> <p>The watermark serves as a heuristic for this purpose.</p>"},{"location":"concepts/#watermarks","title":"Watermarks","text":"<p>Watermarks are special markers indicating event-time progress in streams. This is the core mechanims to trigger computation at <code>event-time</code>.  They determine when windows can safely close by estimating when all events for a time period have arrived.</p> <p>Key Concepts:</p> <ul> <li>Generated in the data stream at regular intervals</li> <li>Watermark timestamp = largest seen timestamp - estimated out-of-orderness. This timestamp are always increasing. </li> <li>Events arriving after watermarks are considered late and typically discarded</li> <li>Essential for triggering window computations in event-time processing</li> </ul> Watermark concept <p>Within a window, states are saved on disk and need to be cleaned once the window is closed. The watermark is the limit from where the garbage collection can occur. </p> <p>The out-of-orderness estimate serves as an educated guess and is defined for each individual stream. Watermarks are essential for comparing timestamps of events, allowing the  system to assert that no earlier events will arrive after the watermark's timestamp.</p> <p>Watermarks are crucial when dealing with multiple sources. In scenarios involving IoT devices and network latency, it's possible to receive an event with an earlier timestamp even  after the operator has already processed events with that timestamp from other sources. Importantly, watermarks are applicable to any timestamps and are not limited to window  semantics.</p> <p>When working with Kafka topic partitions, the absence of watermarks may represent some challenges. Watermarks are generated independently for each stream and partition. When two  partitions are combined, the resulting watermark will be the oldest of the two, reflecting the point at which the system has complete information. If one partition stops receiving  new events, the watermark for that partition will not progress. To ensure that processing continues over time, an idle timeout configuration can be implemented.</p> <p>Each task has its own watermark, and at the arrival of a new watermark, it checks if it needs to advance its own watermark. When it is advanced, the task performs all triggered computations and emits all result records. The watermark is broadcasted to all output of the task.</p> <p>The watermark of a task is the mininum of all per-connection watermarks. Task with multiple input, like JOINs or UNIONs maintains a single watermark, which is the minimum between the input watermarks.</p> <p>Additionally, it is possible to configure the system to accept late events by specifying an <code>allowed lateness</code> period. This defines how late an element can arrive before it is discarded. Flink maintains the state of the window until the allowed lateness time has expired, allowing for flexible handling of late-arriving data while ensuring that the processing remains efficient and accurate.</p> <p>When using processing time, the watermark advances at regular intervals, typically every second. Events within the window are emitted for processing, once the watermark surpasses  the end of that window.</p> <p>Parallel watermarking is an example of getting data from 4 partitions with 2 kafka consumers and 2 windows:</p> Parallel watermarking <p>Shuffling is done as windows are computing some COUNT or GROUP BY operations. Event A arriving at 3:13, and B[3:20] on green partitions, and are processed by Window 1 which considers 60 minutes time between 3:00 and 4:00. </p> <p>The source connector sends a Watermark for each partition independently. If the out-of-orderness is set to be 5 minutes, a watermark is created with a timestamp 3:08 = 3:13 - 5 (partition 0) and at 3:15 (3:20 - 5) for partition 1. The generator sends the minimum of both. The timestamp reflects how complete the stream is so far: it could not be no more completed than the further behind which was event at 3:13, </p> <p>In the case of a partition does not get any events, as there is no watermark generated for this partition, it may mean the watermark does no advance, and as a side effect it prevents windows from producing events. To avoid this problem, we need to balance kafka partitions so none are empty or idle, or configure the watermarking to use idleness detection.</p> <ul> <li>See example TumblingWindowOnSale.java in my-fink folder and to test it, do the following:</li> </ul> <pre><code># Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket\njava -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer\n# inside the job manager container started with \n`flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar`.\n# The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling window\n(June,Bat,Category5,154,6)\n(August,PC,Category5,74,2)\n(July,Television,Category1,50,1)\n(June,Tablet,Category2,142,5)\n(July,Steamer,Category5,123,6)\n...\n</code></pre>"},{"location":"concepts/#monitoring-watermark","title":"Monitoring watermark","text":"<p>The following metrics are used at the operator and task level:</p> <ul> <li><code>currentInputWatermark</code>: the last watermark received by the operator in its n inputs.</li> <li><code>currentOutputWatermark</code>: last emitted watermark by the operator</li> <li><code>watermarkAlignmentDrift</code>: current drift from the minimal watermakr emitted by all sources beloging to the same watermark group.</li> </ul> <p>Watermarks can be seen in Apache flink Console. </p>"},{"location":"concepts/#identify-which-watermark-is-calculated","title":"Identify which watermark is calculated","text":"<p>The approach is to add a virtual column to keep the Kafka partition number:</p> <pre><code>ALTER TABLE &lt;table_name&gt; ADD _part INT METADATA FROM 'partition' VIRTUAL;\n</code></pre> <p>then assess if there is a value on the \"Operator Watermark\" column with</p> <pre><code>SELECT\n  *,\n  _part AS `Row Partition`,\n  $rowtime AS `Row Timestamp`,\n  CURRENT_WATERMARK($rowtime) AS `Operator Watermark`\nFROM  &lt;table_name&gt;;\n</code></pre> <p>If not all partitions are included in the result, it may indicate a watermark issue with those partitions. We need to ensure that events are sent across all partitions. To test a statement, we can configure it to avoid being an unbounded query by consuming until the latest offset. This can be done by setting: <code>SET 'sql.tables.scan.bounded.mode' = 'latest-offset';</code></p> <p>Flink statement consumes data up to the most recent available offset at the job submission moment. Upon reaching this time, Flink ensures that a final watermark is propagated, indicating that all results are complete and ready for reporting. The statement then transitions into a 'COMPLETED' state.\"</p> <p>The table alteration can be undone with:</p> <pre><code>ALTER TABLE &lt;table_name&gt; DROP _part;\n</code></pre>"},{"location":"concepts/#trigger","title":"Trigger","text":"<p>A Trigger in Flink, determines when a window is ready to be processed. </p> <p>Each window has a default trigger associated with it. For example, a tumbling window might have a default trigger set to 2 seconds, while a global window requires an explicit trigger definition.</p> <p>You can implement custom triggers by creating a class that implements the Trigger interface, which includes methods such as onElement(..), onEventTime(..), and onProcessingTime(..).</p> <p>Flink provides several default triggers::</p> <ul> <li>EventTimeTrigger fires based upon progress of event time</li> <li>ProcessingTimeTrigger fires based upon progress of processing time</li> <li>CountTrigger fires when # of elements in a window exceeds a specified parameter.</li> <li>PurgingTrigger is used for purging the window, allowing for more flexible management of state.</li> </ul>"},{"location":"concepts/#eviction","title":"Eviction","text":"<p>Evictor is used to remove elements from a window either after the trigger fires or before/after the window function is applied. The specific logic for removing elements is application-specific and can be tailored to meet the needs of your use case.</p> <p>The predefined evictors: </p> <ul> <li>CountEvictor removes elements based on a specified count, allowing for fine control over how many elements remain in the window.</li> <li>DeltaEvictor evicts elements based on the difference between the current and previous counts, useful for scenarios where you want to maintain a specific change threshold.</li> <li>TimeEvictor removes elements based on time, allowing you to keep only the most recent elements within a given time frame.</li> </ul>"},{"location":"concepts/#source-of-knowledge","title":"Source of knowledge","text":"<ul> <li> Apache Flink Product documentation. </li> <li> Official Apache Flink training.</li> <li> Confluent \"Fundamentals of Apache Flink\" training- David Andersion.</li> <li> Anatomy of a Flink Cluster - product documentation.</li> <li> Jobs and Scheduling - Flink product documentation.</li> <li> Confluent Cloud Flink product documentation</li> <li> Confluent Plaform for Flink product documentation</li> <li>Base docker image is: https://hub.docker.com/_/flink</li> <li>Flink docker setup and the docker-compose files in this repo.</li> <li>FAQ</li> <li> Cloudera flink stateful tutorial: very good example for inventory transaction and queries on item considered as stream</li> <li>Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana</li> </ul>"},{"location":"labs/","title":"Local labs / demos and other public demonstrations","text":"<p>This section lists the current demonstrations and labs in this git repository or the interesting public repositories about Flink</p>"},{"location":"labs/#local-demonstrations-and-end-to-end-studies","title":"Local demonstrations and end to end studies","text":"<p>All the local demonstrations run on local Kubernetes, some on Confluent Cloud. Most of them are still work in progress.</p> <p>See the e2e-demos folder for a set of available demos based on the Flink local deployment or using Confluent Cloud for Flink.</p> <ul> <li> Record deduplication using Flink SQL or Table API deployed on Confluent Platform</li> <li> Change Data Capture with Postgresql, CDC Debezium, Confluent Platformm v8.0+, Cloud Native for Postgresql Kuberneted Operator</li> <li> e-commerce sale</li> <li> Transform json records</li> <li> Qlik CDC emulated to Flink dedup, filtering, transform logic</li> <li> External lookup</li> <li> Flink to JDBC Sink connector</li> <li> Savepoint demonstration</li> <li> SQL Gateway demonstration</li> <li> Terraform deployment</li> <li> GitOps with Openshift, ArgoCD and Tekton</li> </ul>"},{"location":"labs/#public-repositories-with-valuable-demonstrations","title":"Public repositories with valuable demonstrations","text":"<ul> <li>Shoes Store Labs  to run demonstrations on Confluent Cloud. </li> <li>Managing you Confluent Cloud Flink project at scale with a CLI</li> <li>Confluent Flink how to</li> <li>Confluent demonstration scene: a lot of Kafka, Connect, and ksqlDB demos</li> <li> <p>Confluent developer SQL training</p> </li> <li> <p>Demonstrate Flink SQL testing on Confluent Cloud</p> </li> <li>Demonstrations for Shift left project migration and for data as a product management.</li> </ul>"},{"location":"labs/#interesting-blogs","title":"Interesting Blogs","text":"<ul> <li>Building Streaming Data Pipelines, Part 1: Data Exploration With Tableflow</li> <li>Building Streaming Data Pipelines, Part 2: Data Processing and Enrichment With SQ</li> </ul>"},{"location":"labs/#quick-personal-demo-for-confluent-cloud-for-flink","title":"Quick personal demo for Confluent Cloud for Flink","text":"<p>Using the data generator and the <code>confluent flink shell</code></p> <ul> <li>Login to Confluent using cli</li> <li>Be sure to use the environment with the compute pool: </li> </ul> <pre><code>confluent environment list\nconfluent environment use &lt;env_id&gt;\nconfluent flink compute-pool list\n# get the region and cloud and the current max CFU\nconfluent flink compute-pool use &lt;pool_id&gt;\n</code></pre> <ul> <li>Start one of the Datagen in the Confluent Console. </li> </ul> <p>TBC</p>"},{"location":"methodology/data_as_a_product/","title":"Moving to a data as a product architecture","text":"<p>This chapter provides a practical overview of current data lake and lakehouse challenges, discusses the implementation of 'data as a product' principles, and demonstrates how real-time streaming can be effectively integrated into modern data architectures.</p>"},{"location":"methodology/data_as_a_product/#context","title":"Context","text":""},{"location":"methodology/data_as_a_product/#operational-data-and-analytical-data","title":"Operational Data and Analytical data","text":"<p>The classical data landscape is split between operational data, which powers real-time applications, and analytical data, which provides historical insights for decision-making and machine learning. This separation has created complex and fragile data architectures, marked by problematic ETL processes and intricate data pipelines. The challenge lies in effectively bridging these two distinct data planes to ensure seamless data flow and integration.</p> Two data planes: real-time applications, and analytical data <p>The initial data platform architecture comprised a database on one side and a data warehouse on the other, with ETL jobs facilitating data movement between them. This setup can lead to bottlenecks, especially when different teams are working on various parts of an application but all relying on the same data source. It might also complicate scalability and flexibility.</p> <p>To address scaling challenges and support unstructured data, the second generation of data platforms, emerging in the mid-2000s, adopted distributed object storage, leading to the development of the Data Lake.</p> <p>The medallion architecture, a three-layered approach, is a common framework for organizing data lakes. This structure, as illustrated in the figure below, is driven by several key motivations:</p> Medaillion Architecture <ul> <li>Leveraging cloud object storage to accommodate large volumes of both structured and unstructured data.</li> <li>Implementing data pipelines to transform data progressively, from raw landing zones to business-level aggregates.</li> <li>Facilitating data management and governance through data cataloging and distributed query tools.</li> <li>Organizing data based on its transformation stage, rather than business domains or specific use cases.</li> </ul> <p>Data product and its extension with Data Mesh helps to restructure those two planes with a domain and use case centric approach, and not a technology stack.</p>"},{"location":"methodology/data_as_a_product/#current-challenges","title":"Current Challenges","text":"<p>In Lakehouse or data lake architecture: </p> <ul> <li>We observe complex ETL jobs landscape, with high failure rate.</li> <li>Not all data needs the three layers architecture, but a more service contract type of data usage. Data becoming a product like a microservice.</li> <li>There is a latency issue to get the data, we talk about T + 1 to get fresh data. The + 1 can be one day or one hour, but it has latency that may not what business requirements need.</li> <li>Simple transformations need to be done with the ETL or ELT tool with the predefined staging. Not specific use-case driven implementation of the data retrieval and processing. </li> <li>Data are pulled from their sources and between layers. It could be micro-batches, or long-running batches. At the bronze layer, the data are duplicated, and there is minimum of quality control done.</li> <li>In the silver layer the filtering and transformations are also generic with no specific business context.</li> <li>The gold layer includes all data of all use cases. This is where most of the work is done for data preparation and develop higher quality level. This is the layer with a lot of demands from end-user and continuous update and new aggregation developments. </li> <li>This is the final consumer of the data lake gold layer that are pulling the data with specific Service Level Objectives. </li> <li>Data created at the gold level, most likely needs to be reingected to the operational databases to be visible to operation applications. This introduces the concept of reverse ETL. </li> <li>Each layer may have dfferent actors responsible to process the data: data platform engineer, analytic engineers and data modelers, and at the application level, the application developers.</li> <li>Storing multiple copies of data across layers inflates cloud storage expenses. Data become quickly stale and unreliable.</li> <li>Constant movement of data through layers results in unnecessary processing and query inefficiencies.</li> <li>The operational estate is also continuously growing, by adding mobile applications, serverless functions, cloud native apps, etc...</li> </ul>"},{"location":"methodology/data_as_a_product/#core-principles-for-data-mesh","title":"Core principles for Data Mesh","text":"<p>To address the concerns of siloed and incompatible data, while addressing scaling to constant change of data landscape, adding more data source and consumers, adding more transformations and processing resources, the data mesh is based on four core principles:</p> <ol> <li> <p>Domain-oriented decentralized data ownership and architecture. The components are the analytical data, the metadata and the computer resources to serve it. Data ownership is linked to the DDD bounded context. For a product management use case, the bounded context of a <code>Product</code>, supports operational APIs and analytical data endpoints to address active users, feature usage, and conversion rates, for example: </p> <p> Data as a product  - Bounded context </p> <p>Also multiple bounded contexts could be presented via their dependencies to other domain operational and analytical data endpoints.</p> </li> <li> <p>Data as a product, includes clear scope definition, product ownership and metrics to ensure data quality, user acceptance, lead time for data consumption. Data as a product includes documenting the users, how they access the data, and for what kind of operations. The accountability of the data quality shifts to the source of the data. It encapsulates three structural components: 1/ code (data pipelines, schema definitions, APIs, event processing, monitoring metrics, access control), 2/ data and metadata in a polyglot form (events, REST, tables, graphs, batch files...), 3/ infrastructure (to run code, store data and metadata).</p> <p> Data as a product: component view </p> </li> <li> <p>Self-serve data infrastructure as a platform, to enable domain autonomy, as microservices are defined and orchestrated. It includes callable polyglot data storage, data products schema, data pipeline declaration and orchestration, data products lineage, compute and data locality. The capabilities includes 1/ infrastructure provisioning via code for storage, service accounts, access policies, server provisioning for running code and jobs, 2/ data product interface, declarative interfaces to manage the life cycle of a data product, 3/ supervision plane to present the relation between data products, support discovery, build data catalog, to execute semantic query.</p> <p> Data as a product: infrastructure platform </p> </li> <li> <p>Federated governance to address interoperability of the data products. This needs to support decentralized and domain self-sovereignty, interoperability through standardization. </p> </li> </ol> Moving to Kafka and real-time processing is not the full story <p>Changing the batch pipeline processing technologies to real-time processing using the medallion architecture does not solve the previously mentionned problems. We still need to shift paradign and adopt a data as a product centric architecture. The following diagram illustrates the mediallon layers, done with Flink processing and Kafka topics for storage.</p> <p> Real-time intgration </p> <p>Using topics as data record storage and Flink statements for transforming, filtering and enriching to the silver layer, also using kafka topics is the same ETL approach but with different technologies. </p> <p>Another, more detailed view, using Kafka Connectors will look like in the diagram below, where the three layers are using the Kimdall practices of source processing, intermediates and sinks.</p> <p> Generic source to sink pipeline </p> <p>Even if append-logs are part of the data as a product architecture, there are more to address and to organize the component development.</p>"},{"location":"methodology/data_as_a_product/#a-data-product-approach","title":"A data product approach","text":"<p>As seen previously, domains need to host and serve their domain datasets in an easily consumable way, rather than flowing the data from domains into a centrally owned data lake or platform. Dataset from one domain may be consumed by another domains in a format suitable for its own application. Consumer pulls the dataset.</p> Data product reused by other domains <p>So developing data as a product means shifting from push and ingest of ETL and ELT processes to serving and pull model across all domains. </p>"},{"location":"methodology/data_as_a_product/#data-as-a-product","title":"Data as a product","text":"<p>Data products serve analytical data, they are self-contained, deployable, valuable and exhibit eight characteristics:</p> <ul> <li>Discoverable: data consumers can easily find the data product for their use case.  A common implementation is to have a registry, a data catalogue, of all available data products with their meta information. Domain data products need to register themselves to the catalog.</li> <li>Addressable: with a unique address accessible programmatically. This implies to define naming convention and may be SDK code.</li> <li>Self describable: Clear description of the purpose and usage patterns as well as the semantics and syntax. The schema definition and registry are used for that purpose. </li> <li>Trustworthy: clear definition of the Service Level Objectives and Service Level Indicators conformance. </li> <li>Native access: adapt the data access interface to the consumer: APIs, events, SQL views, reports, widgets</li> <li>Composable: integrate with other data products, for joining, filtering and aggregation. Nedd to define standards for field type formatting, identifying polysemes across different domains, datasets address conventions, common metadata fields, event formats such as CloudEvents. Federated identity may also being used to keep unique identifier cross domain for a business entity.</li> <li>Valuable: represent a cohesive concept within its domain. Sourced from unstructured, semi-structured and structured data. To maximize value within a data mesh, data products should have narrow, specific definitions, enabling reusable blueprints and efficient management.</li> <li>Secure: with access control rules and enforcement, and single sign on capability.</li> </ul> <p>To support the implementation of those characteristics, it is relevant to name a domain data product owner, who is also responsible to measure data quality, the decreased lead time of data consumption, and the data user satisfaction, or net promoter score. The most important questions a product owner should be able to answer are:</p> <ol> <li>Who are the data users?</li> <li>How do they use the data?</li> <li>What are the native methods that they are comfortable with to consume the data?</li> </ol> <p>Data products are not data applications, data warehouses, PDF reports, dashboards, tables (without proper metadata), or kafka topics. The data products may, and should be shared using streams, to be able to replay from sources of events and scale the consumption. </p>"},{"location":"methodology/data_as_a_product/#elements-of-a-data-product","title":"Elements of a Data Product","text":"<p>The following elements are part of a data product owner to develop and manage, with application developers:</p> <ul> <li>Metadata of what the data product is, human readable, parseable for tool to build and deploy data product to orchestration layer. This includes using naming convention, and poliglot definition. </li> <li>API definition for request-response consumptions</li> <li>Event model definition for asynch consumptions</li> <li>Storage definition, service account, roles and access policies</li> <li>Table definitions</li> <li>Flink statement definitions for deduplication, enrichment, aggregation, and deployment definitions</li> <li>Microservice code implementation, packaging and deployment definitions</li> </ul> <p>All those elements can be defined as code in a git repository or between a gitops repo and a code repository. It is recommended to keep one bounded context per repository.</p> Data lake, lakehouse and data warehouse <p>Data lake is no more a central piece of the architecture with complex pipelines, they are becoming a node in the data mesh, to expose a dataset. It may not be used as the source of truth is becoming the immutable distributes logs and storage that holds the dataset available for replayability. Datawarehouse for business intelligence is also a node, and consumer of the data product.</p>"},{"location":"methodology/data_as_a_product/#methodology","title":"Methodology","text":"<p>Defining, designing and implementing data products follow the same principles as other software development and should start by the end goal and use case. This should solidify clear product objectives. Domain discovery is part of the DDD methodology, and in the data product a domain may be more oriented to source and some to consumers. But use cases and what needs to be created as analytical data should be the main goals of the design and implementation activities. Source domain datasets represent the facts of the business. The source domain datasets capture the data that is mapped very closely to what the operational systems of their origin, generate.</p> <p>Consumer domain datasets, on the other hand, are built to serve a tightly coupled group of use cases. Distinct from source domain datasets, they undergo more structural modifications as they process source domain events into aggregated formats optimized for a specific access model.</p>"},{"location":"methodology/data_as_a_product/#formalize-the-use-cases-user-stories","title":"Formalize the use cases / user stories","text":"<p>The following table illustrates some use cases:</p> User Story Data as a Product As a marketing strategist, I need to provide predictive churn scores and customer segmentation based on behavior and demographics. This will allow me to proactively target at-risk customers with personalized retention campaigns and optimize marketing spend. <ul><li>Churn probability scores for each customer.</li><li>Customer segments based on churn risk and value.</li> <li>Key factors influencing churn.</li></ul> As a product manager, I need to visualize key product usage metrics and performance indicators. This will enable me to monitor product adoption, identify usage patterns, and make data-driven decisions for product improvements. <ul><li>Active users, feature usage, and conversion rates.</li><li>Historical trends and comparisons of product performance.</li><li>Breakdowns of product usage by customer segment</li><li>Alerts for anomalies or significant changes in product usage</li></ul> As a supply chain manager, I need to get real-time visibility into inventory levels, supplier performance, and delivery timelines. This will help me proactively identify potential disruptions, optimize inventory management, and ensure timely product delivery. <ul><li>Real-time inventory levels across all warehouses.</li><li>Supplier performance metrics, such as on-time delivery rates and quality scores.</li><li>Predictive alerts for potential stockouts or delivery delays.</li><li>Visualizations of delivery routes and timelines.</li><li>Historical data that can be used to perform trend analysis, and find bottlenecks.</li></ul> As a Consultant Director, I need to be able to continuously access a holistic view of each consultant, including their skill level, matching resume, current training and skill levels, and certification status, so that I can effectively staff projects, identify skill gaps, plan professional development, and ensure compliance. <ul><li>Aggregated and real-time data on consultant skill levels</li> <li>Matching resumes (potentially key skills extracted)</li><li>Current training completions and skill levels derived from training</li><li>Certification statuses</li><li>Visualization of skill gaps by practice area or project type</li></ul> <p>Using a classical system context diagram for the supply chain management use case, we may define the high level view of a data product as:</p> Data as a product: system context view <p>The skill analysis use case may define the following data product:</p> <ul> <li> <p>Certification Compliance Tracker:</p> <ul> <li>Data: Real-time status of all consultant certifications, including expiration dates and renewal progress.</li> <li>Value Proposition: Ensures the organization maintains necessary certifications for compliance and client engagements, mitigating potential risks and penalties.</li> <li>Potential Features: Automated alerts for upcoming expirations; reporting on certification coverage by practice area or client; integration with certification management platforms.</li> </ul> </li> </ul>"},{"location":"methodology/data_as_a_product/#using-bounded-context","title":"Using bounded context","text":"<p>Data as a product is designed with a domain-driven model combined with analytical and operational use cases. </p> <p>The methodology to define data product may be compared to the event-driven microservice adoption. Business, operational application, manages their aggregates but also are responsible to publish the business events, as facts, to share their datasets. Aggregation processing is considered as a service pushing data product to other consumers. The aggregate models make specific queries on other data products and serve the results with SLOs.</p> <p>The design starts by the user input, which are part of a business domain and bounded context. The data may be represented as DDD aggregate with a semantic model. Entities and Value objects are represented to assess the need to reuse other data product and potentially assess the need for anti-corruption layer. </p> What should be part of a bounded context for data as a product <p>A Bounded Context should encapsulate everything needed to model and implement a specific business capability or set of related capabilities. This typically includes:</p> <ul> <li>Entities: Domain objects with identity that persist over time and represent core concepts of the subdomain. Their behavior and attributes are specific to this context.</li> <li>Value Objects: Immutable objects that describe characteristics of entities. Their meaning is specific to the context.</li> <li>Aggregates: Clusters of related entities and value objects that are treated as a single unit for data changes. One entity within the aggregate serves as the root and is responsible for maintaining the consistency of the entire aggregate. Transactions should operate on aggregates.</li> <li>Domain Services: Operations that don't naturally belong to an entity or value object but are still part of the domain logic within this context. They often involve interactions between multiple aggregates or external systems.</li> <li>Domain Events: Significant occurrences within the domain that the business cares about. They are immutable records of something that has happened and can trigger actions within the same or other bounded contexts.</li> <li>Repositories: Interfaces for persisting and retrieving aggregates within the bounded context. The actual implementation of the repository might use a specific database technology.</li> <li>Factories: Objects responsible for creating complex domain objects, often encapsulating complex instantiation logic.</li> <li>Use Cases/Application Services: (Sometimes considered outside the core domain but within the Bounded Context) These orchestrate interactions between domain objects to fulfill specific user requests or system behaviors. They reside at the application layer and interact with repositories and domain services.</li> <li>Data Transfer Objects (DTOs): Objects used to transfer data across boundaries (e.g., between layers or bounded contexts). Their structure is often optimized for transport rather than representing the domain model directly.</li> <li>Infrastructure Concerns: Code related to persistence, messaging, external service integrations, and UI specific to this bounded context.</li> </ul> <p>In data products, DDD bounded context, translates to defining clear boundaries for the data products, ensuring each product serves a specific business domain. A <code>customer data product</code> and a <code>product inventory data product</code> would be distinct bounded contexts, each with its own data model and terminology.</p> <p>Data pushed to higher consumer are part of the semantic model, and of the event-driven design. Analytics Engineers and Data Modellers building aggregate Data Products know exactly what to collect and what quality metrics to serve. The aggregation may use lean-pull mechanism, focusing on their use case needs only. The data is becoming a product as close to the operational source, so shifting the processing to the left of the architecture. This Shift-Left approach where quality controls, validation, and governance mechanisms are embedded as early in the data lineage map as possible. The consumption patterns are designed as part of the data product, and may include APIs, events, real-time streams or even scheduled batch. </p> <p>Source data domains need to make easily consumable historical snapshots of their datasets available, not just timed events. These snapshots should be aggregated based on a time frame that matches the typical rate of change within their domain.</p> <p>Even though domains now own their datasets instead of a central platform, the essential tasks of cleansing, preparing, aggregating, and serving data persist, as does the use of data pipelines, which are now integrated into domain logic.</p> Data product within bounded context <p>Each domain dataset must establish Service Level Objectives for the quality of the data it provides: timeliness, error rates...</p> <p>Moving from technical data delivery to product thinking requires changes in how organizations approach data management. The data product is decomposed of real-time events exposed on event streams, and aggregated analytical data exposed as serialized files on an object store.</p> <p>New requirements are added to the context of the source semantic model.</p>"},{"location":"methodology/data_as_a_product/#_1","title":"Data as a product","text":""},{"location":"methodology/data_as_a_product/#motivations-for-moving-to-data-stream-processing","title":"Motivations for moving to data stream processing","text":"<p>The Data integration adoption is evolving with new needs to act on real-time data and reduce batch processing cost and complexity. The following table illustrates the pros and cons of data integration practices for two axes: time to insights and data integity</p> Time to insights Data integrity Low High High Lakehouse or ELT: <ul><li>+ Self-service</li><li>- Runaway cost</li><li>- No knowledge of data lost</li><li>- complext data governance</li><li>- data silos.</li></ul> Data Stream Platform: <ul><li>+ RT decision making</li><li>+ Operation and analytics on same platform</li><li>+ Single source of truth</li><li>+ Reduced TCO</li><li>+ Governance</li></ul> Low Hand coding: <ul><li>+ customized solution specific to needs.</li><li>- Slow</li><li>- difficult to scale</li><li>- opaque</li><li>- challenging governance.</li></ul> ETL:<ul><li>+ Rigorous</li><li>+ data model design</li><li>+ governed</li><li>+ reliable</li><li>- Slow</li><li>- Point to point</li><li>- Difficult to scale.</li></ul>"},{"location":"methodology/data_as_a_product/#assessment-questions","title":"Assessment questions","text":"<p>Try to get an understanding of the data integration requirements by looking at:</p> <ul> <li>Current data systems and data producers to a messaging system like Kafka</li> <li>Development time to develop new streaming logic or ETL job</li> <li>What are the different data landing zones and for what purpose. Review zone ownership.</li> <li>Level of Lakehouse adoption and governance, which technology used (Iceberg?)</li> <li>Is there a data loop back from the data lake to the OLTP?</li> <li>Where data cleaning is done?</li> <li>Is there any micro-batching jobs currently done, at which frequency, for which consuners?</li> <li>What data governance used?</li> <li>How data quality control is done?</li> </ul>"},{"location":"methodology/data_as_a_product/#migration-context","title":"Migration Context","text":"<p>A direct \"lift and shift\" approach\u2014where batch SQL scripts are converted to Flink statements on a one-to-one basis\u2014is not recommended. Refactoring is essential, as SQL processing often differs significantly in the cases involving complexity and stateful operators, such as joins.</p> <p>Most of the filtering and selection scripts can be ported 1 to 1. While most stateful processing needs to be refactorized and deeply adapted to better manage states and complexity.</p> <p>There is a repository with tools to process existing dbt project to find dependencies between tables, use local LLM to do some migrations, and create target pipelines per sink table. </p>"},{"location":"methodology/data_as_a_product/#time-condiderations","title":"Time condiderations","text":""},{"location":"methodology/data_as_a_product/#some-implementation-challenges","title":"Some implementation challenges","text":""},{"location":"methodology/data_as_a_product/#git-project-organization","title":"Git project organization","text":"<p>For closely related Bounded Contexts within the same application we can have a repository with different folders per bounded context.</p> <p>The internal structure of each Bounded Context folder typically follows a layered or modular architecture:</p> <ul> <li>domain/: Contains the core domain logic: entities, value objects, aggregates, domain services, and domain events. This layer should be independent of any infrastructure concerns.</li> <li>application/: Contains use cases or application services that orchestrate interactions with the domain layer to fulfill specific business requirements. It often handles transactions and authorization.</li> <li> <p>infrastructure/: Contains the implementation details for interacting with the outside world:</p> <ul> <li>persistence/: Repository implementations using specific database technologies.</li> <li>messaging/: Implementations for sending and receiving domain events or commands using message queues or kafka topics.</li> <li>external-services/: Clients for interacting with other systems or APIs.</li> </ul> </li> <li> <p>interfaces/ (or api/, web/): Contains the entry points to the Bounded Context, such as REST API controllers, GraphQL resolvers, or UI-specific code. It's responsible for request handling and response formatting, often using DTOs to translate between the interface and the application layer.</p> </li> <li>tests/: Contains unit tests, integration tests, and potentially end-to-end tests for the Bounded Context.</li> <li>shared/ (within a Bounded Context): Might contain utilities or helper classes specific to this Bounded Context.</li> </ul> <p>This structure can be in <code>src/main/java</code> for Java project, or src for python/ Fast API project.</p>"},{"location":"methodology/data_as_a_product/#joins-considerations","title":"Joins considerations","text":"<p>The SQL, LEFT JOIN, joins records that match and don\u2019t match on the condition specified. For non matching record the left columns are populated with NULL. SQL supports LEFT ANTI JOIN, but not Flink. So one solution in Flink SQL is to use a null filter on the left join condition:</p> <pre><code>from table_left\nleft join table_right\n    on table_left.column_used_for_join = table_right.column_used_for_join\n    where table_right.column_used_for_join is NULL;\n</code></pre>"},{"location":"methodology/data_as_a_product/#source-of-information-go-deeper","title":"Source of information - go deeper","text":"<ul> <li>Martin Fowler - Designing Data Product</li> <li>Data Mesh Principals - Zhamak Dehghani</li> <li>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh - Zhamak Dehghani</li> <li>Confluent Blog - Data Products, Data Contracts, and Change Data Capture - Adam Bellemare</li> </ul>"},{"location":"techno/cc-tableflow/","title":"Confluent Tableflow","text":"<p>Product blog</p>"},{"location":"techno/cc-tableflow/#goals","title":"Goals","text":"<p>Allow to represent a Kafka topic as a table in Apache Iceberg or Delta Lake format. It becomes the mediation layer between operational data and analytical data zone. It is using Schema registry to get the schema definition of the table.  It addresses a unified storage view on top of object storage.</p> <p>Kafka topic is the source of truth of the data. Tableflow supports the open table format: a table and catalog for analytics. It is part of the data as a product architecture.</p> <p>For Data engineers in data lakehouse environment, kafka topic is seen as table.</p>"},{"location":"techno/cc-tableflow/#value-propositions","title":"Value Propositions","text":"<ul> <li>It is a Cloud service, per region.</li> <li>The data from the topic is moved to object storage in parquet format with Iceberg metadata. </li> <li>Need to bring your own object storage (s3 bucket)</li> <li>Work with private network, using gateway private endpoints to S3. No traffic over public internet.</li> <li>Data refreshness is down to 15mn, default 6 hours. For higher need, it can read from broker directly, at the minute level.</li> <li>Start from the earliest offset.</li> <li>Can compact multiple small files in bigger file.</li> <li>It keeps track of committed osffset in iceberg.</li> <li>Write data as encrypted at source level.</li> <li>Charge for sink connector and egress is waived, pricing is based on per topic/hour and GB processed.</li> </ul>"},{"location":"techno/cc-tableflow/#current-limitations","title":"Current limitations","text":"<ul> <li>Consume only from append log topic, and non compacted topic.</li> <li>DLQ not supported yet</li> <li>Not changing the log type once enabled.</li> <li>No support BYOK clusters</li> <li>Could </li> </ul>"},{"location":"techno/cc-tableflow/#architecture","title":"Architecture","text":"<ul> <li>Kafka cluster on Confluent Cloud</li> <li>Tableflow capability</li> <li>S3 access policy and service role</li> </ul>"},{"location":"techno/cc-tableflow/#persisted-data-in-s3","title":"Persisted Data in S3","text":"<ul> <li>Keep metadata of records in original topic like topic name, timestamp and offset</li> </ul>"},{"location":"techno/ccloud-flink/","title":"Confluent Cloud for Apache Flink","text":"Chapter updates <ul> <li>Created 10/2024 </li> <li>Review 10/31/24 Updated 4/08/2025</li> </ul> <p>Confluent Cloud for Apache Flink\u00ae is a cloud-native, managed service, for Flink, strongly integrated with the Confluent Cloud Kafka managed service. It is a simple, serverless and scalable way to build real-time, reusable data products over streams.</p> <p></p> <p>Confluent Cloud Flink is built on the same open-source version as Apache Flink\u00ae with additional features:</p> <ul> <li>Regional service to run Flink in a serverless offering</li> <li>Auto-inference of the Confluent Cloud environment, Kafka cluster , topics and schemas, to Flink SQL constructs of catalog, databases and tables.</li> <li>Autoscaling capabilities, up and down</li> <li>Default system column for timestamps using the <code>$rowtime</code> column.</li> <li>Default watermark strategy based on <code>$rowtime</code>.</li> <li>Support for Avro, JSON Schema, and Protobuf.</li> <li>CREATE TABLE statements provision resources as Kafka topics and schemas (temporary tables not supported).</li> <li>Read from and write to Kafka in two modes: append-stream or update-stream (upsert and retract).</li> </ul> <p>Some limitations:</p> <ul> <li>No support for DataStream apps.</li> <li>No support or Flink connectors, only Kafka</li> </ul>"},{"location":"techno/ccloud-flink/#key-concepts","title":"Key Concepts","text":"<ul> <li>This is a regional service, in one of the three major cloud providers. It is defined in a context of a Confluent's environment.</li> <li>Compute pools groups resources for running Flink clusters, which may scale down to zero. They run SQL statements. Maximum pool size is defined at creation. Statements, in different compute pools, are isolated from each other. </li> <li>Capacity is measured in Confluent Flink Unit, CFU. Each statement is at least 1 CFU-minute.</li> <li>A statement may be structural (DDL) and stop once completed, or runs in background to write data to table (DML).</li> <li>Supports multiple Kafka clusters within the same Confluent Cloud organization in a single region.s</li> <li>Any table created in CC Flink appears as a topic in CC Kafka. Kafka Topics and schemas are always in synch with Flink.</li> <li>The differences with the OSS version, is that the DDL statements of catalog, database, table are mapped to physical Kafka objects. Table is a schema and a topic, catalog is an environment, and database is a Kafka cluster.</li> <li>Developers work in a workspace, to manage their Apache Flink\u00ae streaming applications, allowing them to easily write, execute, and monitor real-time data processing queries using a user-friendly SQL editor. Workspaces are not mandatory, as Developers may also deploy Flink statements via CLI or REST API.</li> <li>CC offers the Autopilot feature, to automatically adjusts resources for SQL statements based on demand. When messages processing starts to be behind, Autopilot adjusts resource allocation.</li> <li>Supports role-based access control for both user and service accounts.</li> <li>Stream lineage provides insights at the topic level about data origin to destinations. </li> <li>For Watermark configuration, Confluent Cloud for Apache Flink\u00ae manages it automatically, by using the <code>$rowtime</code> column, which is mapped to the Kafka record timestamp, and by observing the behavior of the streams to dynamically adapt the configuration.</li> <li>Service accounts are used for production deployment to enforce security boundaries. Permissions are done with ACL and role binding. They can own any type of API keys that can be used for CLI or API access.</li> <li> <p>Snapshot query helps to do apoint-in-time/snapshot query, to get a result at the moment of query submission, and that query would transition to Completed once done. Generates only one final result set. It will query from kafka topic earliest record, until now, or can mix with Tableflow parquet table. This is a combination of Flink batch + time constraint query.      <pre><code>SET 'sql.snapshot.mode' = 'now';\nSELECT count(*) as nb_records from tablename;\n</code></pre></p> <p>See simple demo</p> </li> <li> <p>External lookups</p> </li> </ul> Statement life cycle <p>Use a service account for background statements. Submit a SQL statement using the client shell:</p> <pre><code>confluent flink shell --compute-pool ${COMPUTE_POOL_ID} --environment ${ENV_ID} --service-account ${account_id}\n</code></pre> <p>It is possible to pause and resume a SQL statement. See cookbook for the best practices and process to update existing statements. </p> How to change the CFU limit? <p>CFU can be changed via the console or the cli, up to the limit of 50. Going above developers need to open a ticket to the Confluent support.</p> What is behind a compute pool? <p>A compute pool groups 1 job manager and n task manager. Task manager resource configuration is not configurable and is designed to support small usage as well as moderate traffic. The limit to 50 CFUs is to address trade-off between coordination overhead and scaling needs. A Flink dag with source and sink operators impact the throughput of task manager so it is always challenging to assess how many task manager to be support by a job manager.  Large states are persisted to disk and this impact the compute pool resources too. </p> <ul> <li>Statement can be moved between compute pools</li> </ul>"},{"location":"techno/ccloud-flink/#confluent-cloud-architecture","title":"Confluent Cloud Architecture","text":"<p>The Confluent Cloud for Kafka and for Flink is based on the SaaS pattern of control and data planes. See this presentation - video from Frank Greco Jr.</p> <p></p> <ul> <li>Each data plane is made of a VPC, a kubernetes cluster, a set of Kafka clusters and some management services to support platform management and communication with the control plane.</li> <li>The control plane is called  the mothership, and refers to VPC, services, Database to manage the multi-tenancy platform, a kubernetes cluster, Kafka cluster, and other components. This is where the Confluent console runs for users to administer the Kafka clusters. </li> <li>For each data plane VPC, outbound connections are allowed through internet gateways.</li> <li>There is a scheduler service to provision resources or assign cluster to existing resources. Target states are saved in a SQL database, while states are propagated from the different data planes to the mothership. This communication is async and leverage a global Kafka cluster.</li> <li>There are the concepts of physical Kafka clusters and logical clusters. Logical clusters are groupings of topics on the physical clusters isolated from each other via a prefix. Professional Confluent Cloud organization can only have logical clusters. Enterprise can have physical clusters.</li> </ul>"},{"location":"techno/ccloud-flink/#getting-started","title":"Getting Started","text":"<p>Install the Confluent CLI and get an Confluent Cloud account. </p> <p>See those tutorials for getting started.</p> <ul> <li>Quickstart with Console</li> <li>Apache Flink\u00ae SQL</li> <li>Confluent github, Flink workshop</li> <li>Java Table API Quick Start</li> </ul> <p>There is also a new confluent cli plugin: <code>confluent-flink-quickstart</code> to create an environment, a Flink compute pool, enable a schema registry, create a Kafka cluster and starts a Flink shell. </p> <pre><code>confluent flink quickstart --name my-flink-sql --max-cfu 10 --region us-west-2 --cloud aws\n</code></pre>"},{"location":"techno/ccloud-flink/#some-common-commands-to-manage-confluent-cloud-environment","title":"Some common commands to manage Confluent Cloud environment","text":"<pre><code># Create an environment\nconfluent environment create my_environment --governance-package essentials\n# Set the active environment.\nconfluent environment use &lt;environment id&gt;\n# Create a cluster\nconfluent Kafka cluster create my-cluster --cloud gcp --region us-central1 --type basic\n# Create Kafka API key\nconfluent Kafka cluster list\nexport CLID=&lt;Kafka cluster id&gt;\nconfluent api-key create --resource $CLID\n# Create a compute pool (adjust cloud and region settings as required).\nconfluent flink compute-pool create my-compute-pool --cloud gcp --region us-central1 --max-cfu 10\n# Create a Flink api key which is scoped in an environment + region pair\nconfluent api-key create --resource flink --cloud gcp --region us-central1\n# Define an api key for schema registry\nconfluent schema-registry cluster describe\nconfluent api-key create --resource &lt;schema registry cluster&gt;\n# Get the user id\nconfluent iam user list\n# To shutdown everything:\nconfluent environment list\nconfluent environment delete &lt;ENVIRONMENT_ID&gt;\n</code></pre> <p>For study and demonstration purpose, there is a read-only catalog named <code>examples</code> with database called <code>marketplace</code> which has data generators for different SQL tables. </p> <p>Set the namespace for future query work using:</p> <pre><code>use catalog examples;\nuse marketplace;\nshow tables;\n</code></pre> <p>To use your dedicated environment use the following syntax:</p> <pre><code>use catalog my-flink-sql_environment;\nuse  my-flink-sql_Kafka-cluster;\n</code></pre>"},{"location":"techno/ccloud-flink/#use-the-flink-sql-shell","title":"Use the Flink SQL shell","text":"<p>Using the confluent cli, we can access to the client via:</p> <pre><code>#  \nconfluent environment list\n\n# Get the compute pool id\nconfluent flink compute-pool list\nexport ENV_ID=$(confluent environment list -o json | jq -r '.[] | select(.name == \"aws-west\") | .id')\nexport COMPUTE_POOL_ID=$(confluent flink compute-pool list -o json | jq -r '.[0].id')\nconfluent flink shell --compute-pool $COMPUTE_POOL_ID --environment $ENV_ID\n</code></pre>"},{"location":"techno/ccloud-flink/#using-the-flink-editor-in-confluent-cloud","title":"Using the Flink editor in Confluent Cloud","text":"<p>Nothing special to mention, except that users need to recall that once the job is started, they cannot modify it:they need to stop before any future edition. Restarting may mean reprocess from the earliest records. It is recommended to persist the Flink statement in a git repository and manage the deployment using Confluent CLI or shift_left CLI tool.</p>"},{"location":"techno/ccloud-flink/#using-the-flink-table-api","title":"Using the Flink Table API","text":"<p>Confluent Cloud for Flink supports the Table API, in Java or Python.</p> <p>The Table API is on top of the SQL engine, and so program runs on an external systems, but uses an specific Flink environment for Confluent Cloud to submit the DAG to the remote engine. The program declares the data flow, submit it to the remote job manager.  </p> <p>When running TableAPI with Confluent Cloud for Flink, there are some specifics code to have:</p> <ol> <li> <p>Set the environment variables to connect to Confluent Cloud:     </p> </li> <li> <p>Create a Table environment in the Java or Python code like:     <pre><code>ConfluentSettings.Builder settings = ConfluentSettings.newBuilderFromResource(\"/prod.properties\")\n</code></pre></p> </li> <li> <p>Package and run</p> </li> </ol> <p>Read this chapter for more information.</p>"},{"location":"techno/ccloud-flink/#dlq-support","title":"DLQ support","text":"<p>By integrating Custom Deserialization Error Handling Strategies, data engineers can ensure that only valid, correctly processed messages move downstream, maintaining data quality and integrity. This feature reduces the risk of system crashes and downtime caused by unhandled exceptions, ensuring continuous data processing and availability.</p> <p>All Flink tables have <code>error-handling.mode</code> as a table option, with the default being <code>fail</code>. </p> <ul> <li> <p>If desired, you can run an ALTER TABLE to change this to <code>ignore</code> or <code>log</code>.     <pre><code>ALTER TABLE src_users_table SET ('error-handling.mode' = 'log');\n</code></pre></p> </li> <li> <p>or add this config to the created table:     <pre><code>create table src_users_table (...) WITH (\n    ....\n    'error-handling.mode' = 'log'\n)\n</code></pre></p> </li> </ul> Potential error <p>It is possible to get the following error when altering table failed registering schemas: unable to register schema on 'error_log-value': schema registry request failed error code: 42205: Subject error_log-value in context  is not in read-write mode. In this case, you will run into this error as flink is trying to register a schema for the DLQ and Schema Regisry is being schema-linked as a result the default context is in Read only mode. You need to create your own DLQ table. </p> <ul> <li> <p>Or create a special DLQ topic:     <pre><code>CREATE TABLE `my_error_log` (\n    `error_timestamp` TIMESTAMP_LTZ(3) NOT NULL,\n    `error_code` INT NOT NULL,\n    `error_reason` STRING NOT NULL,\n    `error_message` STRING NOT NULL,\n    `error_details` MAP&lt;STRING NOT NULL, STRING&gt; NOT NULL,\n    `processor` STRING NOT NULL,\n    `statement_name` STRING,\n    `affected_type` STRING NOT NULL,\n    `affected_catalog` STRING,\n    `affected_database` STRING,\n    `affected_name` STRING,\n    `source_record` ROW&lt;`topic` STRING, `partition` INT, `offset` BIGINT, `timestamp` TIMESTAMP_LTZ(3), `timestamp_type` STRING, `headers` MAP&lt;STRING NOT NULL, VARBINARY&gt;, `key` VARBINARY, `value` VARBINARY&gt;\n) WITH (\n    'value.avro-registry.schema-context' = '.flink-stage',\n    'value.format' = 'avro-registry'\n    )\n</code></pre></p> <p>Alter the source table to enable with a DLQ that was created above.</p> <pre><code>ALTER TABLE src_users_table SET ('error-handling.mode' = 'log', 'error-handling.log.target' = 'my_error_log' );\n</code></pre> </li> </ul>"},{"location":"techno/ccloud-flink/#networking-overview","title":"Networking overview","text":"<p>See the product documentation for managing Networking on Confluent Cloud. </p> <p>Kafka clusters have the following properties:</p> <ul> <li>Basic and standard clusters are multi-tenant and accessible via secured (TLS encrypted) public endpoints.</li> <li>Using private link does not expose Kafka clusters to the public.</li> <li>Enterprise clusters are accessible through secure AWS PrivateLink or Azure Private Link connections.</li> <li>Secure public endpoints are protected by a proxy layer that prevents types of DoS, DDoS, syn flooding, and other network-level attacks.</li> <li>A Confluent Cloud network is an abstraction for a single tenant network environment. See setup CC network on AWS.. </li> <li>For AWS and Confluent Dedicated Clusters, networking can be done via VPC peering, transit gateway, inbound and outbound private link (for Kafka and Flink): this is a one-way connection access from a VPC to CC.</li> <li>Flink Private Networking requires a PrivateLink Attachment (PLATT) to access Kafka clusters with private networking. It is used to connect clients such as confluent CLI, the console, the rest api or terraform with Flink. Flink-to-Kafka is routed internally within Confluent Cloud.</li> </ul> <p></p> <ul> <li>PLATT is independant of the network type: PrivateLink, VPC peering or transit GTW.</li> </ul>"},{"location":"techno/ccloud-flink/#autopilot","title":"Autopilot","text":"<p>Autopilot automatically scales up and down compute pool resources needed by SQL statements. It uses the property of parallelism for operator to be able to scale up and down. <code>SELECT</code> always runs a parallelism of 1. Only <code>CREATE TABLE AS</code>, <code>INSERT INTO</code> and <code>EXECUTE STATEMENT SET</code> are considered by Autopilot for scaling. Global aggregate are not parallelized. The main goal of the auto scaler is to maintain optimum  throughput and number of resources (or CFUs). </p> <p>The SQL workspace reports the scaling status.  It is important that each job has a maximum parallelism, limited by the number of resource available. For source operators within a Flink DAG the limit is the number of partitions in the input topics. </p> <p>If there is some data skew and one operator is set with a parallel of 1 then there is no need to scale.</p> <p>When the compute pool is exhausted, try to add more CFU or stop some running statements to free up resources.</p> <p>The autoscaler is using historical metrics to take the decision to scale up. 3 to 4 minutes of data are needed. A job should scale up within minutes if the backlog is constantly growing, and scale down if there are no input data and the backlog. The interesting metrics is the pending records. The algorithm needs to take into account the pending records amount, the current processing capacity, the time to scale up, but also the input data rate, the output data rate for each operator in the DAG. There is no way updfront to estimate the needed capacity. This is why it is important to assess the raw input table/kafka size and avoid restarting the first Flink statements that are filtering, deduplicating records to reduce the number of messages to process downstream of the data pipeline.</p> <p>Autopilot exposes the CFU usage in CFU minutes via the metrics API at the compute pool level.</p> <p>When multiple statements are in the same compute pool, new statement will not get resource until existing one scales down. Consider looking at Statement in Pending state and reallocated them to other compute pool. The total number of jobs is less than the CFU limit.</p> When a statement is not scaling up what can be done? <pre><code>Consider looking at the CFU limit of the compute poolas it may has been reached. The Flink job may have reached it\u2019s effective max parallelism, due to not enough Kafka topic partition from the input tables. Consider looking at the data skew, as a potential cause for scale-ups inefficiency. \nInternally to Confluent Cloud for Flink, checkpoints may take a long time. The autopilot may rescale only after the current checkpoint has completed or 2 checkpoints have failed in a row.\n</code></pre> <p>See discussion of adaptive scheduler from Flink FLIP-291.</p>"},{"location":"techno/ccloud-flink/#cross-region-processing","title":"Cross-region processing","text":"<p>Within an environment, there is one schema registry. We can have multiple Kafka clusters per region and multiple Flink compute pools per region. Any tables created in both region with the same name will have the value and key schemas shared in the central schema registry. The SQL Metastore, Flink compute pools and Kafka clusters are regional. </p>"},{"location":"techno/ccloud-flink/#monitoring-and-troubleshouting","title":"Monitoring and troubleshouting","text":"<p>Once the Flink SQL statement runs, Data Engineers may use the Console, (Environment &gt; Flink &gt; Flink page &gt; Flink statements) to assess the list of statements and their state of processing. </p> <p></p> <p>See the monitoring product documentation for explanations of the different fields. The following fields are important to consider:</p> Field Why to consider Status Verify the state of the Flink query Statement CFU Server resource used by the statement Messages Behind Is the query behind, is there some backpressure applied Message out Rate of messages created by the query State Size in GB Keep it low, alert at 300+ GB <p>Look at the statement status, consider failed, pending, degraded. Some issues are recoverables, some not:</p> Recoverable Non-recoverable User Kafka topic deletion, loss of access to cloud resources De/Serialization exception, arithmetic exception, any exception thrown in user code System checkpointing failure, networking disruption Actions If recovery takes a long time or fails repeatedly, and if this is a user execption, the message will be in the status.detail of the statement, else the user may reach to the support. User needs to fix the query or data. <p>Be sure to enable cloud notifications and at least monitor topic consumer lag metric. As a general practices, monitoring for <code>current_cfus = cfu_limit</code> to avoid exhaustion of compute pools.  </p> <p>The <code>flink/pending.records</code> is the most important metrics to consider. It corresponds to consumer lag in Kafka and \u201cMessages Behind\u201d in the Confluent Cloud UI. Monitor for high and increasing consumer lag.</p> <p>At the Statement level we can get the following metrics, over time:</p> <p></p> <p>And with the <code>Query profiler</code>, which represents the same content as the Flink console UI, we can assess each operator of the query DAG, with CPU utilization, state size, ...</p> <p></p> <ul> <li>Confluent Cloud for Apache Flink supports metrics integrations with services like Prometheus, Grafana and Datadog.</li> </ul> <p></p> <ul> <li> <p>DML statement failing, or being degraded, or pending can be notified to external system. See the notification for CC documentation</p> </li> <li> <p>Flink monitoring statement product documentation</p> </li> <li>Docker compose, Prometheus setup and Grafana Dashboard for Confluent Cloud for Flink reporting.</li> </ul>"},{"location":"techno/ccloud-flink/#role-base-access-control","title":"Role Base Access Control","text":""},{"location":"techno/ccloud-flink/#understanding-pricing","title":"Understanding pricing","text":"<p>The CFU pricing is here. Price per hour computed by the minute. </p> <p>Some core principals:</p> <ul> <li>Flink SQL runs each statement independently of any others.</li> <li>Not overpay for processing capacity. Pay for what is used. Increment at the minute level.</li> <li>Short live queries cost a real minimum, and can be done in a shared compute pool</li> <li>Long running queries cost is aggregated per hour with minute increment. So a statement starting at 1 CFU for 10 minutes then 3 CFUs for 30 and back to 2 for 10 and 1 for 10 will use 10 + 90 + 20 + 10 = 130 CFUs for the hour.</li> <li>Statement throughput generally scales linearly in the number of CFUs available to a statement.</li> <li>The Max CFU parameter is a just for Budget control</li> </ul> <p>To estimate CFU consumption we need to:</p> <ol> <li>Expected record per second (RPS) / throughput </li> <li>Message size and total number of messages to process</li> <li> <p>Type of SQL, select only, or joins, grouping...</p> <ul> <li>simple 1 to 1 select stateless transformation is determined by how much write volume the sink topic can handle.</li> <li>For Joins, aggregates, ... the way in which a statement must access and maintains the state is more influential than the raw quantity of state.</li> <li>The total of all CFU estimates across the workload will provide a rough approximation of total CFUs required</li> </ul> </li> </ol> <p>Several factors significantly affect statement throughput:</p> <ul> <li>State Overhead: The overhead related to maintaining state affects JOINs and aggregations more than the quantity of state itself. </li> <li>CPU Load: The complexity of the operations performed by the statement is a major contributor to CPU load.</li> <li>Minimum CFU Consumption: Every statement will consume at least 1 CFU, and for most workloads, CFU consumption is directly proportional to the number of statements execute</li> </ul>"},{"location":"techno/ccloud-flink/#scoping-workload","title":"Scoping workload","text":"<ul> <li>Assess the number of record per seconds</li> <li>Be sure to clarrify that Confluent Cloud for Flink is used for streaming not batching. Operations like update, truncate, delete table are not supported.</li> <li>For stateless the attainable throughput of the statement per CFU will generally be determined by how much write volume the sink topic can handle.</li> <li>Most important throughput factor is the State size, its access and management. </li> <li>Statement throughput generally scales linearly in the number of CFUs available to a statement.</li> <li>UDF impacts throughtput.</li> <li>For each statement, assess the number of records to process per seconds or minutes. Consider ingress message size and egress message size as SQL may generates less data. Also Windowing will generate less messages too. Joins will impact performance depending if they are static or with time window. </li> <li>Look at Kafka message sizes (bytes) as well as message throughput.</li> </ul> <p>As a base for discussion, 10k record/s per CPU is reachable for simple Flink stateless processing.</p>"},{"location":"techno/ccloud-flink/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>CC Flink is a regional, multi-AZ service.</li> <li>In case of Job failure, failed jobs auto-restart using the last known state, last checkpoint.</li> <li>Checkpoint is used for in-region fault tolerance. Checkpoints capture the state of a Flink job at regular intervals, including Kafka consumer offsets, operator states, and internal timers. In CC checkpoints are done every minute</li> <li>In case of Cloud Provider failure, there is no protection, for a region lost. To address that, architects need to set up cross region DR</li> <li>All Flink DR options first require a DR strategy for Kafka &amp; Schema Registry (SR). It needs to have an exact replication of the data (including offsets) and schemas.</li> <li>On CC, cluster link and schema link supports data and schema replication.</li> </ul> <p>As any flink solution, the following need to be deeply assessed:</p> <ul> <li>Can Flink's state be recreated?  This is driven by the underlying Kafka Clusters RPO and their retention.</li> <li>How long is tolerable to recreate that state? This is driven by the overall RTO. </li> <li>What is the semantic expected by consumer apps? This is driven by consuming apps tolerances. Semantics options are: exactly-once, at-least once (duplicate possible), at-most once (data loss and duplicate possible).</li> <li>Is the Flink job processing deterministic? will a Flink job always output the same results?</li> </ul>"},{"location":"techno/ccloud-flink/#active-active","title":"Active / Active","text":"<p>The approach is to have two identical Flink jobs or pipelines of jobs run in parallel continuously in both regions. They process the same data, with some replication delay in the secondary region.</p> <p>This is recommended for low RTO requirements, with Flink jobs with large states, or solutions requiring Exactly-Once semantics, or when it is critical that the 2 regions have exactly the same data results.</p> <p>To consider:</p> <ul> <li>Setup replication only to the input topics. </li> <li>Mirror configuration like service accounts, RBACs, private networking...</li> <li>Ensure Flink jobs have deterministic query results.</li> <li>Jobs should support out-of-order arrival between input tables.</li> </ul>"},{"location":"techno/ccloud-flink/#active-passive","title":"Active / Passive","text":"<p>Flink jobs are started, in second region, only on failover.</p> <p>This approach is possible for stateless jobs, or when states can be created quickly: Flink Jobs Window Size and time to recompute job state &lt; RTO. Solutions based on at-least once, or at-most-once. Even for stateless jobs, Exactly-Once semantics is not supported.</p> <p>To consider:</p> <ul> <li>More complicated to orchestrate as the process needs to recreate tables and jobs during failover</li> <li>Topic retention &gt; Time window needed to recreate state (dictated by window size or TTL). Without enough retention, results will be wrong, or only subset of queries would work.</li> <li>Any time window and aggregation needs to use event time and not processing time.</li> <li>Setup replication only to the input topics. </li> </ul>"},{"location":"techno/ccloud-flink/#some-faqs","title":"Some FAQs","text":"<ul> <li>Processing time support? It is better to use event time to ensure results are correct, deterministic and reproductible. Using processing time for windowing operations may lead to non-determnistic results. Processing time may be relevant for temporal joins.</li> <li>Is Hive load/unload function supported? The Flink OSS has this load/unload Hive functions capability, but all those Hive functions are already available in CC Flink.</li> <li>How to add Jar? ADD and REMOVE JAR are meant for testing purposes in OSS Flink. For UDFs, Jars can be uploaded via Confluent Console, CLI, REST API or Terraform.</li> <li>How to manage Catalog and Database? In Confluent Cloud Catalog is a Confluent Environment so no direct management from Flink session. Database is a Kafka cluster so the same logic applies.</li> </ul>"},{"location":"techno/ccloud-flink/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Confluent Flink workshop to learn how to build stream processing applications using Apache Flink\u00ae on Confluent Cloud.</li> <li>Shoe-store workshop with Terraform and SQL demonstration using DataGen.</li> <li>SQL coding practice from this repo.</li> </ul>"},{"location":"techno/cp-flink/","title":"Confluent Platform for Flink","text":"<p>The official product documentation after 07/2025 release is here. </p> <p>CP Flink is the Confluent Flink product that is extremely flexible and configurable to address a large set of user's requirements. It is a supported Flink with a subset of Flink components: SQL, Table API, DataStream API, ProcessFunction. Using Kafka, FileSystem, JDBC and CDC connectors, and using a central management feature for Kubernettes deployment and security.</p> <p>The main features are:</p> <ul> <li>Fully compatible with open-source Flink. </li> <li>Deploy on Kubernetes using Helm. It is only supported on kube. On Kubernetes, machine or Flink process failures will be recovered by Kubernetes, guaranteeing high uptime, and low latency.</li> <li>Define environment, which does a logical grouping of Flink applications with the goal to provide access isolation, and configuration sharing.</li> <li>Deploy application with user interface and task manager cluster</li> <li>Exposes custom kubernetes operator for specific CRD</li> <li>When integrated with Kafka, Kafka CP can run on bare metal while CP Flink on kube.</li> </ul> <p>The figure below presents the Confluent Flink components deployed on Kubernetes:</p> Confluent Platform for Flink <ul> <li>CFK supports the management of custom CRD, based on the Flink for Kubernetes Operator. (CFK 3.0.0 supports CP 8.0.0)</li> <li>CMF (Confluent Manager for Apache Flink) adds security control, and a REST API server for the cli or a HTTP client</li> <li>FKO is the open source Flink for Kubernetes Operator</li> <li>Flink cluster are created from command and CRDs and run Flink applications within an environment</li> </ul> <p>Be sure to have confluent cli.</p>"},{"location":"techno/cp-flink/#specific-concepts-added-on-top-of-flink","title":"Specific concepts added on top of Flink","text":"<ul> <li> <p>Confluent Manager for Flink (CMF) provides:</p> <ul> <li>Job life-cycle management for Flink jobs.</li> <li>Integration with Confluent Platform for authentication and authorization (RBAC).</li> <li>Well-defined REST APIs and command-line interfaces (CLIs).</li> <li>Store metadata in its own embedded database</li> <li>It is a kubernetes operator to manage custom resources</li> </ul> </li> <li> <p>Environment: for access control isolation, and Flink configuration sharing</p> </li> <li>Compute pool represents resources to run Task manager and Job manager. Each Flink SQL statement is associated with exactly one Compute Pool. See example of pool definition and in cmf folder</li> <li> <p>SQL catalog to group database concept for Flink SQL table queries. It references a Schema Registry instance and one or more Kafka clusters.</p> </li> <li> <p>See Product FAQs</p> </li> </ul>"},{"location":"techno/cp-flink/#product-set-up","title":"Product Set Up","text":"<ul> <li>See my dedicated chapter for Confluent Plaform Kubernetes deployment.</li> <li>See the makefile to deploy CMF, and the product documentation </li> </ul>"},{"location":"techno/cp-flink/#deployment-architecture","title":"Deployment architecture","text":"<ul> <li>A Flink cluster always needs to run in one K8s cluster in one region, as the Flink nodes are typically exchanging a lot of data, requiring low latency network.</li> <li>For failover between data centers, the approach if to share durable storage (HDFS, S3, Minio) accessible between data centers, get Kafka topic replicated, and being able to restart from checkpoints.</li> <li>Recall that Flink can interact with multiple Kafka Clusters at the same time.</li> </ul>"},{"location":"techno/cp-flink/#important-source-of-information-for-deployment","title":"Important Source of Information for Deployment","text":"<ul> <li>Confluent Platform deployment using kubernetes operator</li> <li>Deployment overview and for Apache Flink. </li> <li>CP Flink supports K8s HA only.</li> <li>Flink fine-grained resource management documentation.</li> <li>CP v8 announcement: builds on Kafka 4.0, next-gen Confluent Control Center (integrating with the open telemetry protocol (OTLP),  Confluent Manager for Apache Flink\u00ae (CMF)), Kraft native (support significantly larger clusters with millions of partitions), Client-Side field level encryption. CP for Flink support SQL, Queues for Kafka is now in Early Access, </li> </ul>"},{"location":"techno/cp-flink/#metadata-management-service-for-rbac","title":"Metadata Management Service for RBAC","text":"<ul> <li>Metadata Service Overview</li> <li>Single broker Kafka+MDS Deployment</li> <li>Git repo with working CP on K8s deployment using RBAC via SASL/Plain and LDAP</li> <li>Configure CP Flink to use MDS for Auth</li> <li>Additional Flink RBAC Docs</li> <li>How to secure a Flink job with RBAC</li> <li>Best Practices for K8s + RBAC</li> </ul>"},{"location":"techno/cp-flink/#sql-specific","title":"SQL specific","text":"<p>As of Sept 2025 SQL support is still under-preview.</p>"},{"location":"techno/cp-flink/#applications","title":"Applications","text":"<p>Application documentation or FlinkDeployment, an exposed kubernetes CR.</p> <p>Flink SQL is supported only as part of a packaged JAR application based on the TableEnvironment interface.</p> <p>Important points:</p> <ul> <li>After CMF is installed and running, it will continuously watch Flink applications.</li> <li>Environment control k8s namespace in which the Flink application is deployed.</li> <li>Suspended app, does not consume resource, and may be restored from its savepoint.</li> <li> <p>CLI, REST API or the Console can be used to deploy app.</p> </li> <li> <p>See the getting started with a Java project for Flink, see also the Confluent specific settings</p> </li> </ul> <p>Examples of apps:</p> <ul> <li>code/</li> <li>e2e-demos</li> </ul>"},{"location":"techno/cp-flink/#understanding-sizing","title":"Understanding Sizing","text":"<p>The observed core performance rule is that Flink can process ~10,000 records per second per CPU core. This baseline may decrease with larger messages, bigger state, key skew, or high number of distinct keys.</p> <p>There are a set of characteristics to assess before doing sizing:</p>"},{"location":"techno/cp-flink/#statement-complexity-impact","title":"Statement Complexity Impact","text":"<p>Statement complexity reflects the usage of complex operators like:</p> <ul> <li>Joins between multiple streams</li> <li>Windowed aggregations</li> <li>Complex analytics functions</li> </ul> <p>More complex operations require additional CPU and memory resources.</p>"},{"location":"techno/cp-flink/#architecture-assumptions","title":"Architecture Assumptions","text":"<ul> <li>Source &amp; Sink latency: Assumed to be minimal</li> <li>Key size: Assumed to be small (few bytes)</li> <li>Minimum cluster size: 3 nodes required</li> <li>CPU limit: Maximum 8 CPU cores per Flink node</li> </ul>"},{"location":"techno/cp-flink/#state-management-checkpointing","title":"State Management &amp; Checkpointing","text":"<ul> <li>Working State: Kept locally in RocksDB</li> <li>Backup: State backed up to distributed storage</li> <li>Recovery: Checkpoint size affects recovery time</li> <li>Frequency: Checkpoint interval determines state capture frequency</li> <li>Aggregate state size consumes bandwidth and impacts recovery latency.</li> </ul>"},{"location":"techno/cp-flink/#resource-guidelines","title":"Resource Guidelines","text":"<p>Typical Flink node configuration includes:</p> <ul> <li>4 CPU cores with 16GB memory</li> <li>Should process 20-50 MB/s of data</li> <li>Jobs with significant state benefit from more memory</li> </ul> <p>Scaling Strategy: Scale vertically before horizontally</p>"},{"location":"techno/cp-flink/#latency-impact-on-resources","title":"Latency Impact on Resources","text":"<p>Lower latency requirements significantly increase resource needs:</p> <ul> <li>Sub-500ms latency: +50% CPU, frequent checkpoints (10s), boosted parallelism</li> <li>Sub-1s latency: +20% CPU, extra buffering memory, 30s checkpoints</li> <li>Sub-5s latency: +10% CPU, moderate buffering, 60s checkpoints</li> <li>Relaxed latency (&gt;5s): Standard resource allocation Stricter latency requires more frequent checkpoints for faster recovery, additional memory for buffering, and extra CPU for low-latency optimizations.</li> </ul>"},{"location":"techno/cp-flink/#estimation-heuristics","title":"Estimation Heuristics","text":"<p>The estimator increases task managers until there are sufficient resources for:</p> <ul> <li>Expected throughput requirements</li> <li>Latency-appropriate checkpoint intervals</li> <li>Acceptable recovery times</li> <li>Handling data skew and key distribution</li> </ul> Important Notes <ul> <li>These are estimates - always test with your actual workload</li> <li>Start with conservative estimates and adjust based on testing</li> <li>Monitor your cluster performance and tune as needed</li> <li>Consider your specific data patterns and business requirements</li> </ul> <p>See this Flink estimator project for a tool to help estimating cluster sizing.</p>"},{"location":"techno/cp-flink/#monitoring","title":"Monitoring","text":""},{"location":"techno/cp-flink/#troubleshouting","title":"Troubleshouting","text":""},{"location":"techno/cp-flink/#submitted-query-pending","title":"Submitted query pending","text":"<p>When submitting SQL query from the Flink SQL, it is possible it goes in pending: the job manager pod is created and running, but the task manager is pending. Assess what is going on with <code>kubectl describe pod &lt;pod_id&gt; -n flink</code>, which gives error message like insufficient cpu. Adding compute pool cpu is worse as the compute pool spec for task manager and job manager is for the default settings to create those pods.</p>"},{"location":"techno/fk-k8s-monitor/","title":"Monitoring Flink on Kubernetes","text":"<p>In this chapter, we assume Flink operator is deployed and some FlinkDeployments are running.</p>"},{"location":"techno/fk-k8s-monitor/#verify-operator","title":"Verify operator","text":"<ul> <li>Get the namespace wher Flink operator runs</li> </ul> <pre><code>kubectl use \nkubectl get pods \n</code></pre>"},{"location":"techno/fk-k8s-monitor/#monitoring-operator-health","title":"Monitoring Operator Health","text":""},{"location":"techno/fk-k8s-monitor/#prometheus","title":"Prometheus","text":""},{"location":"techno/fk-k8s-monitor/#dynatrace","title":"Dynatrace","text":""}]}