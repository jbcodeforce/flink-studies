{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udea7 The Complete Guide to Apache Flink and Confluent Flink","text":"<p>Welcome to \"A Guide to Apache Flink and Confluent Flink\" - a comprehensive, hands-on resource for mastering stream processing with Apache Flink and its enterprise distributions. I started this living book, in 2018 while working as IBM Distinguished Enginer / CTO for Event-Driven Architecture, and continue to enhance it while working at AWS and Confluent. It addresses practical implementations, methodologies and best practices I can share with my customers. In 2024 and 2025 I am focusing on Confluent Flink and Data mesh. All this content come from public content, articles, youtube, product documentations, git repositories.</p> <p>To make it fun, here is the book cover! Very imaginative for a book that will never go to press.</p> A virtual book about Apache Flink - 2018 to 2025 Site updates <ul> <li>Created 2018 </li> <li>Updates 10/2024: Reorganized content, separated SQL vs Java, added Confluent Cloud/Platform integration</li> <li>Updates 01/25: Added Terraform deployment examples, expanded SQL samples</li> <li>Updates 07/25: Added e2e demos, Flink estimator webapp, Data mesh</li> <li>Update 09/2025: Work on new demonstration, code samples, improve deployment for Confluent Platform for Flink</li> <li>Update 10/2025: k8s chapter, json xform and external lookup demos</li> <li>Update 02/2026: New event-driven agentic chapter, new e2e demos, SQL examples </li> </ul>"},{"location":"#what-youll-discover","title":"What You'll Discover","text":"<p>This site is designed to share studies of the Flink ecosystem, covering everything from fundamental concepts to advanced real-world implementations:</p>"},{"location":"#foundations-architecture","title":"Foundations &amp; Architecture","text":"<ul> <li>Core Flink concepts and runtime architecture</li> <li>Stream processing fundamentals and event time semantics</li> <li>Fault tolerance, checkpointing, and exactly-once processing</li> <li>State management and stateful stream processing patterns</li> </ul>"},{"location":"#programming-with-flink","title":"Programming with Flink","text":"<ul> <li>Flink SQL: From basic queries to complex analytical pipelines</li> <li>DataStream API: Building robust streaming applications in Java</li> <li>Table API: Bridging SQL and programmatic approaches</li> <li>PyFlink: Stream processing with Python</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Kubernetes-native deployments with Flink Operator</li> <li>Infrastructure as Code</li> <li>Monitoring, alerting, and performance optimization</li> <li>High availability and disaster recovery patterns</li> </ul>"},{"location":"#real-world-streaming-architectures","title":"Real-World Streaming Architectures","text":"<ul> <li>Change Data Capture (CDC) pipelines with Debezium</li> <li>Apache Kafka integration and event-driven architectures</li> <li>Data Lake integration with Apache Iceberg</li> <li>Real-time Analytics with complex event processing</li> </ul> <p>\ud83d\udea7 This guide is still under heavy development. Content, examples, and structure may change frequently. Check back often for updates! \ud83d\udea7</p>"},{"location":"#enterprise-ready-with-confluent","title":"Enterprise-Ready with Confluent","text":"<p>This book provides extensive coverage of Confluent Platform for Apache Flink and Confluent Cloud Flink, including:</p> <ul> <li>Confluent Cloud Flink compute pools and managed services</li> <li>Confluent Platform on-premises deployment patterns</li> <li>ksqlDB to Flink migration strategies and best practices</li> <li>Integration with Confluent's Schema Registry and Connect ecosystem</li> </ul>"},{"location":"#hands-on-learning-approach","title":"Hands-On Learning Approach","text":"<p>Every concept is reinforced with practical implementations:</p>"},{"location":"#code-examples","title":"Code Examples","text":"<ul> <li>20+ Java applications demonstrating DataStream patterns</li> <li>50+ SQL scripts covering joins, aggregations, and window functions</li> <li>Python Table API examples and integration patterns</li> <li>Complete end-to-end demonstrations with multiple components</li> </ul>"},{"location":"#end-to-end-demonstrations","title":"End-to-End Demonstrations","text":"<ul> <li>E-commerce Analytics Pipeline: Real-time user behavior analysis</li> <li>CDC Deduplication: Change data capture with transformation patterns</li> <li>Financial Transaction Processing: Complex event processing for fraud detection</li> <li>IoT Data Processing: Time-series analysis and alerting</li> </ul>"},{"location":"#deployment-ready-infrastructure","title":"Deployment-Ready Infrastructure","text":"<ul> <li>Docker Compose environments for local development</li> <li>Kubernetes manifests for production deployment</li> <li>Terraform modules for cloud infrastructure</li> <li>Monitoring stack with Prometheus and Grafana</li> </ul>"},{"location":"#who-this-guide-is-for","title":"Who This Guide Is For","text":"<p>Whether you're a data engineer building streaming pipelines, a software architect designing event-driven systems, or a platform engineer deploying Flink clusters, this guide provides the depth and breadth you need.</p> <p>Prerequisites: Basic familiarity with distributed systems, SQL, and Java or Python programming.</p>"},{"location":"#learning-path","title":"Learning Path","text":"<p>This book is structured to support both linear reading and focused deep-dives:</p> Quick Start Track (1-2 weeks)\"Foundation Track (3-4 weeks)\"Advanced Track (4-6 weeks)Production Track (6-8 weeks) <ol> <li>Getting Started - Your first Flink application</li> <li>Flink SQL Basics - DDLs - Stream processing with SQL - Table creation</li> <li>Flink SQL Basics - DMLs - Stream processing with SQL</li> <li>Local Deployment - Running Flink on Kubernetes</li> </ol> <ol> <li>Architecture Deep Dive - Understanding Flink internals</li> <li>State Management - Stateful stream processing</li> <li>Fault Tolerance - Exactly-once guarantees</li> </ol> <ol> <li>DataStream Programming - Complex stream processing logic</li> <li>Kafka Integration - Event-driven architectures</li> <li>Production Deployment - Enterprise deployment patterns</li> </ol> <ol> <li>Performance Tuning - Optimization techniques</li> <li>Monitoring &amp; Observability - Production operations</li> <li>End-to-End Projects - Real-world implementations</li> </ol>"},{"location":"#what-makes-this-guide-unique","title":"What Makes This Guide Unique","text":""},{"location":"#living-documentation","title":"Living Documentation","text":"<ul> <li>Continuously Updated: Content evolves with Flink releases and best practices</li> <li>Community Driven: Open source with contributions from practitioners</li> <li>Practical Focus: Every concept backed by working code examples</li> </ul>"},{"location":"#production-tested","title":"Production Tested","text":"<ul> <li>All examples are tested and validated in real environments</li> <li>Performance benchmarks and optimization guidelines</li> <li>Troubleshooting guides based on real-world challenges</li> </ul>"},{"location":"#multi-platform-coverage","title":"Multi-Platform Coverage","text":"<ul> <li>Apache Flink OSS - Complete open-source implementation</li> <li>Confluent Cloud - Fully managed cloud service</li> <li>Confluent Platform - Enterprise on-premises deployment</li> </ul> <p>\ud83d\udca1 Tip: This guide is designed to be read online at https://jbcodeforce.github.io/flink-studies/ with full navigation, search, and interactive examples. You can also clone the GitHub repository to run all examples locally.</p>"},{"location":"contributing/","title":"Contributing to this repository","text":"<p>This chapter addresses how to support the development of this open source project.</p> <p>Anyone can contribute to this repository and associated projects.</p> <p>There are multiple ways to contribute: report bugs and suggest improvements, improve the documentation, and contribute code.</p>"},{"location":"contributing/#bug-reports-documentation-changes-and-feature-requests","title":"Bug reports, documentation changes, and feature requests","text":"<p>If you would like to contribute to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list.</p> <p>Before opening a new issue, please check the existing list to make sure a similar or duplicate item does not already exist.  When you create your issues, please be as explicit as possible and be sure to include the following:</p> <ul> <li> <p>Bug reports</p> <ul> <li>Specific project version</li> <li>Deployment environment</li> <li>A minimal, but complete, setup of steps to recreate the problem</li> </ul> </li> <li> <p>Documentation changes</p> <ul> <li>URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation)</li> <li>Updates required to correct current inconsistency</li> <li>If possible, a link to a project fork, sample, or workflow to expose the gap in documentation.</li> </ul> </li> <li> <p>Feature requests</p> <ul> <li>Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created.</li> <li>A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap.</li> </ul> </li> </ul> <p>The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.  When creating the GitHub issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project).  These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.</p>"},{"location":"contributing/#code-contributions","title":"Code contributions","text":"<p>We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below.  If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only.  These are a few projects that help on-board new contributors to the overall process.</p>"},{"location":"contributing/#coding-and-pull-requests-best-practices","title":"Coding and Pull Requests best practices","text":"<ul> <li> <p>Please ensure you follow the coding standard and code formatting used throughout the existing code base.</p> <ul> <li>This may vary project by project, but any specific diversion from normal language standards will be explicitly noted.</li> </ul> </li> <li> <p>One feature / bug fix / documentation update per pull request</p> <ul> <li>Always pull the latest changes from upstream and rebase before creating any pull request.  </li> <li>New pull requests should be created against the <code>integration</code> branch of the repository, if available.</li> <li>This ensures new code is included in full-stack integration tests before being merged into the <code>main</code> branch</li> </ul> </li> <li> <p>All new features must be accompanied by associated tests.</p> <ul> <li>Make sure all tests pass locally before submitting a pull request.</li> <li>Include tests with every feature enhancement, improve tests with every bug fix</li> </ul> </li> </ul>"},{"location":"contributing/#github-and-git-flow","title":"Github and git flow","text":"<p>The internet is littered with guides and information on how to use and understand git.</p> <p>However, here's a compact guide that follows the suggested workflow that we try to follow:</p> <ol> <li> <p>Fork the desired repo in github.</p> </li> <li> <p>Clone your repo to your local computer.</p> </li> <li> <p>Add the upstream repository</p> <p>Note: Guide for step 1-3 here: forking a repo</p> </li> <li> <p>Create new development branch off the targeted upstream branch.  This will often be <code>main</code>.</p> <pre><code>git checkout -b &lt;my-feature-branch&gt; main\n</code></pre> </li> <li> <p>Do your work:</p> </li> <li> <p>Write your code</p> </li> <li>Write your tests</li> <li>Pass your tests locally</li> <li>Commit your intermediate changes as you go and as appropriate</li> <li> <p>Repeat until satisfied</p> </li> <li> <p>Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature).</p> <pre><code>git fetch upstream\n</code></pre> </li> <li> <p>Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict.</p> <pre><code>git branch --set-upstream-to=upstream/main\ngit rebase\n</code></pre> <p>Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI.</p> </li> <li> <p>Push the changes to your repository</p> <pre><code>git push origin &lt;my-feature-branch&gt;\n</code></pre> </li> <li> <p>Create a pull request against the same targeted upstream branch.</p> <p>Creating a pull request</p> </li> </ol> <p>Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronize your remote and local forked github repository <code>main</code> branch with the upstream main branch. To do so:</p> <ol> <li> <p>Pull to your local forked repository the latest changes upstream (that is, the pull request).</p> <pre><code>git pull upstream main\n</code></pre> </li> <li> <p>Push those latest upstream changes pulled locally to your remote forked repository.</p> <pre><code>git push origin main\n</code></pre> </li> </ol>"},{"location":"contributing/#what-happens-next","title":"What happens next?","text":"<ul> <li>All pull requests will be automatically built with GitHub Action, when implemented by that specific project.</li> <li>You can determine if a given project is enabled for GitHub Action workflow by the existence of a <code>./github/workflow</code> folder in the root of the repository or branch.</li> <li>When in use, unit tests must pass completely before any further review or discussion takes place.</li> <li>The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch.</li> <li>Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch.</li> <li>Commits passing this stage will make it into the next release cycle for the given project.</li> </ul>"},{"location":"contributing/#environment-set-up","title":"Environment set up","text":"<p>We are using uv as a new Python package manager. See uv installation documentation then follow the next steps to set your environment for development:</p> <ul> <li>Clone this repository: </li> </ul> <pre><code>git clone  https://github.com/jbcodeforce/flink-studies.git\ncd flink-studies\n</code></pre> <ul> <li>Create a new virtual environment in any folder, but could be the : <code>uv venv</code></li> <li>Activate the environment:</li> </ul> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"architecture/","title":"Flink architecture","text":"Update <ul> <li>Created 2018 </li> <li>Updated 11/2024 - review done.</li> <li>12/2024: move fault tolerance in this chapter</li> </ul> <p>Once Flink is started (for example with the docker image), Flink Dashboard http://localhost:8081/#/overview presents the execution reporting:</p> Flink User Interface <p>The execution is from one of the training examples, the number of task slot was set to 4, and one job is running.</p> <p>Spark Streaming is using microbatching which is not a true real-time processing while Flink is a RT engine. Both Flink and Spark support batch processing. </p>"},{"location":"architecture/#network-stack","title":"Network Stack","text":"<p>The Flink network stack helps connecting work units across TaskManagers using Netty. Flink uses a credit-based flow control for managing buffer availability and preventing backpressure.</p> <p>See the Deep dive article in Flink network stack</p>"},{"location":"architecture/agentic_flink/","title":"Agentic Applications Cross Systems","text":"Version <p>Create 07/2025 Update - 02/09/26</p> <p>AI agentic applications, at scale will not only be triggered by users, but by systems using asynchronous events. It is assumed that AI Agents are becoming experts to certain tasks within a business workflow using domain-specific knowledge, and acts on direct user's queries or from events coming from other systems.</p> <p>As humans collaborate in a business function and process, AI Agents will collaborate with AI Agents, other systems and humans.</p> <p>The vision is to have a fleet of streaming agents\u2014background \"teammates\" that constantly monitor data to:</p> <ul> <li>Optimize costs and drive revenue.</li> <li>Prevent outages and failures.</li> <li>Operate with varying levels of autonomy (including human-in-the-loop).</li> </ul> <p>As part of the Agentic architecture, there is the planning phase of an agent, which has to use up-to-date data to define the best future actions. With autonomous agents, once they assess the business events, and plan, then cab act on external systems, and emit othe business events for others to consume.</p> <p>AI Agents may predict maintenance needs, adjust operational parameters to prevent downtime, and ensure that energy production meets demand without excess waste. In healthcare, AI Agents may analyze genetic data, medical histories, and real-time responses to various treatments.</p> <p>The Flink's event capabilities for real-time distributed event processing, state management and exact-once consistency, make it well-suited as a framework for building such event-triggered agents. This is where developers build real-time context.</p> <p>Currently, agent frameworks contain some major inhibitors: data preparation and pipeline to build embedding and process semi-structured an unstructured data. Agents shouldn't just query a single database; they should participate in an ecosystem where they consume events, perform logic, and republish results for other agents to use. We are entering the aget of Event-driven agents.</p>"},{"location":"architecture/agentic_flink/#needs","title":"Needs","text":"<ul> <li>One of the most practical insights is the cost-efficiency model. Pushing every business event through an LLM is prohibitively expensive. It is better fit to use traditional machine learning (for anomaly detection, fraud, or forecasting) as a first-pass filter.</li> <li>Deliver fresh data from the transactional systems to the AI Agents: stale data leads to irrelevant recommendations and decisions. This is only used during inference and in the LLM conversation context enrichment.</li> <li>Standard models are trained on historical, public data; they don't understand a company\u2019s specific business or what is happening right now.</li> <li>Model fine tuning needs clean data that is prepared by Data engineers using traditional SQL query, and python code to build relevant AI features. The data are at rest.</li> <li>Search (full text search, vector search and graph search) is used to enrich the LLM context. But point-in-time lookups need to be supported with temporal windows and temporal joins...</li> <li>RAG systems must adapt to information/source changes in real-time.</li> <li>Real-time data processing may need scoring computed by AI model, remotely accessed.</li> <li>Adopt an event-driven architecture for AI Agents integration and data exchanges using messaging: the orchestration is not hard coded into a work flow (Apache airflow) but more event oriented and AI agent consumers act on those events.</li> <li>Event processing can trigger AI agents to act to address customer's inqueries. The MCP protocol can also be used to call external tools.</li> <li>Embeded AI in drones to adapt to environment conditions and task requirements.</li> </ul> Decision Intelligent Platforms <p>Gartner defines decision intelligence platforms (DIPs) as software to create decision-centric solutions that support, augment and automate decision making of humans or machines, powered by the composition of data, analytics, knowledge and AI.</p>"},{"location":"architecture/agentic_flink/#challenges","title":"Challenges","text":"<ul> <li>Not all data is needed in real time, and \u201creal-time\u201d is relative</li> <li>Data generated isn't delivered to AI systems fast enough</li> <li>Current Agentic SDK or framework are not event-driven, lack replayability and production readiness</li> <li>Missing fresh representation of live operational events</li> <li>Currently there is a clear separation between data processing and AI inference</li> </ul>"},{"location":"architecture/agentic_flink/#understanding-real-time-data-needs","title":"Understanding real-time data needs","text":"<p>One of the key aspects of adopting real-time middleware, like Kafka and real-time processing with Flink, is to assess what real-time means for each ML features needed for an agent.</p> <p>Try to assess the following requirements:</p> <ul> <li>Analytics data needed by end-user process for feature engineering and inference use cases</li> <li>Event-driven architectures to automate operational tasks as a response to events, like anomaly detection</li> <li>Next best action to take in real-time</li> <li>Exactly-once semantic or at least once</li> <li>Assess which business processes need real-time data? What processes could become more proactive?</li> <li>What batch processing will perform better with real-time data?</li> <li>Does AI Agents need real-time stream to take decision?</li> </ul>"},{"location":"architecture/agentic_flink/#event-driven-ai-agent","title":"Event-driven AI Agent","text":"<p>Extending the Agentic reference architecture, introduced by Lilian Weng, which defines how agents should be designed, it is important to leverage the experience acquired during microservice implementations to start adopting an event-driven AI agent architecture, which may be represented by the following high-level figure:</p> <p>Because it\u2019s built on Kafka, developers can \"replay\" production data streams to test why a probabilistic AI model gave a certain answer\u2014making, debugging much easier than in traditional batch systems.</p>"},{"location":"architecture/agentic_flink/#technologies","title":"Technologies","text":""},{"location":"architecture/agentic_flink/#apache-flink-agents","title":"Apache Flink-Agents","text":"<p>Apache Flink Agent is an open-source framework for building event-driven streaming agents.</p> <p>There will be multiple patterns supported by Flink. </p> <ol> <li>Workflow streaming agents: a directed workflow of modular steps, called actions, connected by events. To orchestrate complex, multi-stage tasks in a transparent, extensible. This is an implementation of the SAGA choreography pattern. See the appache Flink Agent git repo and python example of workflow</li> <li>ReAct: to combine reasoning and action, where the user's prompt specify the goal, then agent, with tools and LLM, decides how to achieve the goal. A steaming ReAct agent example in python demonstrates how to do this with Flink Agents. See my own implementation with Kafka and OSS Flink</li> </ol>"},{"location":"architecture/agentic_flink/#confluent","title":"Confluent","text":"<p>Confluent builds its AI strategy on three functional layers:</p> <ul> <li>Real-Time Processing (Flink Streaming Agents): Using Kafka and Flink to process data from disparate systems in real-time, built on open-source standards to avoid vendor lock-in.</li> <li>Interoperability (MCP Protocol): Leveraging the Model Context Protocol (MCP) as a universal language, allowing Confluent to serve real-time context to any AI agent or tool regardless of the provider.</li> <li> <p>Governance and Replayability: Ensuring data is secure and auditable. A key advantage is Kafka\u2019s \"replayability,\" which allows developers to re-run data streams for debugging, auditing, and refining AI responses.</p> </li> <li> <p>Flink paired with Kafka is purpose-built for real-time data processing and event-driven microservices. Confluent AI with Flink SQL helps decouple AI agents.</p> </li> <li>Agent communication protocols are available to define agent interations: Agent to Agent from Google and ACP from IBM/ Linux foundations</li> <li>Agent integrate with IT applications and services via Model Context Protocol from Anthropic</li> </ul>"},{"location":"architecture/agentic_flink/#quick-demonstration","title":"Quick demonstration","text":"<p>TO BE CONTINUED</p> <ul> <li>CREATE MODEL: Register a remote AI model (e.g., OpenAI, AWS Bedrock) using Confluent Console or CLI.</li> <li>CREATE TOOL: Encapsulate local Flink UDFs or external MCP server connections into reusable tool resources</li> </ul> <pre><code>CREATE AGENT my_agent \nUSING MODEL my_model \nUSING TOOLS my_tool \nUSING PROMPT 'You are a helpful assistant ...' WITH ( 'max_iterations' = '10' , 'request_timeout' = '60', 'handle_exception' = 'continue' , 'max_consecutive_failures' = '3' , 'max_iterations' = '15' , 'max_tokens_threshold' = '100000' 'request_timeout' = '300' , 'summarization_prompt' = 'concise' , 'tokens_management_strategy' = 'summarize' );\n</code></pre>"},{"location":"architecture/agentic_flink/#confluent-cloud-mcp-server","title":"Confluent Cloud - MCP Server","text":"<p>The Confluent git repository: mcp-confluent enables AI assistants to interact with Confluent Cloud REST APIs. The following figure illustrates the flow, where a human can interact with the Confluent Cloud platform in natural language, create Flink SQL statements that are persisted in Git Repositiory, and deployed using CI/CD pipelines.</p> <p>The server provides 40+ tools organized by Confluent service category.</p> Service Category Tools Kafka Operations list-topics, create-topics, delete-topics, produce-message, consume-messages, alter-topic-config, get-topic-config Flink SQL list-flink-statements, create-flink-statement, read-flink-statement, delete-flink-statements Schema Registry list-schemas, search-topics-by-tag Kafka Connect list-connectors, create-connector, read-connector, delete-connector Tableflow create-tableflow-topic, list-tableflow-topics, read-tableflow-topic, update-tableflow-topic, delete-tableflow-topic, catalog integration tools Platform Management list-environments, read-environment, list-clusters"},{"location":"architecture/agentic_flink/#example-of-natural-language-queries","title":"Example of natural language queries","text":"<ul> <li> <p>Resource based queries     <pre><code>what are the topics in my kafka cluster?\n</code></pre></p> </li> <li> <p>Develop a Flink Statement:     <pre><code>looking at the tx_items topic and schema definitions, I want to implement a flink sql to do deduplication of records from this CDC raw topic \n</code></pre></p> </li> </ul>"},{"location":"architecture/agentic_flink/#project-discovery-questions","title":"Project Discovery Questions","text":"<ul> <li>What are the current GenAI goals and projects?</li> <li>How data are delivered as context to agents?</li> <li>How actions are performed once agent helps defining the next best action?</li> <li>What guardrails are in place to avoid dangerous side effect?</li> </ul>"},{"location":"architecture/agentic_flink/#sources","title":"Sources","text":"<ul> <li>Anomaly detection sample in this folder</li> <li>Confluent Cloud Flink SQL constructs: A set of functions to call an AI or ML model remotely from SQL queries.</li> <li>Confluent AI Workshop git repo: This demo showcases an intelligent, real-time fleet management system that autonomously detects demand surges, identifies their causes using AI-powered reasoning, and automatically dispatches vessels to meet increased demand.</li> <li></li> </ul>"},{"location":"architecture/cookbook/","title":"Flink Cookbook","text":"Chapter updates <ul> <li>Created 12/2024 </li> <li>Updated 1/28/2025: env, testing and statement life cycle.</li> </ul> <p>There is a Confluent cookbook for best practices to run Flink into production. The content of this page is to get a summary of those practices, enhanced from other customers' engagements. It also references hands-on exercises within this repository. </p> <p>Examples may be run inside from Terminal or using Kubernetes cluster locally, they are on Flink 1.20.1 or Flink 2.1. Use Java 11 or 17, see sdkman to manage different java version. </p>"},{"location":"architecture/cookbook/#understand-the-flink-ui","title":"Understand the Flink UI","text":"<p>The Flink Web UI helps to debug misbehaving jobs. </p> <p>The Flink Web UI is  well described in Confluent David Anderson's article, The Apache Flink doc for Web UI, and link to the important execution plan understanding with EXPLAIN.</p> <p>With OSS the Web UI is accessible when the <code>start_cluster.sh</code> is started. Local URL is http://localhost:8081. The Web UI offers the following important features:</p> <ul> <li>Navigating to get the running Jobs, the view is updated periodically. The job graph, which matches the EXPLAIN output, presents the tasks running one or more operators of the DAG.</li> <li>Task metrics are backpressure, busyness, and data skew.<ul> <li>backpressure: percentage of time that the subtask was unable to send output downstream because the downstream subtask had fallen behind, and (temporarily) couldn't receive any more records. <code>Backpressured max</code> is the maximum backpressure across all of the parallel subtasks for a given period.</li> <li>busy reports percentage of time spent doing useful work, aggregated at the task level for a time period.</li> <li>data skew measures the degree of variation in the number of records processed per second by each of the parallel subtasks. 100% is max skew.</li> </ul> </li> <li>Examining the history of checkpoints</li> <li>Monitoring for any potential backpressure</li> <li>Analyzing watermarks</li> <li>Retrieving the job logs</li> </ul> <p>Network metrics (Bytes Received / Records Received ) are inside the Flink cluster, not for source and sink to external systems.</p> <p>In Concluent Cloud the Query Profiler has the same capability then the Flink UI and accessible at the Statement View level: </p>"},{"location":"architecture/cookbook/#classical-deployment-pattern","title":"Classical deployment pattern","text":"<p>For Confluent Cloud for Flink we want to map environments to physical environments, like dev, Staging and Production. Any stream processing has a set of source producer that will be part of an ingestion layer. In the Kafka architecture, it will be a set of Kafka Connector cluster, with for example Change Data Capture connector like Debezium. Most of the time the dev environment may not have the streams coming from this ingestion layer. </p> <p>For the discussion each business application can have one to many pipelines. A Pipeline is a set of Flink jobs running SQL statements or Table API programs. A generic pattern of a pipeline involves at least, the following steps:</p> Figure 1: Generic pipeline structure <ol> <li>A CDC source connector injects data in Kafka topic. Avro schemas are defined for the Key and the Value.</li> <li>A first set of statements are doing deduplication logic, or filtering to ensure only relevant messages are processed by the pipeline</li> <li>There will be zero to many intermediate tables, depending of the business logic needed for the application. Those intermediate tables/topics may get enrichement, aggregation or joins.</li> <li>The final step is to prepare the data for sink processing. The statements may includes joins and filtering out, even may be some deduplication logic too.</li> <li>The data need to land to sink external system, so Kafka Sink connectors are deployed to write to those systems. Here the example illustrate a datawarehouse system based on Postgreql, on which a business intelligent component will implement adhoc queries and dashboards. </li> </ol> <p>The artifacts for development are the DDL and DML statements and test data.</p> <p>Finally to support the deployment and quality control of those pipelines deployment, the following figures illustrates a classical deployment pattern:</p> Figure 2: Environment mapping <ol> <li>Each environment has its own schema registry</li> <li>Once Kafka Cluster per env, with different ACL rules to control who can create topic, read and write.</li> <li>For each application it may be relevant to isolate them in their own Flink Compute pool</li> <li>CI/CD can define infrastructure as code for the Flink Compute pool, the Kafka Cluster, the Kafka Connector cluster and connectors configuration, the input topics, the ACLs, the schema registry.</li> </ol> <p>This architecture helps to clearly separate schema management per environment, and help to promote real-time processing pipelines from dev to staging to production in a control manner using a GitOps approach.</p> Gitops <p>The core concept of GitOps is to maintain a single Git repository that consistently holds declarative descriptions of the desired infrastructure in the production environment. An automated process ensures that the production environment aligns with the state described in the repository. The methodology and tools support changing infrastructure using feature branches, PR, PR review, </p>"},{"location":"architecture/cookbook/#exactly-once-delivery","title":"Exactly-once-delivery","text":"<p>Flink's internal exactly-once guarantee is robust, but for the results to be accurate in the external system, that system (the sink) must cooperate.</p> <p>This is a complex subject to address and context is important on how to assess exactly-once-delivery: within Flink processing, versus with an end-to-end solution context. </p>"},{"location":"architecture/cookbook/#flink-context","title":"Flink context","text":"<p>For Flink, \"Each incoming event affects the final Flink statement results exactly once.\" as said Piotr Nowojski during his presentation at the Flink Forward 2017 conference. No data duplication and no data loss. Flink achieves it through a combination of checkpointing, state management, and transactional sinks. Checkpoints save the state of the stream processing application at regular intervals. State management maintains the consistency of data between the checkpoints. Transactional sinks ensure that data gets written out exactly once, even during failures </p> <p>Flink uses transactions when writing messages into Kafka. Kafka messages are only visible when the transaction is actually committed as part of a Flink checkpoint. <code>read_committed</code> consumers will only get the committed messages. <code>read_uncommitted</code> consumers see all messages.</p> <p>Below is an example of creating Flink Table in Confluent Cloud with reading committed only message (opposite property will be: 'kafka.consumer.isolation-level'='read-uncommitted'):</p> <pre><code>CREATE TABLE exactly_once\n  WITH('kafka.consumer.isolation-level'='read-committed')\n  AS SELECT * FROM `transactions`..\n</code></pre> <p>As the default checkpoint interval is set to 60 seconds, <code>read_committed</code> consumers will see up to one minute latency: a Kafka message sent just before the commit will have few second latency, while older messages will be above 60 seconds.</p> <p>When multiple Flink statements are chained in a pipeline, the latency may be even bigger, as Flink Kafka source connector uses <code>read_committed</code> isolation.</p> <p>The checkpoints frequency can be updated but could not go below 10s. Shorter interval improves fault tolerance, but adds persistence and performance overhead.</p>"},{"location":"architecture/cookbook/#end-to-end-solution","title":"End-to-end solution","text":"<p>On the sink side, Flink has a 2 phase commit sink function on specific data sources, which includes Kafka, message queue and JDBC. </p> <p>For stream processing requiring an upsert capability (insert new records or update existing ones based on a key), the approach is to assess:</p> <ul> <li>if the sink kafka connector support upsert operations: it emits only the latest state for each key, and a tombstone message for delete (which is crucial for Kafka's log compaction to work correctly).</li> <li>For Datsabase, be sure to use a JDBC connector, with upsert support. Achieving exactly-once to a traditional database is done by leveraging the sink's implementation of Flink's Two-Phase Commit protocol. The database's transactions must be compatible with this to make sure writes are only committed when a Flink checkpoint successfully completes.</li> <li>For Lakehouse Sink</li> </ul> <p>The external system must provide support for transactions that integrates with a two-phase commit protocol. </p> <p>When using transactions on sink side, there is a pre-commit phase which starts from the checkpointing: the Job Manager injects a checkpoint barrier to seperate streamed in records before or after the barrier. As the barrier flows to the operators, each one of them, takes a snapshot or their state. The sink operators that support transactions, need to start the transaction in the precommit phase while saving its state to the state backend. After a successful pre-commit phase, the commit must guarantee the success for all operators. In case of any failure, the tx is aborted and rolled back.</p> <ul> <li>Article: An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!).</li> <li>Confluent documentation.</li> <li>Confluent Platform - Kafka consumer isolation level property.</li> </ul>"},{"location":"architecture/cookbook/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/cookbook/#a-sql-statement-not-returning-any-result","title":"A SQL statement not returning any result","text":"<p>This could be linked to multiple reasons so verify the following:</p> <ul> <li>Verify there is no exception in the statement itself</li> <li>Query logic being too restrictive or the joins may not match any records. </li> <li>For aggregation, assess if the field used get null values.</li> <li>Source table may be empty, or it consumes the table from a different starting offset (specified via <code>scan.bounded.mode</code>) then expected.</li> <li>Use <code>show create table &lt;table_name&gt;</code> to assess the starting offset strategy or specific values</li> <li>Count all records in a table using <code>SELECT COUNT(*) FROM table_name;</code>, it should be greater then 0.</li> <li>When the statement uses event-time based operation like <code>windowing, top N, OVER, MATCH_RECOGNIZE</code> and temporal joins then verify the watermarks. The following example is from Confluent Cloud for Flink query using the event time from the record, and it should return result. Check if you have produced a minimum of records per Kafka partition, or if the producer has stopped producing data all together.</li> </ul> <pre><code>SELECT ROW_NUMBER() OVER (ORDER BY $rowtime ASC) AS number, *   FROM &lt;table_name&gt;\n</code></pre> <ul> <li>When Data are in topic but not seen by flink <code>select * from &lt;table_name&gt;</code> statement, it may be due to idle partitions and the way watermarks advance and are propagated. Flink automatically marks a Kafka partition as idle if no events come within <code>sql.tables.scan.idle-timeout</code> duration. When a partition is marked as idle, it does not contribute to the watermark calculation until a new event arrives. Try to set the idle timeout for table scans to ensure that Flink considers partitions idle after a certain period of inactivity. Try to create a table with a watermark definition to handle idle partitions and ensure that watermarks advance correctly.</li> </ul>"},{"location":"architecture/cookbook/#high-availability-and-disaster-recovery","title":"High Availability and Disaster Recovery","text":"<p>In Flink high availability goal is to keep the application running and being able to process data. The focus is more on streaming applications then batch. JobManager is a single point of failure but can be configured with standby JobManagers. The coordination can be done with Zookeeper for self managed deployment or via Kubernetes operator.</p> <ul> <li>For Confluent Cloud for Flink see the DR section.</li> </ul>"},{"location":"architecture/cookbook/#security","title":"Security","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#deduplication","title":"Deduplication","text":"<p>Deduplication is documented here and here and at its core principal, it uses a CTE to add a row number, as a unique sequential number to each row. The columns used to de-duplicate are defined in the partitioning. Ordering is using a timestamp to keep the last record or first record. Flink support only ordering on time.</p> <pre><code>SELECT [column_list]\nFROM (\n   SELECT [column_list],\n     ROW_NUMBER() OVER ([PARTITION BY column1[, column2...]]\n       ORDER BY time_attr [asc|desc]) AS rownum\n   FROM table_name\n) WHERE rownum = 1\n</code></pre> <p>When using Kafka Topic to persist Flink table, it is possible to use the <code>upsert</code> or <code>retract</code> change log mode, and define the primary key(s) to remove duplicate records as only the last records will be used:</p> <pre><code>CREATE TABLE orders_deduped (\n  PRIMARY KEY( order_id, member_id) NOT ENFORCED) DISTRIBUTED BY (order_id, member_id) INTO 1 BUCKETS \nWITH (\n  'changelog.mode' = 'upsert',\n  'value.fields-include' = 'all'\n) AS\nSELECT\n  *\nFROM (\n  SELECT\n      *,\n      ROW_NUMBER() OVER (\n        PARTITION BY `order_id`, `member_id`\n        ORDER\n          BY $rowtime DESC\n      ) AS row_num\n    FROM orders_raw\n) WHERE row_num = 1;\n</code></pre> <p>To validate there is no duplicate records in the output table, use a query with tumble window like:</p> <pre><code>  SELECT\n      `order_id`, \n      `user_id`,\n    COUNT(*) AS cnt\n    FROM\n    TABLE(\n        tumble(\n        TABLE orders_raw,\n        DESCRIPTOR($rowtime),\n        INTERVAL '1' MINUTE\n        )\n    )\n    GROUP\n    BY  `order_id`, `user_id` HAVING COUNT(*) &gt; 1;\n</code></pre> <p>Duplicates may still occur on the Sink side of the pipeline, as it is linked to the type of connector used and its configuration, for example reading un-committed offset. Few connector support the retract semantic.</p>"},{"location":"architecture/cookbook/#change-data-capture","title":"Change Data Capture","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#late-data","title":"Late Data","text":"<p>TO BE DONE</p>"},{"location":"architecture/cookbook/#exactly-once","title":"Exactly once","text":""},{"location":"architecture/cookbook/#query-evolution","title":"Query Evolution","text":"<p>In this section, I address streaming architecture only, integrated with Kafka, most likely CDC tables and sink connectors, a classical architecture, simplified in this figure:</p> <p>Once a Flink query is deployed and run 'forever', how to change it? to fix issue or adapt to schema changes. </p> <p>By principles any Flink DAG code is immutable, so statement needs to be stopped and a new version started! This is not as simple as this, as there will be impact to any consumers of the created data, specially in Kafka topic or in non-idempotent consumers. </p> <p>The Flink SQL statements has limited parts that are mutables. See the Confluent Cloud product documentation for details.  In Confluent Cloud the principal name and compute pool metadata are mutable when stopping and resuming the statement. Developers may stop and resume a statement using Console, CLI, API or even Terraform scripts.</p> <p>Here are example using confluent cli:</p> <pre><code># $1 is the statement name\nconfluent flink statement stop $1 --cloud $(CLOUD) --region $(REGION) \n\nconfluent flink statement resume $1 --cloud $(CLOUD) --region $(REGION) \n</code></pre> <p>When a SQL statement is started, it reads the source tables from the beginning (or any specified offset) and the operators, defined in the statement, build their internal state. </p> <p>Source operators use the latest schema version for key and value at the time of deployment. There is a snapshot of the different dependency configuration saved for the statement: the reference to the dependants tables, user-defined functions... Any modifications to these objects are not propagated to running statement, which means that:</p> <ul> <li>A change to the schema of the source topic is not picked up by the running statement that references the topic.</li> <li>Same applies to other objects in the catalog like watermark, UDF etc.</li> </ul> <p>First let review the schema definition evolution best practices for Flink processing.</p>"},{"location":"architecture/cookbook/#schema-compatibility","title":"Schema compatibility","text":"<p>Flink works best when consuming topics with FULL_TRANSITIVE compatibility mode, which allows addition or deletion of fields with default values only. Fields added are ignored by the running Flink Statement</p> <p>The following table lists the schema compatibility types, with the Flink impacts:</p> Compatibility type Change allowed Flink impact BACKWARD Delete fields, add optional field Does not allow to replay from earliest BACKWARD_TRANSITIVE Delete fields, add optional fields with default value. Require all Statements reading from impacted topic to be updated prior to the schema change. FORWARD Add fields, delete optional fields Does not allow to replay from earliest FORWARD_TRANSITIVE Add fields, delete optional fields Does not allow to replay from earliest FULL Add optional fields, delete optional fields Does not allow to replay from earliest FULL_TRANSITIVE Add optional fields, delete optional fields Reprocessing and bootstrapping is always possible: Statements do not need to be updated prior to compatible schema changes. Compatibility rules &amp; migration rules can be used for incompatible changes. NONE All changes accepted Very risky. Does not allow to replay from earliest <p>With FULL_TRANSITIVE, old data can be read with the new schema, and new data can also be read between the schemas X, X-1, and X-2. Which means in a pipeline, downstream Flink statements can read newly created schema, and will be able to replay messages from a previous schemas. Therefore, you can upgrade the producers and consumers independently.</p> <p>Confluent Schema Registry supports Avro, Json or Protobuf, but Avro was designed to support schema evolution, so this is the preferred serialization mechanism to use.</p>"},{"location":"architecture/cookbook/#handling-changes-with-cdc","title":"Handling Changes with CDC","text":"<p>The CDC component may create schema automatically reflecting the source table. See Debezium documentation about schema evolution.</p> <p>Developers may only delete optional columns. During the migration, all current source tables coming as outcome of the Debezium process are not deletable. If a NOT NULL constraint is added to a column, older records with no value will violate the constraint. To handle such situation, there is a way to configure CDC connector (Debezium) to use the <code>envelop structure</code> which uses <code>key, before, after, _ts_ms, op</code> fields. This will result in a standard schema for Flink tables, which allows the source table to evolve by adding / dropping both NULL &amp; NON NULL COLUMNS. </p> <p>The Debezium Envelope pattern offers the most flexibility for upstream Schema Evolution without impacting downstream Flink Statements.</p> <ul> <li>Adding Columns do not break the connectors or the RUNNING DML Statements. But if developers need to access new field, the JSON_* built-in functions may be used, so a new Flink statement version is needed. </li> </ul> Key schema evolution <p>It is discouraged from any changes to the key schema in order to do not adversely affect partitioning for the new topic/ table.</p> Updating schema in schema registry <p>Adding a field directly in the avro-schema with default value, will be visible in the next command like: <code>show create table &lt;table_name&gt;</code>, as tables in flink are virtuals, and the Confluent Cloud schema definition comes from the Schema Registry. The RUNNING Flink DML is not aware of the new added column. Even STOP &amp; RESUME of the Flink DML is not going to pick the new column neither. </p> <p>Only new deployed statement will see the new schema. For column drop, Debezium connector will drop the new column and register a schema version for the topic (if the alteration resulted in a schema that doesnt match with previous versions). Same as above runnning statements will go into <code>degraded</code> mode.</p> Migrate a NULL column to NOT NULL with retro update of rows with a default value <p>Most likely such operation will result in a connector failure as you are adding a NOT NULL column without a default value, even if you retro updated all the rows. </p> <p>If you have control of the change do the following <pre><code>UPDATE Customers set zipcode=0;\nALTER TABLE Customers alter column zipcode INT NOT NULL CONSTRAINT add_zip DEFAULT 0;\n</code></pre></p> <p>If the upstream has already updated without default value, change the compatibility mode to NONE for the connector to recover and then: <pre><code>ALTER TABLE Customers ADD CONSTRAINT add_zip DEFAULT 0;\n</code></pre></p> Drop a NULL column <p><pre><code>ALTER TABLE Customers DROP CONSTRAINT add_zip;\nALTER TABLE Customers DROP COLUMN zipcode\n</code></pre> Running Flink DML is not aware of the dropped <code>zipcode</code> column. User may STOP &amp; RESUME the Flink DML with no impact. But the statement will go to DEGRADED mode if the dropped column is part of the SELECT statement. For new deployment, user may update the statement to remove the dropped column and start from the beginning or specific offsets.</p>"},{"location":"architecture/cookbook/#statement-evolution","title":"Statement evolution","text":"<p>The general strategy for query evolution is to replace the existing statement and the corresponding tables it maintains with a new statement and new tables. The process is described in this product chapter and should be viewed within two folds depending of stateless or statefulness of the statement. The initial state of the process involves a pipeline of Flink SQL statements and consumers processing Kafka records from various topics. We assume that the blue records are processed end-to-end, and the upgrade process begins at a specific point, from where all green records to be processed. The migration should start with the Flink statement that needs modification and proceed step by step to the statement creating the sink.</p> <p>For stateless statement evolution the figure below illustrates the process:</p> <p></p> <p>Figure: Stateless schema evolution process</p> <ol> <li>The blue statement, version 1, is stopped, and from the output, developer gets the last offset</li> <li>The green statement includes a DDL to create the new table with schema v2.</li> <li>The green DML statement, version 2, is started from the last offset or timestamp and produces records to the new table/topic</li> <li>Consumers are stopped. They have produced so far records to their own output topic with source from blue records.</li> <li>Consumers restarted with the new table name. Which means changing their own SQL statements, but now producing output with green records as source.</li> </ol> <p>At a high level, stateless statements can be updated by stopping the old statement, creating a new one, and transferring the offsets from the old statement. As mentioned, we need to support FULL TRANSITIVE updates to add or delete optional columns/fields.</p> <p>For stateful statements, it is necessary to bootstrap from history, which the below process accomplishes:</p> <p></p> <p>Figure: Stateful schema evolution process</p> <p>The migration process consists of the following steps:</p> <ol> <li>Create a DDL statement to define the new schema for <code>table_v2</code> which means topic v2.</li> <li>Deploy the new statement with the v2 name, starting from the earliest records to ensure semantic consistency for the blue records, when they are stateful, or from the last offset when stateless.</li> <li>Once the new statement is running, it will build its own state and continue processing new records. Wait for it to retrieve the latest messages from the source tables before migrating existing consumers to the new table v2. The old blue records will be re-emitted. While the new statement </li> <li>Stop processing first statement.</li> <li>Halt any downstream consumers, retaining their offsets if those are stateless or idempotent, if they are stateful they will process from the earliest.</li> <li>Reconnect the consumers to the new table. For Flink statements acting as consumers, they will need to manage their own upgrades. To avoid duplicates or not missing records, the offset for the consumers to the new topic needs ot be carrefuly selected.</li> <li>Once all consumers have migrated to the new output topic, the original statement and output topic can be deprovisioned.</li> </ol> <p>This process requires to centrally control the deployment of those pipeline.</p> <p>Using Open Source Flink, creating a snapshot is one potential solution for restarting. However, if the DML logic has changed, it may not be possible to rebuild the DAG and the state for a significantly altered flow. Thus, in a managed service, the approach is to avoid using snapshots to restart the statements.</p> <p>Also when the state size is too large, consider separating the new statement into a different compute pool than the older one.</p>"},{"location":"architecture/cookbook/#restart-a-statement-from-a-specific-time-or-offset","title":"Restart a statement from a specific time or offset","text":"<p>Using time window, it may be relevant to restart from the beginning of the time window when the job was terminated. Which means using a <code>WHERE event_time &gt; window_start_time_stamp</code> to get the records from the source table for an aligned time.</p> <p>Use the <code>/*+ options</code> at the table level or a <code>set statement</code> for all the table to read from.</p> <pre><code>FROM orders /*+ OPTIONS('scan.startup.mode' = 'timestamp', 'scan.startup.timestamp-millis' = '1717763226336') */\n</code></pre> <p>For offset, the <code>status.latest_offsets</code> includes the lastest offset read for each partition. Note that reading from an offset is applicable only for stateless statements to ensure exactly-once delivery. or</p> <pre><code>SET `sql.tables.scan.startup.mode`= \"earliest\"\n</code></pre>"},{"location":"architecture/cookbook/#materialized-table","title":"Materialized Table","text":"<p>FLIP 435 presents a new table construct to simplify streaming and batch pipelines. Users can define a data transformation with a single declarative</p> <pre><code>CREATE MATERIALIZED TABLE ... AS SELECT ... \n</code></pre> <p>statement and specify a desired FRESHNESS interval. Flink then automatically creates and manages the underlying data refresh pipeline, intelligently choosing between a continuous streaming job for low-latency updates or a scheduled batch job for less frequent refreshes. This eliminates the need for separate codebases, manual job orchestration, and complex parameter tuning. The entire lifecycle of the data pipeline, including pausing, resuming, manual backfills, and altering data freshness, can be managed through simple SQL commands, allowing developers to focus on business logic rather than the complexities of stream and batch execution.</p> <p>See the Flink 2.x documentation.</p> <ul> <li>It supports the FRESHNESS keyword,  to define the maximum amount of time that the materialized table\u2019s content should lag behind updates to the base tables.</li> <li>By default the CONTINUOUS mode is a 3 minutes: so the refresh pipeline is a streaming job with a checkpoint interval of 3 minutes. For FULL mode, the pipeline is a scheduled job executed every 1 hour.   <pre><code>CREATE MATERIALIZED TABLE my_materialized_table_full\n  REFRESH_MODE = FULL\n  AS SELECT * FROM source_table;\n</code></pre></li> <li>Alter materialized table allows developer to suspend and resume refresh pipeline. When suspending a table in CONTINUOUS mode, the job will be paused using STOP WITH SAVEPOINT by default.</li> <li>Altering an existing materialized table, schema evolution currently only supports adding nullable columns to the end of the original materialized table\u2019s schema.</li> <li>Materialized Tables must be created through SQL Gateway,</li> </ul> <p>TO CONTINUE</p>"},{"location":"architecture/cookbook/#change-stateful-statement","title":"Change stateful statement","text":"<p>We have seen the stateful processing leverages checkpoints and savepoints. With the open source Flink, developers need to enable checkpointing and manually triggering a savepoint when they need to restart from a specific point in time.</p> <p>Deploy the new statement to compute the stateful operation, and use a template like the following. Then stop the first statement.</p> <pre><code>create table table_v2\nas \nselect * from table_v1\nwhere window_time &lt;= a_time_stamp_when_stopped\nunion \nselect * from (tumble table)\nwhere the $rowtime &gt; a_time_stamp_when_stopped\norder by window_time\n</code></pre> <p>It is possible to do an in-place upgrade if the table uses primary key.</p> <p>Restarting a job is not a retry mechanism but a fault tolerance one. Normally only cluster level issues should ever cause a job to restart. When doing Java or Python application, developers need to do not throw exceptions within the main function but handle them and perform retries, backoff, and loop forever. as part of the exception management it is important to provide diagnostic data to administrators.</p> <p>When integrated with Kafka, networking latency may trigger losing connection, or some user errors like deleting the cluster, a topic, or a connector, may lead the Flink job getting in retry loop. </p> <p>Savepoints are manually triggered snapshots of the job state, which can be used to upgrade a job or to perform manual recovery.</p> <p>Full checkpoints and savepoints take a long time, but incremental checkpoints are faster.</p>"},{"location":"architecture/cookbook/#demonstrate-in-place-upgrade-of-stateless-statement","title":"Demonstrate in-place upgrade of stateless statement","text":""},{"location":"architecture/cookbook/#demonstrate-stateful-statement-upgrade","title":"Demonstrate stateful statement upgrade","text":""},{"location":"architecture/cookbook/#flink-on-kubernetes","title":"Flink on Kubernetes","text":"<ul> <li>To trigger a savepoints</li> <li></li> </ul>"},{"location":"architecture/cookbook/#testing-sql-statement-during-pipeline-development","title":"Testing SQL statement during pipeline development","text":"<p>We should differentiate two types of testing: Flink statement developer testing, like unit / component tests, and integration tests with other tables and with real data streams.</p> <p>The objective of a test harness for developer and system integration is to validate the quality of a new Flink SQL statement deployed on Confluent Cloud (or Flink managed service) to address at least the following needs:</p> <ol> <li>be able to deploy a flink statement (the ones we want to focus on are DMLs, or CTAS)</li> <li>be able to generate test data from schema registry - and with developer being able to tune test data for each test cases.</li> <li>produce test data to n source topics, consumes from output topic and validates expected results. All this flow being one test case. This may be automated for non-regression testing to ensure continuous quality.</li> <li>support multiple testcase definitions</li> <li>tear done topics and data.</li> </ol> <p>The following diagram illustrates the global infrastructure deployment context:</p> <p></p> <p>The following diagram illustrates the target unit testing environment:</p> <p></p> <ul> <li>The Kafka Cluster is a shared cluster with topics getting real-time streams from production</li> </ul>"},{"location":"architecture/cookbook/#measuring-latency","title":"Measuring Latency","text":"<p>The goal is to measure the end-to-end data latency, which is the time from when data is created at its source to when it becomes available for end-user consumption. In any Apache Flink solution, especially those with chained Flink jobs, measuring this latency is a critical and planned activity within the deployment process.</p> <p>For example, consider a typical real-time data processing architecture. We'll use this architecture to illustrate how to measure and represent latency throughout the data flow:</p> <p>The following timestamps may be considered:</p> <ol> <li>UpdatedDate column in the transactional database may be used to know when a record was created / updated, this will serve to measure end-to-end latency. </li> <li>Source CDC connector like Debezium, may add a timestamp in their message envelop that could be used, if injected in the messages to measure messaging latency.</li> <li>Within the Flink processing, event time, may be mapped to the Kafka Topic record time. Ths could be used to measure Flink pipeline latency.</li> </ol> <p>It is important to note that Flink latency results may seem inconsistent. This is normal, and due to the semantic of the Flink processing. To ensure exactly-once delivery, Flink uses transactions when writing messages into Kafka. It is part of a consume-process-produce pattern, and adds the offset of the messages it has consumed to the transaction. </p> <p>Flink persists its state, including the offsets of the Kafka source, via checkpoints, which are done once per minute. The frequency may be configured. Queries submitted through Confluent Cloud Flink SQL Workbench or inspecting the content of a topic in the console generate output based on <code>read_uncommitted</code> isolation. The latency may seem reasonalbe during iterative development in the console, it may increase during more rigorous tests that use a read_committed consumer.</p> <p>Consumer reading committed messages will observe latency (consumer's property of <code>isolation.level= read-committed</code>). Recall that, when a consumer starts up, or when a partition is newly assigned to it, the consumer will check the <code>__consumer_offsets</code> topic to find the latest committed offset for its consumer group and partition. It will then begin reading messages from the next offset. Consumer will wait and not advance its position until the transaction is either committed or aborted. This ensures it never sees messages from aborted transactions and only sees a complete, consistent set of records. </p> <p>When producer iniates a transaction, and writes messages to topic/partition, those messages are not yet visible to consumers configured to read committed data, when there is no error, the producer commits the transaction. </p> <p>When consumer applications may tolerate at-least once semantics, they may simply configure all consumers with <code>read_uncommitted</code> isolation, at the risk that during failure recovery and scaling activities, the Flink statement will reingest messages from the last checkpoint, causing duplicates and time-travel for end consumers.</p> Flink transaction process <p>Flink's core mechanism for fault tolerance is checkpointing. It periodically takes a consistent snapshot of the entire application state, including the offsets of the Kafka source and the state of all operators.</p> <ul> <li>Flink's Kafka sink connector uses a two-phase commit protocol.</li> <li>When a checkpoint is triggered, the Flink Kafka producer (the sink) writes any pending data to Kafka within a transaction. It then prepares to commit this transaction but waits for a signal from the Flink JobManager.</li> <li>Once the JobManager confirms that all operators have successfully snapshotted their state, it tells the Kafka producer to commit the transaction. This makes the new data visible to read_committed consumers.</li> <li>If any part of the checkpoint fails (e.g., a Flink task manager crashes), the transaction is aborted. The uncommitted messages become \"ghost\" records on the Kafka topic, invisible to read_committed consumers. When the Flink job restarts, it will restore its state from the last successful checkpoint and reprocess the data from that point, avoiding data loss or duplicate.</li> </ul> <p>The shift_left utility has an integration tests harness feature to do end-to-end testing with timestamp.</p>"},{"location":"architecture/cookbook/#other-sources","title":"Other sources","text":"<p>The current content is sourced from the following cookbooks and lesson learnt while engaging with customers.</p> <ul> <li>Confluent Flink Cookbook</li> <li>Ververica Flink cookbook</li> </ul>"},{"location":"architecture/fitforpurpose/","title":"Fit for purpose","text":""},{"location":"architecture/fitforpurpose/#difference-between-kafka-streams-and-flink","title":"Difference between Kafka Streams and Flink","text":"<ul> <li>Flink is a complete streaming computation system that supports HA, Fault-tolerance, self-monitoring, and a variety of deployment models.</li> <li>Kafka Streams is a library that any  standard Java application can embed and hence does not attempt to dictate a deployment method</li> <li>Kafka Streams within k8s will provide horizontal scaling. But it is bounded by the number of partitions. Resilience is ensured with Kafka topics.</li> <li>In term of application Life Cycle:<ul> <li>Flink: User\u2019s stream processing code is deployed and run as a job in the Flink cluster</li> <li>Kakfa Streams: User\u2019s stream processing code runs inside Java application</li> </ul> </li> <li>Flink supports data at rest or in motion, and multiple sources and sinks, no need to be only Kafka as KStream.</li> <li>Flink has Complex Event Processing capabilities to search for pattern of event occurences.</li> <li>Restorate State after Failure<ul> <li>Flink can restore state after failure from most recent incremental snapshot</li> <li>KStreams and KSQL Restore state after failure by replaying all messages </li> </ul> </li> <li>Coordination<ul> <li>Flink JobManager is part of the streaming application and orchestrate task manager. Job manager orchestration is done via Kubernetes scheduler.</li> <li>KStreams - Leverages the Kafka cluster for coordination, load balancing, and  fault-tolerance.</li> </ul> </li> <li>Bounded and unbounded data streams       &amp; Flink: Stream or Batch processing on Bounded<ul> <li>Kstreams: Stream only</li> </ul> </li> <li> <p>Language Flexibility</p> <ul> <li>Flink has a layered API - with most popular languages being Java, Python and SQL</li> <li>KStreams is Java only.</li> </ul> </li> <li> <p>Flink needs a custom implementation of <code>KafkaDeserializationSchema&lt;T&gt;</code> to read both key and value from Kafka topic.</p> </li> <li>Kafka streams is easier to define a pipeline for Kafka records and to do the <code>consume - process - produce</code> loop. </li> <li>KStreams uses the Kafka Record time stamp, while with Flink we need to implement how to deserialize the KafkaRecord and get the timestamp from it.</li> <li>Support of late arrival is easier with KStreams, while Flink uses the concept of watermark.</li> </ul>"},{"location":"architecture/fitforpurpose/#when-to-use-rule-engine-versus-flink","title":"When to use rule engine versus Flink","text":"<p>By rule engine, we are talking about libraries / products that are implementing the Rete Algorithm and extends from there.  Some of those engines are also supporting time windowing operators.  The major use case is to implement prescriptive logic based on <code>if ... then ...else</code> constructs and define the knowledge base as a set of rules.  This is the base of expert systems and it was part of the early years of Artificial Intelligence.  Expert systems have still their role in modern IT and AI solution. They help to:</p> <ul> <li>automate human's decisions as an expert will do. In fact it is better to say like a worker will apply his/her decisions on data and still be involved in addressing the more difficult decisions.</li> <li>have a clear understanding of the logic executed behind a decision, which is a real challenge in AI and deep learning models.</li> <li>reprocess rules when new facts are added so rule engine can be used to maintain a conversation with the client application  to enrich facts and take decision</li> <li>externalize the business logic from code:  it is easier to test and help to develop what-if scenarios with champion and challenger decision evaluation methodology</li> </ul> <p>Flink can do Complex Event Processing and Stream processing with time windowing.</p> <p>The technologies are indeed complementary: if we consider to get a stream of events from a event backbone like Kafka and then process those events with Flink we can also call a remote decision service via REST end point within the flink flow. </p> <p></p> <p>The figure above illustrates a generic processing, where event sources are injecting events to Kafka topics, Flink application processes the events as part of a situation detection pattern.  The situation detection is supported by the Flink processing and the rule engine: the responsability to implement the complex time windowing logic is assigned to a Developer, while the business logic to support scoring or assessing best action, may be done by business analysts using a high level rule language and a decision management platform.  It is important to note that once a situation is detected, it is important to publish it as a fact in a Kafka topic, to adopt an event sourcing and event-driven architecture approach.  The down stream processing is to compute the next best action. This component can enrich the data from the situation event received, so the best action decision can consider more data elements. This is a classical approach to develop rule based application. </p> <p>Once the action is decided, it is published to a topic, and this orchestration service (named here \"entity service\") may call different external services, like a business process execution environment, and robot process automation,...</p> <p>Another effective way is to embed the rule engine and the ruleset inside the Flink application:</p> <p></p> <p>The goal is to reduce latency and avoid unnecessary remote calls which adds complexity with retries, circuit breaker and fail over.</p>"},{"location":"architecture/kafka/","title":"Integration with Kafka","text":"<p>Flink has a Kafka connector for consuming and producing messages.  We need the connector jar, define Kafka server properties and then define the source for the stream.</p>"},{"location":"architecture/kafka/#consuming-from-kafka","title":"Consuming from Kafka","text":"<p>So the product documentation is wrong (03/2021): here are some notes and read the code under kafka-flink-demo folder and the TelemetryAggregate class.</p> <p>The major change is to implement a DeserializerSchema to process the event as java bean: (See code)</p> <pre><code>public class TelemetryDeserializationSchema implements DeserializationSchema&lt;TelemetryEvent&gt; {\n\n    private static final long serialVersionUID = -3142470930494715773L;\n    private static final ObjectMapper objectMapper = new ObjectMapper();\n\n    @Override\n    public TelemetryEvent deserialize(byte[] message) throws IOException {\n        return objectMapper.readValue(message, TelemetryEvent.class);\n    }\n\n    @Override\n    public boolean isEndOfStream(TelemetryEvent nextElement) {\n        return false;\n    }\n\n    @Override\n    public TypeInformation&lt;TelemetryEvent&gt; getProducedType() {\n        return TypeInformation.of(TelemetryEvent.class);\n    }\n\n}\n</code></pre> <p>Three serializers exist: TypeInformationSerializationSchema, JsonDeserializationSchema, AvroDeserializationSchema.</p> <p>The creation of the Kafka data source uses the traditional Kafka properties, and the following construct:</p> <pre><code>// better to use a separate class for config to be injected via CDI\nKafkaConfiguration configuration = new KafkaConfiguration();\n\nProperties kafkaProperties = configuration.getConsumerProperties(\"telemetryAggregators\");\n\nFlinkKafkaConsumer&lt;TelemetryEvent&gt; kafkaConsumer = new FlinkKafkaConsumer&lt;TelemetryEvent&gt;(configuration.mainTopicName, new TelemetryDeserializationSchema(), kafkaProperties);\nkafkaConsumer.setStartFromEarliest();\n\nDataStream&lt;TelemetryEvent&gt; telemetryStream = env.addSource(kafkaConsumer);\n</code></pre> <p>If no checkpointing is enabled then the consumer will periodically commit the offsets to Kafka (set <code>enable.auto.commit</code> and <code>auto.commit.interval.ms</code>) . It does not rely on the committed offsets for fault tolerance guarantees.</p> <p>With Flink\u2019s checkpointing enabled, the Flink Kafka Consumer will consume records from a topic and periodically checkpoint all its Kafka offsets, together with the state of other operations.  When the checkpoints are completed then it will commit offsets to kafka. In case of a job failure, Flink will restore the streaming program to the state of the latest  checkpoint and re-consume the records from Kafka, starting from the offsets that were stored  in the checkpoint.</p> <p></p> <p>But when it reprocesses the records again it will generate duplicate at the consumer level. </p> <p></p> <p>Therefore the Sink connector needs to support transactional producer, and uses the producer API to support avoid duplication with transaction id, idempotence and acknowledge on all replicas:</p> <p></p> <p>Partition discover should be enable by properties so Flink job can discover newly added partitions.</p>"},{"location":"architecture/kafka/#timestamp-and-watermark","title":"TimeStamp and Watermark","text":"<p>Timestamp can be in the ConsumerRecord or in the payload itself. so the app needs to specify how to extract the timestamp to be used for time window logic.</p>"},{"location":"coding/cep/","title":"Complex event processing","text":"<p>The goal of CEP is to analyzing pattern relationships between streamed events. Complex processing can be done using Flink using three capabilities: </p> <ul> <li>Stateful Function</li> <li>Flink CEP is a library to assess event pattern within a data stream.</li> <li>Flink SQL with pattern recognition</li> </ul>"},{"location":"coding/cep/#use-cases","title":"Use cases","text":"<ul> <li>Real-time marketing</li> <li>Anomalies detection</li> <li>Financial apps to check the trend in the stock market.</li> <li>Credit card fraud detection.</li> <li>RFID based tracking and monitoring systems</li> </ul>"},{"location":"coding/cep/#concepts","title":"Concepts","text":"<p>It uses the pattern API to define complex pattern sequences  that we want to extract from the input stream.</p> <p>The events in the DataStream to which you want to apply pattern matching must implement proper <code>equals()</code> and <code>hashCode()</code> methods because FlinkCEP uses them for comparing and matching events.</p> <p>The approach is to define simple patterns and then combine them in pattern sequence. </p> <p>A pattern can be either a singleton (accept a single event) or a looping pattern (accept more than one).</p> <p>Here is a generic pattern to illustrate the following sequencing of events like: A B* C</p> <pre><code>Pattern\n        .begin(\"A\").where(/* conditions */)\n        .next(\"B\").oneOrMore().optional().where(/* conditions */)\n        .next(\"C\").where(/* conditions */)\n</code></pre> <p>Quantifier specificies the pattern type.  </p> <pre><code>pattern.oneOrMore()\npattern.times(#ofTimes)\npattern.times(#fromTimes, #toTimes)\n// expecting 1 or more occurrences and repeating as many as possible\npattern.oneOrMore().greedy();\n</code></pre> <p>Each pattern can have one or more conditions based on which it accepts events.</p> <pre><code>pattern.where() \npattern.or()\npattern.until()\n</code></pre> <p>With Iterative condition we can specify a condition that accepts subsequent events  based on properties of the previously accepted events or a statistic over a subset of them.  Iterative conditions can be powerful, especially in combination with looping patterns, e.g. <code>oneOrMore()</code>.</p> <p>We can combine patterns by specifying the desired contiguity conditions between them.</p> <p>FlinkCEP supports the following forms of contiguity between events:</p> <ul> <li>Strict Contiguity: Expects all matching events to appear strictly one after the other, without any non-matching events in-between (use <code>next()</code> function).</li> <li>Relaxed Contiguity: Ignores non-matching events appearing in-between the matching ones (use <code>followedBy()</code>): \"\u201cskip non-matching events till the next matching one\"</li> <li>Non-Deterministic Relaxed Contiguity: Further relaxes contiguity, allowing additional matches that ignore some matching events (use <code>followedByAny()</code>).</li> </ul>"},{"location":"coding/datastream/","title":"DataStreams Programming guidances and examples","text":"<p>This chapter is a summary on how to use DataStream API, laborate from Apache Flink documentation, and other existing examples for Flink DataStream.</p>"},{"location":"coding/datastream/#key-concepts","title":"Key Concepts","text":"<ul> <li>A DataStream program is a regular Java program packaged in a JAR file.  Each Java Flink app is a Java main function which defines the data flow to execute on one or more data streams.</li> <li>In application mode, the <code>main()</code> function runs in the Job manager which constructs a directed acyclic graph and executes the graph operators within task managers.</li> <li>In session mode, a cli submits the jar file to a Flink JobManager which does the same processing.</li> <li>All programs get access to a Flink environment.     <pre><code>import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n...\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setParallelism(2);\n// define the topology...\n\nenv.execute(\"program name for UI ref\");\n</code></pre></li> <li>The datastream API is an abstraction to define topology of operators to be executed on the stream of events/records.</li> <li>Use Datastream when strong control and customisation is needed. </li> <li>Programs are organized as a set of user functions. A function can be implemented as class, anonymous class, or lambda.</li> </ul> Classical Code structure <p>The code structure follows the following standard steps:</p> <ol> <li>Obtain a Flink execution environment</li> <li> <p>Define the data pipeline graph (as a separate method to simplify unit testing)</p> <ol> <li>Load/create the initial data from the stream</li> <li>Specify transformations on the data</li> <li>Specify where to put the results of the computations</li> </ol> </li> <li> <p>Trigger the program execution</p> </li> </ol> <ul> <li>It is easy to go from DataStream to TableAPI. TableAPI is simpler to use for joins and windows logic implementation.  <pre><code>import org.apache.flink.table.api.Table;\nimport org.apache.flink.table.api.TableDescriptor;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n...\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nStreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);\n// Stream -&gt; Table\nDataStream&lt;?&gt; inStream1 = ...\nTable appendOnlyTable = tableEnv.fromDataStream(inStream1)\n// Table -&gt; Stream\nDataStream&lt;T&gt; appendOnlyStream = tableEnv.toDataStream(insertOnlyTable, T.class)\n// using Row type\nDataStream&lt;Row&gt; changelogStream = tableEnv.toChangelogStream(anyTable)\n</code></pre></li> </ul>"},{"location":"coding/datastream/#create-a-java-project-with-maven","title":"Create a Java project with maven","text":"<p>See the Apache Flink product documentation which can be summarized as:</p> <ol> <li>Be sure to have maven <code>mvn</code> cli and JAVA_HOME setup</li> <li> <p>Create a project template, named quickstart, using the Flink quickstart shell and specifying the Flink version (e.g. 1.20.2)</p> <pre><code>curl https://flink.apache.org/q/quickstart.sh | bash -s 1.20.2\n</code></pre> </li> <li> <p>Add the following maven dependencies into the <code>pom.xml</code>:</p> <pre><code>&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n    &lt;artifactId&gt;flink-java&lt;/artifactId&gt;\n    &lt;version&gt;${flink-version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> </li> <li> <p>When using Kafka, add Flink kafka connector dependency in pom.xml</p> <pre><code>  &lt;dependency&gt;\n      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n      &lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;\n      &lt;version&gt;1.20.2&lt;/version&gt;\n  &lt;/dependency&gt;\n</code></pre> </li> <li> <p>Create a Java Class with a main function and the following code structure (See 01-word-count example):</p> <ul> <li>get Flink execution context</li> <li>defined process flow to apply to the data stream</li> <li>start the execution</li> </ul> <pre><code>// Get execution context\n  ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n  // use file as input so use program arguments to get file name\n  ParameterTool params = ParameterTool.fromArgs(args);\n  env.getConfig().setGlobalJobParameters(params);\n  defineWorkflow(env)\n  env.execute();\n</code></pre> <p>The code above uses the ParameterTool  class to process the program arguments.  So most of the basic examples use <code>--input filename</code> and <code>--output filename</code> as java arguments. So <code>params</code> will have those arguments in a Map. </p> </li> <li> <p>Define event structure as POJO, as a separate Java Bean in the <code>event</code> folder.</p> </li> <li>Implement the process logic and the event mapping, filtering logic... We will see more examples later.</li> <li>Package with <code>mvn package</code>. Create a UBER jar if using external system, for reading from a filesystem, Flink has the predefined connectors. The <code>maven-shade-plugin</code> maven plugin creates such UBER jar.</li> <li>Access to a Flink compute pool, locally using local installation, docker, Kubernetes or Confluent Cloud for Flink. Use the <code>flink cli</code> to submit the job for a local cluster, use FlinkDeployment(KFF) or FlinkApplication (CMF)    <pre><code>flink run -c j9r.flink.MyJob myapp.jar\n</code></pre></li> </ol> <p>With the open source version, we have access to a lot of different connectors, for example to load data from csv file. This is convenient to do local testing or batch processing.</p> Create a Quarkus Flink java app <ul> <li>Create a Quarkus app: <code>quarkus create app -DprojectGroupId=jbcodeforce -DprojectArtifactId=my-flink</code>. See code examples under <code>flink-java/my-flink</code> folder and <code>jbcodeforce.p1</code> package.</li> <li>Do the same steps as above for the main class.</li> <li>Be sure to set quarkus uber-jar generation (<code>quarkus.package.type=uber-jar</code>) in the <code>application.properties</code> to get all the dependencies in a unique jar: Flink needs all dependencies in the classpath.</li> </ul>"},{"location":"coding/datastream/#submit-job-to-flink","title":"Submit job to Flink","text":"<p>When the Flink cluster runs with a Job Manager, it is possible to submit the application via the <code>flink</code> cli.</p>"},{"location":"coding/datastream/#using-a-jobmanager-running-as-docker-image","title":"Using a JobManager running as docker image","text":"<p>The jar file is mounted to the image to <code>/home/my-flink/target/</code>.</p> <pre><code># One way with mounted files to task manager and job manager containers.\nCNAME=\"jbcodeforce.p1.WordCountMain\"\nJMC=$(docker ps --filter name=jobmanager --format={{.ID}})\ndocker exec -ti $JMC flink run -d -c $CNAME /home/my-flink/target/my-flink-1.0.0-runner.jar --input file:///home/my-flink/data/wc.txt --output file:///home/my-flink/data/out.csv \n\n# inside the jobmanager\nflink run -d -c jbcodeforce.p1.WordCountMain /home/my-flink/target/my-flink-\n1.0.0-runner.jar --input file:///home/my-flink/data/wc.txt --output file:///home/my-flink/data/out.csv\n</code></pre>"},{"location":"coding/datastream/#using-flinkdeployment-and-k8s-operator","title":"Using FlinkDeployment and K8S Operator","text":"<ul> <li>Create FlinkDeployment manifest with the JobSpec</li> <li>Deploy with Kubectl     <pre><code>kubectl apply -f app_deploy.yaml -n flink-oss\n</code></pre></li> </ul>"},{"location":"coding/datastream/#best-practices","title":"Best Practices","text":"<ul> <li>Isolate the business logic outside of the main, and use Flink unit test environment to validate the topology logic.</li> <li>In production, run the main() method, to use the real sources and sinks</li> <li>Clearly assess the type of tests to implement: unit test to validate individual operator. For integration tests, use bounded sources with controlled records, and sinks to collectors.</li> <li>Use MiniClusterExtension for testing.</li> </ul>"},{"location":"coding/datastream/#datastream-deeper-dive","title":"Datastream deeper dive","text":"<ul> <li>Datastream open source documentation with the API.</li> <li>Confluent Flink Cookbook, is a set of recipes around Flink using DataStream, TableAPI. Once cloned, load one of the folder as a java project in IDE, build and run unit tests. This repository includes tools to run a mini Flink cluster in Java directly.</li> <li>DataStream v2</li> </ul>"},{"location":"coding/datastream/#unit-testing","title":"Unit Testing","text":"<p>To rework</p>"},{"location":"coding/datastream/#data-set-basic-apps","title":"Data set basic apps","text":"<p>Attention Scala and DataSet aPi do not exist anymore</p> <p>Deprecated from Flink 1.18.</p> <p>The examples directly in the my-flink project under the  jbcodeforce.p1 package:</p> <ul> <li>PersonFiltering.java filter a persons datastream using person's age to create a new \"adult\" output data stream. This example uses test data from a list of person and uses a filtering class which implements the filter method. This code can execute in VSCode or any IDE</li> <li>InnerJoin Proceed two files and do an inner join by using the same key on both files. See next section for details.</li> <li>LeftOuterJoin results will include matching records from both tuples and non matching from left (so person) (<code>personSet.leftOuterJoin(locationSet)</code>).</li> <li>RightOuterJoin matching records present in both data sets and non matching from the right.</li> <li>Full outer join when matching and non matching are present. See fulljoinout.csv output file.</li> <li>Traditional word count from a text uses a filter function to keep line starting by a pattern (letter 'N'), then it uses a tokenizer function to build a tuple for each word with a count of 1. The last step of the flow is to groupBy word and sum the element. Not obvious.</li> </ul>"},{"location":"coding/datastream/#inner-join","title":"Inner join","text":"<p>Need to read from two files and prepare them as tuples. Then process each record of the first tuple with the second one  using field 0 on both tuples as join key. The <code>with()</code> build the new tuple with combined values.  <code>with()</code> need a join function to implement the joining logic and attributes selection.</p> <pre><code> DataSet&lt;Tuple3&lt;Integer,String,String&gt;&gt; joinedSet = \n      personSet.join(locationSet)\n      .where(0) // indice of the field to be used to do join from first tuple\n      .equalTo(0)  // to match the field in idx 0 of the second tuple\n      .with( new JoinFunction&lt;Tuple2&lt;Integer, String&gt;, \n                              Tuple2&lt;Integer, String&gt;, \n                              Tuple3&lt;Integer, String, String&gt;&gt;() {\n\n          public Tuple3&lt;Integer, String, String&gt; join(Tuple2&lt;Integer, String&gt; person,  Tuple2&lt;Integer, String&gt; location)  {\n              return new Tuple3&lt;Integer, String, String&gt;(person.f0,   person.f1,  location.f1);\n          }              \n      });\n</code></pre> <p>Exec within the <code>JobManager</code> container.</p> <pre><code>flink run -d -c jbcodeforce.p1.InnerJoin /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar --persons file:///home/my-flink/data/persons.txt --locations file:///home/my-flink/data/locations.txt --output file:///home/my-flink/data/joinout.csv \n</code></pre>"},{"location":"coding/datastream/#left-outer-join","title":"Left outer join","text":"<p>The construct is the same as above, except the results will include matching records from both tuples and  non matching records coming from the left part of the join:</p> <pre><code> DataSet&lt;Tuple3&lt;Integer,String,String&gt;&gt; joinedSet = \n            personSet.leftOuterJoin(locationSet)\n            ....\n\n      public Tuple3&lt;Integer, String, String&gt; join(\n                        Tuple2&lt;Integer, String&gt; person,  \n                        Tuple2&lt;Integer, String&gt; location)  {\n          if (location == null) {\n              return new Tuple3&lt;Integer, String, String&gt;(person.f0, person.f1, \"NULL\");\n          }\n          return new Tuple3&lt;Integer, String, String&gt;(person.f0,   person.f1,  location.f1);\n      }  \n</code></pre>"},{"location":"coding/datastream/#data-stream-examples","title":"Data Stream examples","text":"<p>Data stream API is used to get real time data. It can come from file with readFile with watching folder for new file to be read, or use <code>socketTextStream</code> or any streaming source (addSource) like Twitter, Kafka...</p> <p>The output can also be a stream (as sink): writeAsText(),.. writeToSocket, addSink...</p> <p>See example in <code>my-flink</code> project source WordCountSocketStream, and to test it, use the <code>nc -l 9999</code> tool to open a socket on port 9999 and send text message.</p> <p>When using docker we need to open a socket in the same network as the Flink task manager, the command looks like:</p> <pre><code>docker run -t --rm --network  flink-studies_default --name ncs -h ncshost subfuzion/netcat -l 9999\n</code></pre>"},{"location":"coding/datastream/#compute-average-profit-per-product","title":"Compute average profit per product","text":"<p>The data set avg.txt represents transactions for a given product with its sale profit. The goal is to compute the average profit per product per month. </p> <p>The solution use Map - Reduce functions.</p> <ul> <li>Input sample:</li> </ul> <pre><code>01-06-2018,June,Category5,Bat,12\n01-06-2108,June,Category4,Perfume,10\n</code></pre> <ul> <li>Output:</li> </ul> <p>In the class datastream.ProfitAverageMR, the DataStream loads the input file as specified in  <code>--input</code> argument and then splits record to get columns as tuple attributes.</p> <pre><code> DataStream&lt;String&gt; saleStream = env.readTextFile(params.get(\"input\"));\n // month, product, category, profit, count\n DataStream&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; mappedSale = saleStream.map(new Splitter()); \n</code></pre> <p>The <code>Splitter</code> class implements a MapFunction which splits the csv string and select the attributes needed to generate the tuple.</p> <p>A first reduce operation is used on the sale tuple where the key is a month (output from GetMonthAsKey) to accumulating profit and the number of record:</p> <pre><code>DataStream&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; reduced = \n  mappedSale.keyBy(new GetMonthAsKey())\n  .reduce(new AccumulateProfitAndRecordCount()); \nDataStream&lt;Tuple2&lt;String, Double&gt;&gt; profitPerMonth = reduced.map(new MapOnMonth());\n</code></pre> <p>here is the main reduce function: the field f3 is the profit, and f4 the number of sale.</p> <pre><code> public static class AccumulateProfitAndRecordCount implements ReduceFunction&lt;Tuple5&lt;String, String, String, Integer, Integer&gt;&gt; {\n\n    private static final long serialVersionUID = 1L;\n    @Override\n    public Tuple5&lt;String, String, String, Integer, Integer&gt; reduce(\n            Tuple5&lt;String, String, String, Integer, Integer&gt; current,\n            Tuple5&lt;String, String, String, Integer, Integer&gt; previous) throws Exception {\n\n        return new Tuple5&lt;String, String, String, Integer, Integer&gt;(current.f0,current.f1,current.f2,current.f3 + previous.f3, current.f4 + previous.f4);\n    }\n}\n</code></pre> <p>To run the example once the cluster is started use:</p> <pre><code> docker exec -ti $JMC flink run -d -c jbcodeforce.datastream.ProfitAverageMR /home/my-flink/target/my-flink-1.0.0-runner.jar --input file:///home/my-flink/data/avg.txt \n</code></pre>"},{"location":"coding/datastream/#aggregates","title":"Aggregates","text":"<p>See all the operators examples in this note.</p> <p>Examples of aggregate API to compute min, max... using field at index 3</p> <pre><code>mapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).sum(3).writeAsText(\"/home/my-flink/data/out1\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).min(3).writeAsText(\"/home/my-flink/data/out2\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0) .minBy(3).writeAsText(\"/home/my-flink/data/out3\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).max(3).writeAsText(\"/home/my-flink/data/out4\");\n\nmapped.keyBy(( Tuple4&lt;String, String, String, Integer&gt; record) -&gt; record.f0 ).maxBy(3).writeAsText(\"/home/my-flink/data/out5\");\n</code></pre>"},{"location":"coding/datastream/#taxi-rides-examples","title":"Taxi rides examples","text":"<p>This is a more complex solution with a lot of good inspirations for utilities class and way to work on Java Beans.</p> <p>See the flink-training github to access to the source code.</p> <ul> <li>Lab 1- filter non NY taxi rides, the process flow uses the <code>DataStream::filter</code> method. The NYCFilter is a class-filter-function.</li> </ul> <pre><code>DataStream&lt;TaxiRide&gt; filteredRides = rides\n    // keep only those rides and both start and end in NYC\n    .filter(new NYCFilter());\n// ...\n\npublic static class NYCFilter implements FilterFunction&lt;TaxiRide&gt; {\n    @Override\n    public boolean filter(TaxiRide taxiRide) {\n        return GeoUtils.isInNYC(taxiRide.startLon, taxiRide.startLat) &amp;&amp;\n                GeoUtils.isInNYC(taxiRide.endLon, taxiRide.endLat);\n    }\n}\n</code></pre> <p>This exercise uses a lot of utility classes for data and tests which hide the complexity of the data preparation  (see the common folder within the training repository).</p> <ul> <li>Process ride and fare data streams for stateful enrichment.  The result should be a DataStream&gt;, with one record for each distinct rideId.  Each tuple should pair the TaxiRide START event for some rideId with its matching TaxiFare.  There is no control over the order of arrival of the ride and fare records for each rideId. <pre><code>DataStream&lt;TaxiRide&gt; rides = env\n        .addSource(rideSourceOrTest(new TaxiRideGenerator()))\n        .filter((TaxiRide ride) -&gt; ride.isStart)\n        .keyBy((TaxiRide ride) -&gt; ride.rideId);\n\nDataStream&lt;TaxiFare&gt; fares = env\n        .addSource(fareSourceOrTest(new TaxiFareGenerator()))\n        .keyBy((TaxiFare fare) -&gt; fare.rideId);\n\n// Set a UID on the stateful flatmap operator so we can read its state using the State Processor API.\nDataStream&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; enrichedRides = rides\n        .connect(fares)\n        .flatMap(new EnrichmentFunction())\n        .uid(\"enrichment\");\n</code></pre> <p>The join and stateful implementation are done in the EnrichmentFunction as a <code>RichCoFlatMap</code>. A CoFlatMapFunction implements a flat-map transformation over two connected streams. The same instance of the transformation function is used to transform both of the connected streams. That way, the stream transformations can share state.</p> <p>RidesAndFaresSolution.java</p> <p><code>ValueState&lt;TaxiRide&gt; rideState</code> is a partitioned single-value state.</p> <p><code>flatMap1(TaxiRide ride, Collector&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; out)</code> method is called for each element in the first of the connected streams. So here on a ride event, if there is a matching fare already computed then generate the output tuple, if not update keep the ride to be used for the fare event processing.</p> <p><code>flatMap2(TaxiFare fare, Collector&lt;Tuple2&lt;TaxiRide, TaxiFare&gt;&gt; out)</code> method is called on the second connected streams. When a fare event arrives, if there is a ride with the same key, join, if not keep the fare for future ride event.</p> <p>So one of the trick is in the ValueState class.</p> <ul> <li>Hourly tips is a time windowed analytics to identify, for each hour, the driver earning the most tips. The approach is to use hour-long windows that compute the total tips for each driver during the hour, and then from that stream of window results, find the driver with the maximum tip total for each hour.</li> </ul> <p>The first data stream below applies a window on a keyed stream. Process is one of the function to use on the window. (reduce and aggregate are the others). </p> <pre><code>    DataStream&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyTips = fares\n            .keyBy((TaxiFare fare) -&gt; fare.driverId)\n            .window(TumblingEventTimeWindows.of(Time.hours(1)))\n            .process(new AddTips());\n\n    DataStream&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyMax = hourlyTips\n            .windowAll(TumblingEventTimeWindows.of(Time.hours(1)))\n            .maxBy(2);\n</code></pre> <p>A process window has an iterable on the collection of events in the window to work with:</p> <pre><code>public static class AddTips extends ProcessWindowFunction&lt;\n            TaxiFare, Tuple3&lt;Long, Long, Float&gt;, Long, TimeWindow&gt; {\n\n    @Override\n    public void process(Long key, Context context, Iterable&lt;TaxiFare&gt; fares, Collector&lt;Tuple3&lt;Long, Long, Float&gt;&gt; out) {\n        float sumOfTips = 0F;\n        for (TaxiFare f : fares) {\n            sumOfTips += f.tip;\n        }\n        out.collect(Tuple3.of(context.window().getEnd(), key, sumOfTips));\n    }\n}\n</code></pre> <p>Time windowing has limitations:</p> <ul> <li>can not correctly process historic data</li> <li>can not correctly handle out-of-order data</li> <li> <p>results will be non-deterministic</p> </li> <li> <p>Long ride alert is an example of Event driven application where alerts are created if a taxi ride started two hours ago is still ongoing. It uses event timestamp and watermarks.</p> </li> </ul> <p>The key is in the MatchFunction process function implementation in which START or END events are kept in a value state, but a timer is set on the context, so the method may get a timer trigger with a processing event that will trigger the onTimer() callback method.</p> <pre><code>context.timerService().registerEventTimeTimer(getTimerTime(ride));\n</code></pre> <p>It generates to the output stream / sink only records from this onTimer.</p>"},{"location":"coding/datastream/#fraud-detection","title":"Fraud detection","text":"<p>This example is based on traditional card transaction fraud detection evaluation. The logic  may need to support:</p> <ul> <li>verify card is not already reported as lost or stolen</li> <li>verify the customer is not already alerted (avoid over alerting)</li> <li>multiple transactions in short time period</li> <li>duplicate transactions from a specific merchant type</li> <li>online transaction and in-person transaction followed in short time period</li> <li>transaction location too far to be feasible in the time period</li> </ul> <p>For lost cards and customer alerted the lists are sent to each node processing the flow so the lookup / join is local and based on the card_id and customer_id. In an EDA implementation the sources will come from Kafka Topics. </p> <p>To support the search for transaction.card_number being lost or not, we use the concepts of broadcast state and stream. The BroadcastState  the same elements are sent to all instances of an operator.</p>"},{"location":"coding/datastream/#interesting-articles","title":"Interesting articles","text":"<ul> <li>Event Driven File Ingestion using Flink Source API</li> </ul>"},{"location":"coding/firstapp/","title":"First Java Applications","text":""},{"location":"coding/firstapp/#an-example-of-batch-processing","title":"An example of Batch processing","text":"<p>The goals is to read a csv file and compute aggregates in Java DataStream, and then TableAPI. The code is in code/table-api/loan-batch-processing</p> <p>See the coding practice summary for more datastream examples.</p> <p>And the official operators documentation to understand how to transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated data flow topologies.</p> <p>TO REWORK</p>"},{"location":"coding/firstapp/#unit-testing","title":"Unit testing","text":"<p>There are three type of function to test:</p> <ul> <li>Stateless</li> <li>Stateful</li> <li>Timed process</li> </ul>"},{"location":"coding/firstapp/#stateless","title":"Stateless","text":"<p>For stateless, the data flow can be isolated in static method within the main class, or defined within a separate class. The test instantiates the class and provides the data.</p> <p>For example testing a string to a tuple mapping (MapTrip() is a MapFunction(...) extension):</p> <pre><code> public void testMapToTuple() throws Exception {\n        MapTrip mapFunction = new MapTrip();\n        Tuple5&lt;String,String,String, Boolean, Integer&gt; t = mapFunction.map(\"id_4214,PB7526,Sedan,Wanda,yes,Sector 19,Sector 10,5\");\n        assertEquals(\"Wanda\",t.f0);\n        assertEquals(\"Sector 19\",t.f1);\n        assertEquals(\"Sector 10\",t.f2);\n        assertTrue(t.f3);\n        assertEquals(5,t.f4);\n    }\n</code></pre>"},{"location":"coding/firstapp/#stateful","title":"Stateful","text":"<p>The test needs to check whether the operator state is updated correctly and if it is cleaned up properly, along with the output of the operator. Flink provides TestHarness classes so that we don\u2019t have to create the mock objects.</p>"},{"location":"coding/flink-sql-1/","title":"Create Table SQL (DDLs)","text":"Updates <p>Created 10/24, Updated 12/20/24 Revised 12/06/24</p> <p>This chapter offers a compilation of best practices for implementing Flink SQL solutions, applicable to local Flink open-source, the Confluent Platform for Flink or the Confluent Cloud for Flink.</p> <p>Data Definition Language (DDL) are statements to define metadata in Flink SQL by creating, updating, or deleting tables. See the Flink SQL Examples in Confluent Cloud for Apache Flink documentation.</p> <p>A table registered with the CREATE TABLE statement can be used as a table source or a table sink.</p> <p>Create table statements do not changes between managed services and standalone Flink deployment, except the metadata. Each examples with specific standalone content, will be marked as Flink OSS, which also works for Confluent Platform Flink.</p>"},{"location":"coding/flink-sql-1/#sources-of-information","title":"Sources Of Information","text":"<ul> <li>Confluent SQL documentation for DDL  with useful examples.</li> <li>Apache Flink CREATE SQL</li> <li>Confluent Developer Flink tutorials</li> <li>See SQL examples from product documentation.</li> </ul> <p>Create table syntax</p>"},{"location":"coding/flink-sql-1/#common-principles","title":"Common Principles","text":"<ul> <li>Primary key can have one or more columns, all of them should be not null, and only being <code>NOT ENFORCED</code></li> <li>Inside Flink processing, the primary key declaration, partitions the table implicitly by the key column(s)</li> <li>Flink uses the primary key for state management and deduplication with upsert. While the partition key is what determines which Kafka partition a message will be written to. This is a Kafka-level concept.</li> <li> <p>For upsert mode, the bucket key must be equal to primary key. While for append/retract mode, the bucket key can be a subset of the primary key. See more on changelog.mode</p> </li> <li> <p>Understanding the table structure and mapping from a Kafka topic: the basic query helps a lot to understand table configuration and column types. This is helpful to assess time column, to confirm if the type of a column as event_time is a STRING, TIMESTAMP or TIMESTAMP_LTZ.</p> <pre><code>show create table\n</code></pre> <p>The <code>TYPEOF(column_name)</code> can also being used in a select to understand the type, and troubleshooting a type error.</p> </li> <li> <p>Understand the execution plan of a query using <code>EXPLAIN insert or select</code>, helps to assess if the query supports the expected semantic like window aggregation, interval joins or classical joins. The change to append to retract or upsert is also very important for state assessment. Recall that properties of the destination table will influence the plan. </p> </li> </ul>"},{"location":"coding/flink-sql-1/#table-creation-how-tos","title":"Table Creation How-Tos","text":""},{"location":"coding/flink-sql-1/#sql-table-creation-any-product","title":"SQL Table Creation (any product)","text":"<p>Apache Flink Create statements documentation.</p> Create a table as another table by inserting all records (CTAS create table as select) <p>CREATE TABLE AS SELECT is used to create table and insert values in the same statement. It derives the physical column data types and names (from aliased columns), the changelog.mode (from involved tables, operations, and upsert keys), and the primary key.</p> <p>By using a primary key:</p> <pre><code>create table shoe_customer_keyed(\n    primary key(id) not enforced\n) distributed by(id) into 1 buckets\nas select id, first_name, last_name, email from shoe_customers;\n</code></pre> How to support nested rows, DDL and inserts? <p>Avro, Protobuf or Json schemas are very often hierarchical per design. ROW and ARRAY are the objects with nested elements. <code>StatesTable</code> is a column in table <code>t</code>. It has rows of <code>states</code> column which is itself a array of <code>name,city,lg,lat</code>.</p> <pre><code>-- DDL\ncreate table t (\n    StatesTable ROW&lt; states ARRAY&lt;ROW&lt;name STRING, city STRING, lg DOUBLE, lat DOUBLE&gt;&gt;&gt;,\n    creationDate STRING\n)\n\ninsert into t(group_id, StatesTable,creationDate)\nvalues( \n    'grp_1',\n    (\n    ARRAY[ row('California', 'San Francisco', -122.4194, 37.7749),\n        row('New York', 'New York City', -74.0060, 40.7128),\n        row('Texas', 'Austin', -97.7431, 30.2672)\n    ]\n    ),            -- StatesTable\n    '2020-10-10'  -- creationDate\n)\n</code></pre> How to support nested rows, and inserts from previous table definition? <p>From previous table definition, to extract lat and long:</p> <pre><code>select\ngroup_id,\n-- example of defining attribute from the idx element of an array (starts from 1) from a nested schema:\n  CAST(StatesTable.states[3] AS DECIMAL(10, 4)) AS longitude,\n  CAST(StatesTable.states[4] AS DECIMAL(10, 4)) AS latitude\nfrom t\n-- results\n 'grp_1', -122.4194, 37.7749\n 'grp_1', -74.0060, 40.7128\n 'grp_1', -97.7431, 30.2672\n</code></pre> <p>See also the CROSS JOIN UNNEST keywords.</p> <p>See also a running demo in flink-sql/03-nested-row</p> How to add a metadata field in a table? <p>Use ALTER TABLE to modify existing table. Below is an example of adding dt attribute.</p> <pre><code>alter table flight_schedules add(dt string metadata virtual);\n</code></pre> How to transfer the source timestamp to another table <p>As $rowtime is the timestamp of the record in Kafka, it may be interesting to keep the source timestamp for the downstream tables.</p> <pre><code>create table `some_clicks` (\n      `order_id` STRING NOT NULL,\n      ...\n      `event_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'),\n      PRIMARY KEY(order_id) NOT ENFORCED\ndistributed by hash(order_id) into '4' buckets\n</code></pre> <p>Then the statement to insert record to the new table:</p> <pre><code>insert into `some_clicks`\nselect\n    order_id, \n    user_id,\n    $rowtime as event_time\nfrom  `src_table`;\n</code></pre> Views over Tables <p>Recall views, are read-only, and have no insert operation and are used to encapsulate complex queries and reference them like regular tables. It acts as a virtual table that refers to the result of the specified statement expression. </p> <pre><code>CREATE VIEW IF NOT EXISTS table_view AS SELECT ...\n</code></pre> <p>Views and tables share the same namespace in Flink. See array of row view example.</p> Materialized Table <p>Materialized Table is an encapsulation for Data Freshness(data lag behind the base table), Refresh Mode (FULL or CONTINUOUS), Query Definition and Schema(the column names and types are inferred). Materialized Tables aim to give developers safe evolution of streaming queries (and schemas) via CREATE OR ALTER MATERIALIZED TABLE ... rather than stop/delete/recreate statements.  It automates offset &amp; state handling. Example of creation: <pre><code>    CREATE MATERIALIZED TABLE [catalog_name.][db_name.]table_name\n    -- Primary key constraint\n    [([CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED)]\n\n    [COMMENT table_comment]\n    -- Partition key\n    [DISTRIBUTED BY (partition_column_name1, partition_column_name2, ...)]\n    -- WITH options\n    [WITH (key1=val1, key2=val2, ...)]\n    -- Data freshness\n    FRESHNESS = INTERVAL '&lt;num&gt;' { SECOND | MINUTE | HOUR | DAY }\n    -- Refresh mode\n    [REFRESH_MODE = { CONTINUOUS | FULL }]\n    AS  &lt;select_statement&gt; \n</code></pre></p> <p>FULL will trigger a batch snapshot query to fully recompute the table from bounded or snapshot\u2011only sources (e.g., Iceberg/Delta), and then swap the content atomically. This gives you a managed full backfill / reconciliation path and a cheaper option for low\u2011velocity data (nightly refresh instead of 24/7 streaming job)</p> Dealing with late event <p>Any streams mapped to a table have records arriving more-or-less in order, according to the <code>$rowtime</code>, and the watermarks let the Flink SQL runtime know how much buffering of the incoming stream is needed to iron out any out-of-order-ness before emitting the sorted output stream.</p> <p>We need to  specify the watermark strategy: for example within 30 second of the event time: </p> <pre><code>create table new_table (\n    ....\n    `event_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp',\n watermark for `event_time` as `event_time` - INTERVAL '30' SECOND\n);\n</code></pre> <p>Event_time could be a ts from the source table (like CDC table). On CCF, the watermark is on the <code>$rowtime</code> by default, but it can be changed. </p> How to change system watermark? <p>Modify the WATERMARK metadata using alter table.</p> <pre><code>ALTER TABLE table_name MODIFY WATERMARK FOR $rowtime AS $rowtime - INTERVAL '1' SECOND;\n-- in case we need to reverse back\nALTER TABLE table_name DROP WATERMARK;\n</code></pre> <p>This can be used when doing enrichment join on reference table. We do not want to wait for watermark arriving on the reference table, so set the watermark of this reference table to the max INT using <code>ALTER TABLE table_name SET</code>$rowtime<code>TO_TIMESTAMP(,0)</code></p> Create a table with topic as one day persistence <p>See the WITH options.</p> <p><pre><code>create table `small-orders` (\n    `order_id` STRING NOT NULL,\n    `customer_id` INT NOT NULL,\n    `product_id` STRING NOT NULL,\n    `price` DOUBLE NOT NULL\n) distributed by hash(order_id) into 1 buckets\nwith (\n    'kafka.retention.time' = '1 d'\n);\n\ninsert into `small-orders` select * from `examples`.`marketplace`.`orders` where price &lt; 20;\n</code></pre> <code>distributed by hash(order_id)</code> and <code>into 1 buckets</code> specify that the table is backed by a Kafka topic with 1 partition, and the <code>order_id</code> field will be used as the Kafka partition key. </p> Table with Kafka Topic metadata <p>The headers and timestamp are the only options not read-only, all are VIRTUAL. Virtual columns are by default excluded from a SELECT * similar to the system column like <code>$rowtime</code>. </p> <pre><code>ALTER TABLE &lt;table_name&gt; ADD (\n    `headers` MAP&lt;STRING,STRING&gt; METADATA,\n    `leader-epoch`INT METADATA VIRTUAL,\n    `offset` BIGINT METADATA VIRTUAL,\n    `partition` BIGINT METADATA VIRTUAL,\n    `timestamp` TIMESTAMP_LTZ(3) METADATA,\n    `timestamp-type` STRING METADATA VIRTUAL,\n    `topic` STRING METADATA VIRTUAL\n);\n</code></pre> <p>The headers can be updated within SQL statements, some values may be static or coming from the value of one of the selected field. The timestamp can also being updated and for example in most time window queries will be set to the <code>window_time</code>.</p> <pre><code>CREATE TABLE clicks_per_seconds (\n    events_per_second BIGINT,\n    window_time TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'\n    )\n\nINSERT INTO clicks_per_seconds\nSELECT\n    COUNT(*) AS events_per_second,\n    window_time\nFROM TABLE(TUMBLE(TABLE clicks, DESCRIPTOR(`$rowtime`), INTERVAL '1' SECOND))\nGROUP BY window_time, window_end, window_start\n</code></pre>"},{"location":"coding/flink-sql-1/#confluent-cloud-specifics","title":"Confluent Cloud Specifics","text":"<ul> <li>In Confluent Cloud, partition key will generate a key schema, except if using the option (<code>'key.format' = 'raw'</code>)</li> <li>If the destination topic doesn't define partitioning key, then CC Flink SQL will write the records in whatever partitioning that was used at the end of the query. if last operation in the query is <code>GROUP BY foo</code> or <code>JOIN ON A.foo = B.foo</code>, then output records would be partitioned on <code>foo</code> values, and they wouldn't be re-partitioned before writing them into Kafka. The <code>foo</code> partitioning is preserved. </li> </ul> Primary key and partition by considerations <ul> <li>If you have parallel queries without any data shuffling, like <code>INSERT INTO Table_A SELECT * FROM Table_B</code>, then any skew from Table_B would be repeated in Table_A. Otherwise, if partitioning key is defined (like <code>DISTRIBUTED BY HASH(metric)</code>), any writes into that topic would be shuffled by that new key.</li> <li>In case of key skew, add more fields in the distribution to partition not just in one key. That would allow Flink to read data in more parallel fashion, improving the problem with readings from Kafka's bottleneck of 18MBs/connection (partition)</li> <li>An interesting metric for Confluent Cloud Flink, 5 partitions feeds 1 CFU. CFU has constraint on memory and cpu.</li> <li>The performance bottleneck will be actually records serialization and deserialisation, avro is slower than protobuf.</li> </ul> <pre><code>-- simplest table\nCREATE TABLE humans (race STRING, origin STRING);\n-- with primary key \nCREATE TABLE manufactures (m_id INT PRIMARY KEY NOT ENFORCED, site_name STRING);\n-- with hash distribution to 2 partitions that match the primary key\nCREATE TABLE humans (hid INT PRIMARY KEY NOT ENFORCED, race STRING, gender INT) DISTRIBUTED BY (hid) INTO 2 BUCKETS;\n</code></pre> How to read all records from topic (at least once)? <p>Create the table with the property <pre><code>CREATE TABLE all_tx (\n    ....\n)\n  WITH(\n    'kafka.consumer.isolation-level'='read-uncommitted',\n    ...)\n</code></pre></p> <p>For exactly once the parameter is set to <code>'read-committed'</code>.</p> How to pass comment from table creation to schema definition doc field? <p>Using <code>COMMENT</code> at the column level will populate the doc field. Here is an example: <pre><code>create table person (\n    lastname STRING COMMENT 'the last name of the person',\n    firstname STRING COMMENT 'the first name of the person',\n    dob STRING COMMENT 'the date of birth'\n)\n</code></pre></p> <p>And the schema has the doc populated for each field</p> <pre><code>{\n\"fields\": [\n    {\n    \"default\": null,\n    \"doc\": \"the last name of the person\",\n    \"name\": \"lastname\",\n    \"type\": [\n        \"null\",\n        \"string\"\n    ]\n    },\n    {\n    \"default\": null,\n    \"doc\": \"the first name of the person\",\n    \"name\": \"firstname\",\n    \"type\": [\n        \"null\",\n        \"string\"\n    ]\n    },\n    {\n    \"default\": null,\n    \"doc\": \"the date of birth\",\n    \"name\": \"dob\",\n    \"type\": [\n        \"null\",\n        \"string\"\n    ]\n    }\n],\n\"name\": \"person_value\",\n\"namespace\": \"org.apache.flink.avro.generated.record\",\n\"type\": \"record\"\n}\n</code></pre> How to generate test data to Confluent Cloud Flink? <p>Use Kafka Connector with DataGen. Those connector exists with a lot of different pre-defined model. Also it is possible to define custom Avro schema and then use predicates to generate data. There is a Produce sample data quick start tutorial from the Confluent Cloud home page. See also this readme.</p> <p>The Shift_left tool has also a test harness to generate synthetic data taking into account the SQL content.</p>"},{"location":"coding/flink-sql-1/#flink-sql-oss-confluent-platform-for-flink","title":"Flink SQL OSS - Confluent Platform for Flink","text":"Create a table with csv file as persistence - Flink OSS <p>We need to use the file system connector.</p> <pre><code>create table user (\n    'user_id' VARCHAR(250),\n    'name' VARCHAR(50)\n) partitioned by ('used-id')\nWITH (\n    'format' = 'json', -- other format are: csv, parquet\n    'connector' = 'filesystem',\n    'path' = '/tmp/users'\n);\n</code></pre> How to consume from a Kafka topic to a SQL table? -- Flink OSS <p>On Confluent Cloud for Flink, there are already tables created for each topic. For local Flink we need to create table with column definitions that maps to attributes of the record. The <code>From</code> right operand proposes the list of topic/table for the catalog and database selected. For Flink OSS or Confluent Platform for Flink the <code>WITH</code> statement helps to specify the source topic.</p> <pre><code>select .... from TableName \nWITH (\n    'connector' = 'kafka',\n    'topic' = 'flight_schedules',\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'properties.group.id' = 'fs_grp',\n    'scan.startup.mode' = 'earliest-offset',\n)\n</code></pre> <p>For Avro and schema registry with open source Flink. See the tool extract_sql_from_avro.py to query Confluent Schema Registry and build the matching SQL to create a table connected to the topic using this schema.</p> <pre><code>CREATE TABLE shoe_customers (\n    id STRING,\n    first_name STRING,\n    last_name STRING,\n    email STRING,\n    phone STRING,\n    street_address STRING,\n    state STRING,\n    zip_code STRING,\n    country STRING,\n    country_code STRING\n) WITH (\n    'connector' = 'kafka',\n    'topic' = 'shoe_customers',\n    'properties.bootstrap.servers' = 'broker:29092',\n    'scan.startup.mode' = 'earliest-offset',\n    'key.format' = 'raw',\n    'key.fields' = 'id',\n    'value.format' = 'avro-confluent',\n    'properties.group.id' = 'flink-sql-consumer',\n    'value.fields-include' = 'ALL',\n    'value.avro-confluent.url' = 'http://schema-registry:8081'\n);\n</code></pre> How to load data from a csv file using filesystem connector using SQL - Flink OSS <p>Enter the following statement in a SQL client session:</p> <p><pre><code>SET execution.runtime-mode=BATCH;\n</code></pre> Create a table from the content of the file, mounted inside the container or accessible on local file system:</p> <pre><code>CREATE TABLE employee_info (\n    emp_id INT,\n    name VARCHAR,\n    dept_id INT\n) WITH ( \n    'connector' = 'filesystem',\n    'path' = '/home/flink-sql-demos/00-basic-sql/data/employees.csv',\n    'format' = 'csv'\n);\n</code></pre> <p>Show tables and list some elements within the table.</p> <pre><code>SHOW TABLES;\n\nSELECT * from employee_info WHERE dept_id = 101;\n</code></pre> <p>Show how the table is created:</p> <pre><code>show create table orders;\n</code></pre> <p>See complete example in the readme</p> How to generate data using Flink Faker? (Flink OSS) <p>Create at table with records generated with Flink faker connector using the DataFaker expressions.. </p> <pre><code>CREATE TABLE `bounded_pageviews` (\n  `url` STRING,\n  `user_id` STRING,\n  `browser` STRING,\n  `ts` TIMESTAMP(3)\n)\nWITH (\n  'connector' = 'faker',\n  'number-of-rows' = '500',  -- or null for infinite\n  'rows-per-second' = '100',\n  'fields.url.expression' = '/#{GreekPhilosopher.name}.html',\n  'fields.user_id.expression' = '#{numerify ''user_##''}',\n  'fields.browser.expression' = '#{Options.option ''chrome'', ''firefox'', ''safari'')}',\n  'fields.ts.expression' =  '#{date.past ''5'',''1'',''SECONDS''}'\n);\n</code></pre> <p>Attention on Confluent Cloud, this is a connector, there is no matching Kafka Topic created. The faker will run as part of the platform and use resources. Better to drop the table once demonstrations are done.</p> Deeper dive into Faker Connector <p>flink-faker is a specialized table source that bridges Apache Flink\u2019s SQL engine with the Java DataFaker. It acts as a ScanTableSource. It creates an internal generator that produces rows on-the-fly. Each column is mapped to a \"Faker expression\" using the syntax #{className.methodName 'parameter'}. By default, it generates an infinite stream of data. You can make it \"bounded\" (stop after a certain number of rows) for batch-style testing.</p> Field operation Description Time-based generation Give a strings that Flink can parse into TIMESTAMP(3): 'fields.ts.expression' = '#{date.past ''15'',''SECONDS''}' categorical values 'fields.status.expression' = '#{Options.option ''PENDING'',''SHIPPED'',''CANCELLED''}' Regex specific pattern 'fields.zip_code.expression' = '#{regexify ''[0-9]{5}-[0-9]{4}''}' Nested json via row ... details ROW ) WITH ( 'connector' = 'faker', 'fields.details.item_name.expression' = '#{commerce.productName}', 'fields.details.price.expression' = '#{commerce.price}' <p>See This test sql for the transaction processing</p> Generate data with DataGen for Flink OSS <p>Use DataGen to do in-memory data generation and the new feature in product how to guide documentation.</p>"},{"location":"coding/flink-sql-1/#applying-to-a-medallion-architecture","title":"Applying to a Medallion Architecture","text":"<p>The current approach can be used for Flink pipeline processing:</p> Table type Goals Parameters Raw table  non debezium format Get the raw data from CDC or outbox pattern cleanup.policy = 'delete', changelog.mode = 'append', value.format = 'avro-registry'... Raw table debezium format Same goals cleanup.policy = 'delete', changelog.mode = 'retract' (retract is the default for Debezium connector),  value.format = 'avro-debezium-registry' Sources Deduplicate and keep last record per key cleanup.policy = 'delete', changelog.mode = 'upsert' Intermediates Enrichment, transformation cleanup.policy = 'delete',  changelog.mode = 'upsert' Sink tables: Facts, Dimensions, Views Create star schema elements cleanup.policy = 'compact', changelog.mode = 'retract' or 'upsert'"},{"location":"coding/flink-sql-1/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Confluent product document - changelog </li> <li>Flink SQL Secrets: Mastering the Art of Changelog Event Out-of-Orderness</li> <li>Resolving: Primary key differs from derived upsert key</li> </ul>"},{"location":"coding/flink-sql-1/#confluent-cloud-flink-table-creation-specific","title":"Confluent Cloud Flink table creation specific","text":"<p>See the product documentation with some specificities, like source and sink tables are directly mapped to Kafka Topics. The <code>$rowtime</code> TIMESTAMP_LTZ(3) NOT NULL is provided as a system column.</p> <ul> <li> <p>For each topic there is an inferred table created. The catalog is the Confluent environment and the Kafka cluster is the database. We can use the ALTER TABLE statement to evolve schemas for those inferred tables.</p> </li> <li> <p>A table by default is mapped to a topic with 6 partitions, and the changelog being append. Primary key leads to an implicit DISTRIBUTED BY(k), and value and key schemas are created in Schema Registry. It is possible to create table with primary key and append mode, while by default it is a upsert mode. </p> <pre><code>CREATE TABLE telemetries (\n    device_id INT PRIMARY KEY NOT ENFORCED, \n    geolocation STRING, metric BIGINT,\n    ts TIMESTAMP_LTZ(3) NOT NULL METADATA FROM 'timestamp')\nDISTRIBUTED INTO 4 BUCKETS\nWITH ('changelog.mode' = 'append');\n</code></pre> </li> </ul> <p>The statement above also creates a metadata column for writing a Kafka message timestamp. This timestamp will not be defined in the schema registry. Compared to <code>$rowtime</code> which is declared as a <code>METADATA VIRTUAL</code> column, <code>ts</code> is selected in a <code>SELECT *</code> and is writable.</p> <ul> <li> <p>When the primary key is specified, then it will not be part of the value schema, except if we specify (using <code>value.fields-include' = 'all'</code>) that the value contains the full table schema. The payload of k is stored twice in Kafka message:</p> <pre><code>CREATE TABLE telemetries (k INT, v STRING)\nDISTRIBUTED BY (k)\nWITH ('value.fields-include' = 'all');\n</code></pre> </li> <li> <p>If the key is a string, it may make sense to do not have a schema for the key in this case declare (the key columns are determined by the DISTRIBUTED BY clause): This does not work if the key name is not <code>key</code>.</p> <pre><code>CREATE TABLE telemetries (device_id STRING, metric BIGINT)\nDISTRIBUTED BY (key)\nWITH ('key.format' = 'raw');\n</code></pre> </li> <li> <p>To keep the record in the topic forever add this <code>kafka.retention.time' = '0'</code> as options in the WITH. The supported units are:     <pre><code>\"d\", \"day\", \"h\", \"hour\", \"m\", \"min\", \"minute\", \"ms\", \"milli\", \"millisecond\",\n\"micro\", \"microsecond\", \"ns\", \"nano\", \"nanosecond\"\n</code></pre></p> </li> </ul> CREATE TABLE in Confluent Cloud for Flink <p>The table creation creates topic and -key, -value schemas in the Schema Registry in the same environment as the compute pool in which the query is run. The <code>connector</code> is automatically set to <code>confluent</code>.  The non null and nullable columns are translated as the following avro fields: <pre><code>  \"fields\": [\n        {\n        \"name\": \"customer_id\",\n        \"type\": \"string\"\n        },\n        {\n        \"default\": null,\n        \"name\": \"first_name\",\n        \"type\": [\n            \"null\",\n            \"string\"\n        ]\n        }\n  ]\n</code></pre></p> <p>DATE as:</p> <pre><code>{\n  \"default\": null,\n  \"name\": \"registration_date\",\n  \"type\": [\n    \"null\",\n    {\n      \"logicalType\": \"local-timestamp-millis\",\n      \"type\": \"long\"\n    }\n  ]\n}\n</code></pre>"},{"location":"coding/flink-sql-1/#analyzing-table","title":"Analyzing Table","text":""},{"location":"coding/flink-sql-1/#understand-the-type-of-attribute-or-get-the-table-structure-with-metadata","title":"Understand the type of attribute or get the table structure with metadata","text":"<pre><code>show create table 'tablename';\n-- for a specific attribute\nselect typeof(column_name) from table_name limit 1;\n</code></pre> <p>Flink SQL planner performs type checking. Assessing the type of inferred table is helpful, specially around timestamp. See Data type mapping documentation.</p>"},{"location":"coding/flink-sql-2/","title":"Flink SQL DML","text":"Updates <p>Created 10/24, Updated 12/20/24 Revised 12/06/24</p> <p>This chapter offers continue on the best practices for implementing Flink SQL solutions, for Data Manipulation Language queries. DML is used to define statements which modify the data and don\u2019t change the metadata.</p>"},{"location":"coding/flink-sql-2/#sources-of-information","title":"Sources Of Information","text":"<ul> <li>Confluent SQL documentation for DML samples </li> <li>Apache Flink SQL</li> <li>Confluent Developer Flink tutorials</li> <li>The Flink built-in system functions.</li> </ul>"},{"location":"coding/flink-sql-2/#common-patterns","title":"Common Patterns","text":"<p>This is important to recall that a select applies to a stream of record so the results will change at each new record. A query as below will show the last top 10 orders, and when a new record arrives this list is updated. </p> <pre><code>select * from `examples`.`marketplace`.`orders` order by $rowtime limit 10;\n</code></pre> What are the different SQL execution modes? (OSS) <p>Using previous table it is possible to count the elements in the table using:</p> <pre><code>select count(*) AS `count` from pageviews;\n</code></pre> <p>and we get different behaviors depending of the execution mode:</p> <pre><code>set 'execution.runtime-mode' = 'batch';\n# default one\nset 'execution.runtime-mode' = 'streaming';\n\nset 'sql-client.execution.result-mode' = 'table';\n</code></pre> <p>In changelog mode, the SQL Client doesn't just update the count in place, but instead displays each message in the stream of updates it's receiving from the Flink SQL runtime.</p> <pre><code>set 'sql-client.execution.result-mode' = 'changelog';\n</code></pre> <p></p> <p>See this lab in changelog mode and this section in SQL concepts chapter.</p>"},{"location":"coding/flink-sql-2/#filtering","title":"Filtering","text":"<ul> <li>Start by looking at this Confluent tutorial or The Apache Flink doc section</li> <li>SELECT ... FROM ... WHERE ... consists of column projections or filters and are stateless. Except if the output table has a a <code>retract</code> changelog while input is <code>upsert</code>, the sink will have a <code>changelog materializer</code> (see section below.)</li> <li>SELECT DISTINCT to remove duplicate rows. Which leads to keep state for each row.</li> </ul> How to filter out records? <p>using the WHERE clause</p> <pre><code>select * from flight_events where status = 'cancelled';\n</code></pre> <p>Count the number of events related to a cancelled flight (need to use one of the selected field as grouping key):</p> <pre><code>select fight_id, count(*) as cancelled_fl from FlightEvents where status = 'cancelled' group by flight_id;\n</code></pre> <p>Recall that this results produces a dynamic table.</p> HAVING to filter after aggregation <p>The <code>HAVING</code> clause is used to filter results after the aggregation (i.e., like <code>GROUP BY</code>). It is similar to the <code>WHERE</code> clause, but while <code>WHERE</code> filters rows before aggregation, <code>HAVING</code> filters rows after aggregation. <pre><code>SELECT url as product_page, \n    COUNT(click_id) as num_of_times_viewed, \n    COUNT(DISTINCT user_id) as num_of_users,\n    AVG(view_time) as avg_view_time,\n    MAX(view_time) as max_view_time\nFROM clicks\nGROUP BY url\nHAVING COUNT(click_id)  &gt; 2;\n</code></pre></p> How to combine records from multiple tables (UNION)? <p>When the two tables has the same number of columns of the same type, then we can combine them:</p> <pre><code>SELECT * FROM T1\nUNION ALL\nSELECT * FROM T2;\n</code></pre> <p>See product documentation on Union. Remember that UNION will apply distinct, and avoid duplicate, while UNION ALL will generate duplicates. </p> How to filter row that has column content not matching a regular expression? <p>Use REGEX</p> <pre><code>WITH filtered_data AS (\nSELECT *,\n    CASE \n        WHEN NOT REGEXP(user_agent, '.*Opera/.*') \n        THEN TRUE  -- Keep this row\n        ELSE FALSE -- Filter out rows that match \"Opera/\"\n    END AS keep_row\nFROM examples.marketplace.clicks\n)\n\nSELECT * \nFROM filtered_data\nWHERE keep_row = TRUE;\n</code></pre> Navigate a hierarchical structure in a table <p>The unique table has node and ancestors representation. Suppose the graph represents a Procedure at the highest level, then an Operation, then a Phase and a Phase Step at the level 4. In the Procedures table we can have rows like:</p> <pre><code>id, parent_ids, depth, information\n'id_1', [], 0 , 'procedure 1'\n'id_2', ['id_1'], 1 , 'operation 1'\n'id_3', ['id_1','id_2'], 2 , 'phase 1'\n'id_4', ['id_1','id_2','id_3'], 3 , 'phase_step 1'\n'id_5', ['id_1','id_2','id_3'], 3 , 'phase_step 2'\n</code></pre> <p>Suppose we want to extract the matching  procedure_id, operation_id, phase_id, phase_step_id like <pre><code>id, procedure_id, operation_id, phase_id, phase_step_id, information\n'id_1', 'id_1', NULL, NULL, NULL, 'procedure 1'\n'id_2', 'id_1', 'id_2', NULL, NULL, 'operation 1'\n'id_3', 'id_1', 'id_2', 'id_3', NULL, 'phase 1'\n'id_4', 'id_1', 'id_2', 'id_3', 'id_4', 'phase_step 1'\n'id_5', 'id_1', 'id_2', 'id_3', 'id_5', 'phase_step 2'\n</code></pre></p> <p>if the depth is 3, then the response should have all ids populated, if 0 only the top level is returned.</p> <pre><code>with `procedures` as (\n    select 'id_1' as id, array[''] as parentIds, 0 as `depth` , 'procedure 1' as info\n    UNION ALL\n    select 'id_2' as id, array['id_1'] as parentIds, 1 as `depth` , 'operation 1' as info\n    UNION ALL\n    select 'id_3' as id, array['id_1','id_2'] as parentIds, 2 as `depth`, 'phase 1' as info\n    UNION ALL\n    select 'id_4' as id, array['id_1','id_2','id_3'] as parentIds, 3 as `depth`, 'phase_step 1' as info\n    UNION ALL\n    select 'id_5' as id, array['id_1','id_2','id_3'] as parentIds, 3 as `depth`, 'phase_step 2' as info\n    )\nselect \n    id, \n    parent_id, \n    case when `depth` = 3 then id end as phase_step_id,\n    case when `depth` = 2 then id end as phase_id,\n    case when `depth` = 1 then id end as operation_id,\n    case when `depth` = 0 then id end as procedure_id,\n    info from `procedures` cross join unnest(parentIds) as ids(parent_id)\n</code></pre>"},{"location":"coding/flink-sql-2/#deduplication","title":"Deduplication","text":"<p>Deduplication will occur on <code>upsert</code> table with primary key: the last records per <code>$rowtime</code> or other timestamp will be kept. When the source table is in <code>append</code> mode, the approach is to use the ROW_NUMBER() combined with OVER():</p> <pre><code>CREATE TABLE unique_clicks\nAS SELECT ip_address, url, TO_TIMESTAMP(FROM_UNIXTIME(click_ts_raw)) as click_timestamp\n    FROM (\n        SELECT *,\n        ROW_NUMBER() OVER ( PARTITION BY ip_address ORDER BY TO_TIMESTAMP(FROM_UNIXTIME(click_ts_raw)) ASC) as rownum \n        FROM clicks\n        )\n    WHERE rownum = 1;\n</code></pre> <ul> <li>The query is designed to identify and persist only the earliest record. Once the record is written to the table_deduped any subsequent events pertaining to the same <code>ip_address</code> are effectively discarded by the <code>WHERE rownum = 1</code> filter.</li> <li>ROW_NUMBER() Assigns an unique, sequential number to each row. It is part of the Top-N queries pattern.</li> <li>Use OVER aggregation to compute agg value for every input window clause and a filter condition to express a Top-N query. Combined with PARTITION BY clause, Flink supports a per group Top-N. </li> <li>The internal CTE add a row_num for each </li> <li>The subsequent <code>WHERE rownum = 1</code> clause filters the results to retain only the very first event observed for each unique <code>ip_address</code> based on its timestamp.</li> <li>The created table is an append table. There is no mechanism within this query to generate update or delete operations for records that have already been processed. Even if the underlying <code>clicks</code> table has a primary key defined, the transformation applied here dictates that the <code>unique_clicks</code> table will only ever grow by appending new, unique <code>ip_address</code> entries.</li> <li>If the sorting was DESC when a new record arrive a retraction/update record is emitted. </li> <li>If after deduplication, the upsert sink needs to have the same key as the partition keys used. </li> </ul> <p>See this example in the Confluent product documentation which demonstrates there is no duplicate in the query result with <code>select * from dedup_table;</code> returns 8 messages. Same in the Kafka topic there are 8 messages . </p> <p>But it does not demonstrate the last message is kept. The deduplication sample demonstrates that an upsert table is already removing duplicates, and keep the last record per key. </p> <ul> <li>Confluent Cloud has limitations: the order by can only be timestamp in ASC mode.</li> <li>Example of deduplication on OSS Flink where order can apply to any column type. <pre><code>select *  FROM (\n    SELECT \n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY emp_id \n            ORDER BY emp_id DESC\n        ) AS row_num\n    FROM employees\n) WHERE row_num = 1\n</code></pre></li> </ul>"},{"location":"coding/flink-sql-2/#transformation","title":"Transformation","text":"<p>Some important resources:</p> <ul> <li>Transform a Topic with Confluent Cloud for Apache Flink - product doc to demonstrate topic transformation from the Data Portal -&gt; Actions -&gt; Transform Topic, which creates a CTAS Flink SQL statment and automatically deploys it to the configured computepool. This is very efficient to do avro to json, field rename, and primary key reallocation.</li> </ul> How to transform a field representing epoch to a timestamp? <p>epoch is a BIGINT.</p> <pre><code> TO_TIMESTAMP(FROM_UNIXTIME(click_ts_epoch)) as click_ts\n</code></pre> How to change a date string to a timestamp? <pre><code>TO_TIMESTAMP('2024-11-20 12:34:568Z'),\n</code></pre> <pre><code>to_timestamp(transaction_date, 'yyyy-MM-dd HH:mm:ss') as tx_date, -- bigint\n</code></pre> <p>See all the date and time functions.</p> How to compare a date field with current system time? <pre><code>WHEN TIMESTAMPDIFF(day, event.event_launch_date, now()) &gt; 120 THEN ...\n</code></pre> <p>If the target table is set with a <code>changelog mode = upsert</code>, the use of the now() function is problematic because the exact execution time is not deterministic for each row. So the above statement works only for <code>append</code> mode.</p> How to extract the number of DAY, from a date field and now? <p>The only diff is on timestamp. So need to first to cast the DATE column to a ts, and then use CURRENT_DATE and the DAY dimension. See the supported dimensions (SECOND, MINUTE, HOUR, DAY, MONTH, or YEAR) <pre><code> TIMESTAMPDIFF(DAY, CAST(created_date AS TIMESTAMP_LTZ(3)), CURRENT_DATE) as days_since_launch,\n</code></pre></p> How to access element of an array of rows? <p>The table has a column that is an array of rows.  <pre><code> CREATE TABLE my_table (\n    key_col INT,\n    nested_data ARRAY&lt;ROW&lt;id INT, name STRING&gt;&gt;\n) WITH (...)\n</code></pre></p> <p>To create one record per row within the array, so exploding the array, use CROSS JOIN UNNEST: <pre><code> SELECT\n    t.key_col,\n    unnested_row.id,\n    unnested_row.name\nFROM\n    my_table AS t\nCROSS JOIN UNNEST(t.nested_data) AS unnested_row;\n</code></pre></p> <p>Each row in the nested_data array will be a row in the output table with the matching key_col. </p> How to access json data from a string column being a json object? <p>Use json_query function in the select.</p> <pre><code> json_query(task.object_state, '$.dueDate') AS due_date,\n</code></pre> <p>Use <code>json_value()</code> instead if the column content is a dict or json {}.</p> How to transform a json array column (named data) into an array to then generate n rows? <p>Returning an array from a json string: <pre><code>json_query(`data`, '$' RETURNING ARRAY&lt;STRING&gt;) as anewcolumn\n</code></pre></p> <p>To create as many rows as there are elements in the nested array: <pre><code>SELECT existing_column, anewcolumn from table_name\ncross join unnest (json_query(`data`, '$' RETURNING ARRAY&lt;STRING&gt;)) as t(anewcolumn)\n</code></pre></p> <p>UNNEST returns a new row for each element in the array See multiset expansion doc</p> How to implement the equivalent of SQL explode? <p>SQL EXPLODE creates a row for each element in the array or map, and ignore null or empty values in array. <pre><code>SELECT explode(col1) from values (array(10,20)), (null)\n</code></pre></p> <p>SQL has also EXPLODE_OUTER, which returns all values in array including null or empty.</p> <p>To translate this to Flink SQL we can use MAP_ENTRIES and MAP_FROM_ARRAYS. MAP_ENTRIES returns an array of all entries in the given map. While MAP_FROM_ARRAYS returns a map created from an arrays of keys and values. <pre><code>select map_entries(map_from_arrays())\n</code></pre></p> How to use conditional functions? <p>Flink has built-in conditional functions (See also Confluent support) and specially the CASE WHEN:</p> <pre><code>SELECT \n    *\n    FROM `stocks`\n    WHERE  \n    CASE \n        WHEN price &gt; 200 THEN 'high'\n        WHEN price &lt;=200 AND price &gt; 150 THEN 'medium'\n        ELSE 'low'\n    END;\n</code></pre> How to mask a field? <p>Create a new table from the existing one, and then use REGEXP_REPLACE to mask an existing attribute</p> <pre><code>create table users_msk like users;\nINSERT INTO users_msk SELECT ..., REGEXP_REPLACE(credit_card,'(\\w)','*') as credit_card FROM users;\n</code></pre>"},{"location":"coding/flink-sql-2/#statement-set","title":"Statement Set","text":"<p>The benefit of bundling statements in a single set is to reduce the repeated read from the source for each Insert. A single read from the source is executed and shared with all downstream INSERTS.</p> <p>Do not use <code>Statement Set</code> when the source are different for all statements within the Statement Sets. Take into account that within the statement set if one statement fails, then all queries fail. The state is shared by all the statements within Statement set, so one stateful query can impact all other statements.</p> How to manage late message to be sent to a DLQ using Statement Set? <p>First, create a DLQ table like <code>late_orders</code> based on the order table:</p> <pre><code>    create table late_orders\n    with (\n        'connector'= ''\n    ) \n    LIKE orders (EXCLUDING OPTIONS)\n</code></pre> <p>Groups the main stream processing and the late arrival processing in a statement set:</p> <pre><code>EXECUTE STATEMENT SET\nBEGIN\n    INSERT INTO late_orders SELECT from orders WHERE `$rowtime` &lt; CURRENT_WATERMARK(`$rowtime`);\n    INSERT INTO order_counts -- the sink table\n        SELECT window_time, COUNT(*) as cnt\n        FROM TABLE(TUMBLE(TABLE orders DESCRIPTOR(`$rowtime`), INTERVAL '1' MINUTE))\n        GROUP BY window_start, window_end, window_time\nEND\n</code></pre>"},{"location":"coding/flink-sql-2/#stateful-aggregations","title":"Stateful aggregations","text":"<p>An aggregate function computes a single result from multiple input rows.</p>"},{"location":"coding/flink-sql-2/#group-by","title":"Group BY","text":"<p>Classical SQL grouping of records, but with Streaming the state may grow infinitely. The size will depend of the # of groups and the amount of data to keep per group. <code>group by</code> generates upsert events as it manages key-value and repartitions data.</p> <pre><code>EXPLAIN \nSELECT \n  account_number,\n  transaction_type,\n  SUM(amount) \n  FROM `transactions` \n  where transaction_type = 'withdrawal' \n  GROUP BY account_number,  transaction_type\nHAVING SUM(amount) &gt; 5000\n</code></pre> <p>The physical plan looks like <pre><code>== Physical Plan ==\n\nStreamSink [6]\n  +- StreamCalc [5]\n    +- StreamGroupAggregate [4]\n      +- StreamExchange [3]\n        +- StreamCalc [2]\n          +- StreamTableSourceScan [1]\n\n== Physical Details ==\n\n[1] StreamTableSourceScan\nTable: `j9r-env`.`j9r-kafka`.`transactions`\nPrimary key: (txn_id)\nChangelog mode: append\nUpsert key: (txn_id)\nState size: low\nStartup mode: earliest-offset\nKey format: avro-registry\nKey registry schemas: (:.:transactions/100220)\nValue format: avro-registry\nValue registry schemas: (:.:transactions/100219)\n\n[4] StreamGroupAggregate\nChangelog mode: retract\nUpsert key: (account_number,transaction_type)\nState size: medium\nState TTL: never\n\n[5] StreamCalc\nChangelog mode: retract\nUpsert key: (account_number,transaction_type)\n\n[6] StreamSink\nTable: Foreground\nChangelog mode: retract\nUpsert key: (account_number,transaction_type)\nState size: low\n</code></pre></p>"},{"location":"coding/flink-sql-2/#distinct","title":"Distinct","text":"<p>Remove duplicate before doing the aggregation:</p> <pre><code>ARRAY_AGG(DISTINCT user_name) as persons\n</code></pre>"},{"location":"coding/flink-sql-2/#array_agg","title":"ARRAY_AGG","text":"<p>Array aggregation refers to the process of combining multiple data elements into a single array structure. The goal is to regroup multiple data points together. Array aggregation plays a pivotal role in data processing due to its ability to manage and analyze vast amounts of information effectively</p> <p>This Flink ARRAY_AGG, aggregate function is a common function used in a lot of record transformation implementations. The function returns an array with elements coming from multiple rows from the input table. </p> <p>Let start by a simple array indexing (the index is between 1 to n_element). Below, the <code>values array</code> creates test data into a memory table aliased a <code>T</code> with a column named <code>array_field</code>:</p> <pre><code>SELECT array_field[4] FROM ((VALUES ARRAY[5,4,3,2,1])) AS T(array_field)\n</code></pre> <p>The following SQL, is creating a view with an array of aggregates, which in this case, is concatenating the urls for each user_id over a 1 minute tumble window.</p> <pre><code>CREATE VIEW visited_pages_per_minute AS \nSELECT \n    window_time,\n    user_id, \n    ARRAY_AGG(url) AS urls\nFROM TABLE(TUMBLE(TABLE `examples.marketplace.clicks`, DESCRIPTOR(`$rowtime`), INTERVAL '1' MINUTE))\nGROUP BY window_start, window_end, window_time, user_id;\n-- once the view is created\nSELECT * from visited_pages_per_minute;\n\n-- it is possible to expand an array into multiple rows using cross join unnest\n\nSELECT v.window_time, v.user_id, u.url FROM visited_pages_per_minute AS v\nCROSS JOIN UNNEST(v.urls) AS u(url)\n</code></pre> <p>One thing important is that new clicks for the same user_id with new url, will create a new output record with the aggregated array. See cc-array-agg study.</p> <p>To optimize the processing it is recommended to deduplicate and/or filter records before computing the aggregation, and using Flink View is a good example:</p> <pre><code>create view suites_versioned as \nselect  suite_id, suite_name, asset_id, asset_name, asset_price_min, asset_price_max, ts_ltz\nfrom (\n    select *,\n       ROW_NUMBER() OVER (PARTITION BY suite_id, asset_id ORDER BY ts_ltz DESC) as rn\n        from suites\n    ) where rn = 1;\n</code></pre> <p>Then the array aggregation:</p> <pre><code>select \n    suite_id,\n    ARRAY_AGG(ROW (asset_id, asset_name,  ROW(asset_price_min, asset_price_max))) as asset_data, \n    max(ts_ltz) as ts_ltz\nfrom suites_versioned group by suite_id;\n</code></pre> <p>As seen in example above the type within the array can be a row and sub-row. See SQL examples</p>"},{"location":"coding/flink-sql-2/#over","title":"OVER","text":"<p>OVER aggregations computes an aggregated value for every input row over a range of ordered rows. It does not reduce the number of resulting rows, as GROUP BY does, but produces one result for every input row. </p> <p>OVER specifies the time window over which the aggregation is performed. A classical example is to get a moving sum or average: the number of orders in the last 10 seconds: </p> <pre><code>SELECT \n    order_id,\n    customer_id,\n    `$rowtime`,\n    SUM(price) OVER w AS total_price_ten_secs, \n    COUNT(*) OVER w AS total_orders_ten_secs\nFROM `examples`.`marketplace`.`orders`\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    RANGE BETWEEN INTERVAL '10' SECONDS PRECEDING AND CURRENT ROW\n)\n</code></pre> <p>The source topic needs to be append mode as over window operaton does not supporting retraction/update semantic.</p> <p>The changelog mode, of the output of OVER is <code>append</code>. This is helpful when we need to act on each input row, but consider some time interval. </p> <p>To get the order exceeding some limits for the first time and then when the computed aggregates go below other limits. LAG</p> <pre><code>-- compute the total price and # of orders for a period of 10s for each customer\nWITH orders_ten_secs AS ( \nSELECT \n    order_id,\n    customer_id,\n    `$rowtime`,\n    SUM(price) OVER w AS total_price_ten_secs, \n    COUNT(*) OVER w AS total_orders_ten_secs\nFROM `examples`.`marketplace`.`orders`\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    RANGE BETWEEN INTERVAL '10' SECONDS PRECEDING AND CURRENT ROW\n    )\n),\n-- get previous orders and current order per customer\norders_ten_secs_with_lag AS (\nSELECT \n    *,\n    LAG(total_price_ten_secs, 1) OVER w AS total_price_ten_secs_lag, \n    LAG(total_orders_ten_secs, 1) OVER w AS total_orders_ten_secs_lag\nFROM orders_ten_secs\nWINDOW w AS (\n    PARTITION BY customer_id\n    ORDER BY `$rowtime`\n    )\n-- Filter orders when the order price and number of orders were above some limits for previous or current order aggregates\n)\nSELECT customer_id, 'BLOCK' AS action, `$rowtime` AS updated_at \nFROM orders_ten_secs_with_lag \nWHERE \n    (total_price_ten_secs &gt; 300 AND total_price_ten_secs_lag &lt;= 300) OR\n    (total_orders_ten_secs &gt; 5 AND total_orders_ten_secs_lag &lt;= 5)\nUNION ALL \nSELECT customer_id, 'UNBLOCK' AS action, `$rowtime` AS updated_at \nFROM orders_ten_secs_with_lag \nWHERE \n    (total_price_ten_secs &lt;= 300 AND total_price_ten_secs_lag &gt; 300) OR\n    (total_orders_ten_secs &lt;= 5 AND total_orders_ten_secs_lag &gt; 5);\n</code></pre> When and how to use custom watermark? <p>Developer should use their own watermark strategy when there are not a lot of records per topic/partition, there is a need for a large watermark delay, and need to use another timestamp.  The default watermark strategy in SOUCE_WATERMARK(), a watermark defined by the source. The common strategy used is the <code>maximim-out-of-orderness</code> to allow messages arriving later to be part of the window, to ensure more accurate results, as a tradeoff of latency. It can be defined using:</p> <pre><code>ALTER TABLE &lt;table_name&gt; MODIFY WATERMARK for `$rowtime` as `$rowtime` - INTERVAL '20' SECONDS\n</code></pre> <p>The minimum out-of-orderness is 50ms and can be set up to 7 days. See Confluent documentation. </p>"},{"location":"coding/flink-sql-2/#joins","title":"Joins","text":"<p>When doing a join in a database, the result reflects the state of the join at the time we execute the query. In streaming, as both side of a join receive new rows, both side of joins need to continuously change. This is a continuous query on dynamic tables, where the engine needs to keep a lot of state: each row of each table. </p> <p>This is the common join we do between two tables: </p> <pre><code>SELECT t.amount, t.order_type, s.name, s.opening_value FROM transactions t\nLEFT JOIN stocks s\nON t.stockid = s.id\n</code></pre> <p>On the left side, the fact table, has high velocity of changes, but the events are immutables, while on the right side, the dimension, the new records arrive slowly.</p> <p>When doing a join, Flink needs to fully materialize both the right and left of the join tables in state, which may cost a lot of memory, because if a row in the left-hand table (LHT), also named the probe side, is updated, the operator needs to emit an updated match for all matching rows in the right-hand table (RHT) or build side. The cardinality of right side will be mostly bounded at a given point of time, but the left side may vary a lot. A join emits matching rows to downstream operator.</p> <p>The key points to keep in mind are:</p> <ul> <li>Regular joins typically produce a full cross-product of all matching records. However, in streaming scenarios, this behavior is often undesirable, for example if you want to enrich an event with additional information.</li> <li>The order of joins is important, try to get the first join done on table with the lowest update frequency.</li> <li>Cross join makes the query fails</li> <li>When the RHS is an upsert table, the result will be upsert too. Which means a result will be re-emitted if the RHS change. To avoid that we need to take the reference data, RHS, in effect at time of the event on LHS. The result is becoming time-versioned.</li> </ul>"},{"location":"coding/flink-sql-2/#temporal-join","title":"Temporal Join","text":"<p>A temporal join joins one table with another table that is updated over time. This join is made possible by linking both tables using a time attribute, which allows the join to consider the historical changes in the table. </p> <pre><code>```sql\ninsert into enriched_transactions\nSELECT t.amount, t.order_type, s.name, s.opening_value FROM transactions t\nLEFT JOIN stocks s FOR SYSTEM_TIME AS OF t.purchase_ts\nON t.stockid = s.id\n```\n</code></pre> <ul> <li>The above query is a Temporal join. Temporal joins help to reduce the state size, as we need to keep only recent records on both side. The time will be linked to the watermark progress. If the opening_value of the stock change over time, it will not trigger update to the previously generated enriched transactions.  </li> <li> <p>Temporal JOINs must include all of the PRIMARY KEY columns of the versioned (right-side) table: the ON conditions need to include exactly the same primary key columns:     <code>sql     create table dim_rule_config(        tenant_id STRING NOT NULL,        rule_id BIGINT NOT NULL,        rule_name STRING NOT NULL,        parameter_id BIGINT NOT NULL,        paremeter_value BIGINT,        primary key (tenant_id, rule_id, parameter_id) not enforced     )     # joining in a separate DML     ...     from extended_sensors s     left join dim_rule_config for system as of s.event_ts as rule     ON  s.tenant_id = rule.tenant_id        AND s.rule_id = rule.rule_id         AND s.parameter_id = rule.parameter_id</code></p> <p>The extended_sensors is a CTE to add specific constant columns, as the ON conditions need to apply to columns on on the LHS. So to be able to search for specific rule id and parameter id, the approach is to create a CTE: <pre><code>with extended_sensors (\n    select\n    ... all sensors attributes\n    10 as rule_id,\n    1 as parameter_id\n    from sensors\n) select ... \n</code></pre></p> <p>See also this code sample for an example of rule based control with temporal joins and contant columns.</p> </li> <li> <p>When the LHS of the temporal join is upsert then the sink table needs to be retract. While if it is an append changelog then the sink should be append too.</p> </li> <li>INNER JOIN is a cartesian product.</li> <li>OUTER joins like left, right or full, may generate records with empty columns for non-matching row.</li> </ul>"},{"location":"coding/flink-sql-2/#interval-join","title":"Interval Join","text":"<ul> <li> <p>Interval joins are particularly useful when working with unbounded data streams. Here is an example for orders and payments, where      <pre><code>CREATE TABLE valid_orders (\n    order_id STRING,\n    customer_id INT,\n    product_id STRING,\n    order_time TIMESTAMP_LTZ(3),\n    payment_time TIMESTAMP_LTZ(3),\n    amount DECIMAL,\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND\n) AS SELECT unique_orders.order_id, customer_id, product_id, unique_orders.`$rowtime` AS order_time, payment_time, amount\nFROM unique_orders\n    INNER JOIN payments\n    ON unique_orders.order_id = payments.order_id\n    WHERE unique_orders.`$rowtime` BETWEEN payment_time - INTERVAL '10' MINUTES AND payment_time;\n</code></pre></p> </li> <li> <p>INTERVAL JOIN requires at least one equi-join predicate and a join condition that bounds the time on both sides.     <pre><code>SELECT t.amount, t.order_type, s.name, s.opening_value FROM transactions t, stocks s\nWHERE t.stockid = s.id AND t.ts BETWEEN s.ts - INTERVAL '6' HOURS AND s.ts\n</code></pre></p> </li> </ul>"},{"location":"coding/flink-sql-2/#lateral-joins","title":"Lateral Joins","text":"<p>LATERAL TABLE clause is used to invoke a Table-Valued Function (TVF) or a User-Defined Table Function (UDTF) for every row of a base (outer) table. The Lateral Join evaluates a subquery or a function for each row of the first table, and the result of that evaluation is then joined back to the original row. The UDTF/TVF on the right side of the join can reference columns from the table on the left side.</p> <pre><code>SELECT\n    t.*,\n    tf.*\nFROM\n    input_table AS t,\n    LATERAL TABLE(udtf_function(t.column_a, t.column_b)) AS tf(output_column_1, output_column_2)\n</code></pre> <p>We can combine the LATERAL TABLE with different join types to control which rows are preserved</p> Joins Behavior CROSS JOIN LATERAL TABLE(...) If the UDTF returns zero rows for a given input row, the original row from the outer table is discarded. LEFT JOIN LATERAL Returns all rows from the outer table, even if the UDTF produces zero rows. <p>The Lateral Table allows to dynamically transform stream records in a row-by-row fashion, which is often difficult with standard joins.</p>"},{"location":"coding/flink-sql-2/#multi-way-joins","title":"Multi way joins","text":"<p>See Confluent Flink SQL documentation for multi-way joins as part of optimizing the flink state. Need to add annotations:     <pre><code>SELECT /*+ MULTI_JOIN(o, c, a) */ \n* FROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN addresses a ON c.id = a.customer_id;\n</code></pre></p>"},{"location":"coding/flink-sql-2/#references","title":"References","text":"<p>Here is a list of important tutorials on Joins:</p> <ul> <li>Confluent Cloud: video on joins with details on fow joins work.</li> <li>Confluent -developer: How to join streams. The matching content is in flink-sql/04-joins folder for Confluent Cloud or Platform for Flink. This folder also includes more SQL exercises.</li> <li>Confluent temporal join documentation.</li> <li>Window Join Queries in Confluent Cloud for Apache Flink</li> <li>Temporal Join Study in this repo.</li> </ul>"},{"location":"coding/flink-sql-2/#faqs","title":"FAQs","text":"Inner knowledge on temporal join <p>Event-time temporal joins are used to join two or more tables based on a common event time (in one of the record table or the kafka record: <code>$rowtime</code> system column). With an event-time attribute, the operator can retrieve the value of a key as it was at some point in the past. The right-side, versioned table, stores all versions, identified by time, since the last watermark.</p> <p>The temporal Flink sql looks like:</p> <pre><code>SELECT [column_list]\nFROM table1 [AS &lt;alias1&gt;]\n[LEFT] JOIN table2 FOR SYSTEM_TIME AS OF table1.{ rowtime } [AS &lt;alias2&gt;]\nON table1.column-name1 = table2.column-name1\n</code></pre> <p>When enriching a particular <code>table1</code>, an event-time temporal join waits until the watermark on the table2 stream reaches the timestamp of that <code>table1</code> row, because only then, it is reasonable to be confident that the result of the join is being produced with complete knowledge of the relevant <code>table2</code> data. This table2 records can be old as the watermark on that table being late.</p> How to join two tables on a key within a time window using event column as timestamp and store results in a target table? <p>Full example:</p> <pre><code>-- use separate statements to create the tables\ncreate table Transactions (ts TIMESTAMP(3), tid BIGINT, amount INT);\ncreate table Payments (ts TIMESTAMP(3), tid BIGINT, type STRING);\ncreate table Matched (tid BIGINT, amount INT, type STRING);\n\nexecute statement set\nbegin\ninsert into Transactions values(now(), 10,20),(now(),11,25),(now(),12,34);\ninsert into Payments values(now(), 10, 'debit'),(now(),11,'debit'),(now(),12,'credit');\ninsert into Matched \n    select T.tid, T.amount, P.type\n    from Transactions T join Payments P ON T.tid = P.tid \n    where P.ts between T.ts and T.ts + interval '1' minutes;\nend\n</code></pre> how primary key selection impacts joins? <p>Primary keys on source tables do not impact joins as the joins can be done on any column of the left and right tables. The important keys are the one on the sink table and they need to match the last join columns.</p> Understand Left side velocity and update strategy <p>If the left table has a high velocity and there is no new event for the defined primary key, then it is important to set a TTL on the left side that is short so the state will stay under control. Use the aliases set on the tables and set the retention time. <pre><code>select /* STATE_TTL('tx'='120s', 'c'='4d') */\n    tx_id, account_id, amount, merchant_id, tx_type \nfrom transaction tx\nLEFT JOIN account c\non tx.account_id = c.id\n</code></pre></p> <p>Remark: Running an EXPLAIN on the above statement give the TTL.</p> Join on 1x1 relationship <p>In current Flink SQL it is not possible to efficiently join elements from two tables when we know the relation is 1 to 1: one transaction to one account, one shipment to one order. As soon as there is a match, normally we want to emit the result and clear the state. This is possible to do so with the DataStream API, not SQL.</p>"},{"location":"coding/flink-sql-2/#windowing-table-value-functions","title":"Windowing / Table Value Functions","text":"<p>Windowing Table-Valued Functions groups the Tumble, Hop, Cumulate, and Session Windows. Windows split the stream into \u201cbuckets\u201d of finite size, over which we can implement logic. The return value adds three additional columns named \u201cwindow_start\u201d, \u201cwindow_end\u201d, \u201cwindow_time\u201d to indicate the assigned window.</p> <ul> <li>The TUMBLE function assigns each element to a window of specified window size. Tumbling windows have a fixed size and do not overlap.</li> </ul> Count the number of different product type per 10 minutes (TUMBLE window) <p>Aggregate a Stream in a Tumbling Window documentation..  The following query counts the number of different product types arriving from the event stream by interval of 10 minutes.</p> <p><pre><code>SELECT window_start, product_type, count(product_type) as num_ptype\n    FROM TABLE(\n        TUMBLE(\n            TABLE events,\n            DESCRIPTOR(`$rowtime`),\n            INTERVAL '10' MINUTES\n        )\n    )\n    GROUP BY window_start, window_end, ;\n</code></pre> DESCRIPTOR indicates which time attributes column should be mapped to tumbling windows (here the kafka record ingestion timestamp). </p> <p>When the internal time has expired the results will be published. This puts an upper bound on how much state Flink needs to keep to handle a query, which in this case is related to the number of different product type. </p> <p>It is possible to use another timestamp from the input table. For example the <code>transaction_ts TIMESTAMP(3),</code>  then we need to declare a watermark on this ts:</p> <p><code>WATERMARK FOR transaction_ts AS transaction_ts - INTERVAL '5' SECOND,</code> so it can be used in the descriptor function.</p> <pre><code>INSERT INTO app_orders\nselect \n    window_start, \n    window_end, \n    customer_id, sum(order_amount) \nfrom table(tumble(table `daily_spend`, DESCRIPTOR(transaction_ts), interval '24' hours)) \ngroup by window_start, window_end, customer_id \n</code></pre> Aggregation over a window <p>Windows over approach is to end with the current row, and stretches backwards through the history of the stream for a specific interval, either measured in time, or by some number of rows. For example counting the umber of flight_schedule events of the same key over the last 100 events:</p> <pre><code>select\n    flight_id,\n    evt_type,\n    count(evt_type) OVER w as number_evt,\nfrom flight_events\nwindow w as( partition by flight_id order by $rowtime rows between 100 preceding and current row);\n</code></pre> <p>The results are updated for every input row. The partition is by flight_id. Order by $rowtime is necessary.</p> Find the number of elements in x minutes intervals advanced by 5 minutes? (HOP) <p>Confluent documentation on window integration.. For HOP wuindow, there is the slide parameter to control how frequently a hopping window is started:</p> <pre><code>    SELECT\n        window_start, window_end,\n        COUNT(DISTINCT order_id) AS num_orders\n    FROM TABLE(\n        HOP(TABLE shoe_orders, DESCRIPTOR(`$rowtime`), INTERVAL '5' MINUTES, INTERVAL '10' MINUTES))\n    GROUP BY window_start, window_end;\n</code></pre> How to compute the accumulate price over time in a day (CUMULATE) <p>Needs to use the cumulate window, which adds up records to the window until max size, but emits results at each window steps.  The is image summarizes well the behavior: </p> <pre><code>SELECT window_start, window_end, SUM(price) as `sum`\n    FROM TABLE(\n        CUMULATE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '30' SECONDES, INTERVAL '3' MINUTES))\n    GROUP BY window_start, window_end;\n</code></pre>"},{"location":"coding/flink-sql-2/#sinkupsertmaterializer","title":"SinkUpsertMaterializer","text":"<p>When operating in upsert mode and processing two update events, a potential issue arises. If input operators for two tables in upsert mode are followed by a join and then a sink operator, update events might arrive at the sink out of order. If the downstream operator's implementation doesn't account for this out-of-order delivery, it can lead to incorrect results.</p> <p>Flink typically determines the ordering of update history based on the primary key (or upsert keys) through a global analysis in the Flink planner. However, a mismatch can occur between the upsert keys of the join output and the primary key of the sink table. The <code>SinkUpsertMaterializer</code> operator addresses this mapping discrepancy.</p> <p>This operator maintains a complete list of RowData in its state to correctly process any deletion events originating from the source table. However, this approach can lead to a significant state size, resulting in increased state access I/O overhead and reduced job throughput. Also the output value for each primary key is always the last (tail) element in the maintained list. It is generally advisable to avoid using <code>SinkUpsertMaterializer</code> whenever possible. </p> <p>Consider a scenario where 1 million records need to be processed across a small set of 1,000 keys. In this case, <code>SinkUpsertMaterializer</code> would need to store a potentially long list, averaging approximately 1,000 records per key. </p> <p>To mitigate the usage of <code>SinkUpsertMaterializer</code>:</p> <ul> <li>Ensure that the partition keys used for deduplication, group aggregation, etc., are identical to the sink table's primary keys.</li> <li><code>SinkUpsertMaterializer</code> is unnecessary if retractions are generated using the same key as the sink table's primary key. If a large number of records are processed but most are subsequently retracted, SinkUpsertMaterializer can significantly reduce its state size.</li> <li>Utilize Time-To-Live (TTL) to limit the state size based on time.</li> <li>A higher number of distinct values per primary key directly increases the state size of the SinkUpsertMaterializer.</li> </ul>"},{"location":"coding/flink-sql-2/#row-pattern-recognition","title":"Row pattern recognition","text":"Find the longest period of time for which the average price of a stock did not go below a value <p>Create a Datagen to publish StockTicker to a Kafka topic. See product documentation on CEP pattern with SQL</p> <pre><code>create table StockTicker(symbol string, price int tax int) with ('connector' = 'kafka',...)\nSELECT * From StockTicker \nMATCH_RECOGNIZE ( \n    partition by symbol \n    order by rowtime\n    measures\n        FIRST(A.rowtime) as start_tstamp,\n        LAST(A.rowtime) as last_tstamp,\n        AVG(A.price) as avgPrice\n    ONE ROW PER MATCH\n    AFTER MATCH SKIP PAST LAST ROW\n    PATTERN (A+ B)\n    DEFINE\n        A as AVG(A.price) &lt; 15\n);\n</code></pre> <p>MATCH_RECOGNIZE helps to logically partition and order the data that is used with the PARTITION BY and ORDER BY clauses, then defines patterns of rows to seek using the PATTERN clause. The logical components of the row pattern variables are specified in the DEFINE clause. B is defined implicitly as not being A.</p>"},{"location":"coding/flink-sql-2/#confluent-cloud-specifics","title":"Confluent Cloud Specifics","text":"<p>See Flink Confluent Cloud queries documentation.</p> <p>Each topic is automatically mapped to a table with some metadata fields added, like the watermark in the form of <code>$rowtime</code> field, which is mapped to the Kafka record timestamp. To see it, run <code>describe extended table_name;</code> With watermarking. arriving event records will be ingested roughly in order with  respect to the <code>$rowtime</code> time attribute field.</p> Mapping from Kafka record timestamp and table $rowtime <p>The Kafka record timestamp is automatically mapped to the <code>$rowtime</code> attribute, which is a read only field. Using this field we can order the record by arrival time:</p> <pre><code>select 'flight_id', 'aircraft_id', 'status', $rowtime\nfrom Aircrafts\norder by $rowtime;\n</code></pre> How to run Confluent Cloud for Flink? <p>See the note, but can be summarized as: 1/ create a stream processing compute pool in the same environment and region as the Kafka cluster, 2/ use Console or CLI (flink shell) to interact with topics.</p> <p></p> <pre><code>confluent flink quickstart --name my-flink-sql --max-cfu 10 --region us-west-2 --cloud aws\n</code></pre> Running Confluent Cloud Kafka with local Flink <p>The goal is to demonstrate how to get a cluster created in an existing Confluent Cloud environment and then send message via FlinkFaker using local table to Kafka topic:</p> <p></p> <p>The scripts and readme .</p> Reading from a topic specific offsets <pre><code>ALTER TABLE table_name SET (\n    'scan.startup.mode' = 'specific-offsets',\n    'scan.startup.specific-offsets' = 'partition:0,offset:25; partition:1,offset:10'\n);\n-- Returns from offsets 26 and 11\nSELECT * FROM table_name;\n</code></pre> create a long running SQL with cli <p>Get or create a service account.</p> <pre><code>confluent iam service-account create my-service-account --description \"new description\"\nconfluent iam service-account list\nconfluent iam service-account describe &lt;id_of_the_sa&gt;\n</code></pre> <pre><code>confluent flink statement create my-statement --sql \"SELECT * FROM my-topic;\" --compute-pool &lt;compute_pool_id&gt; --service-account sa-123456 --database my-cluster\n</code></pre>"},{"location":"coding/flink-sql-2/#analyzing-statements","title":"Analyzing Statements","text":"Assess the current flink statement running in Confluent Cloud <p>To assess which jobs are still running, which jobs failed, and which stopped, we can use the user interface, go to the Flink console &gt; . Or the <code>confluent</code> CLI:</p> <pre><code>confluent environment list\nconfluent flink compute-pool list\nconfluent flink statement list --cloud aws --region us-west-2 --environment &lt;your env-id&gt; --compute-pool &lt;your pool id&gt;\n</code></pre>"},{"location":"coding/flink-sql-2/#understand-the-physical-execution-plan-for-a-sql-query","title":"Understand the physical execution plan for a SQL query","text":"<p>See the explain keyword or Confluent Flink documentation for the output explanations.</p> <pre><code>explain select ...\n</code></pre> <p>Indentation indicates data flow, with each operator passing results to its parent. </p> <p>Review the state size, the changelog mode, the upsert key... Operators change changelog modes when different update patterns are needed, such as when moving from streaming reads to aggregations.</p> <p>Pay special attention to data skew when designing your queries. If a particular key value appears much more frequently than others, it can lead to uneven processing where a single parallel instance becomes overwhelmed handling that key\u2019s data. Consider strategies like adding additional dimensions to your keys or pre-aggregating hot keys to distribute the workload more evenly. Whenever possible, configure the primary key to be identical to the upsert key.</p>"},{"location":"coding/flink-sql-2/#troubleshooting-sql-statement-running-slow","title":"Troubleshooting SQL statement running slow","text":"How to search for hot key? <pre><code>SELECT \n    id, \n    tenant_id, \n    count(*) as record_count,\nFROM table_name \nGROUP BY id, tenant_id\n</code></pre> <p>A more advanced statistical query ( TO BE TESTED) <pre><code>WITH key_stats AS (\n    SELECT \n        id,\n        tenant_id,\n        count(*) as record_count\n    FROM src_aqem_tag_tag \n    GROUP BY id, tenant_id\n),\ndistribution_stats AS (\n    SELECT \n        AVG(record_count) as mean_count,\n        STDDEV(record_count) as stddev_count,\n        PERCENTILE_APPROX(record_count, 0.75) as q3,\n        PERCENTILE_APPROX(record_count, 0.95) as p95,\n        PERCENTILE_APPROX(record_count, 0.99) as p99\n    FROM key_stats\n)\nSELECT \n    ks.*,\n    ds.mean_count,\n    ds.stddev_count,\n    -- Z-score calculation for outlier detection\n    CASE \n        WHEN ds.stddev_count &gt; 0 \n        THEN (ks.record_count - ds.mean_count) / ds.stddev_count\n        ELSE 0\n    END as z_score,\n    -- Hot key classification\n    CASE \n        WHEN ks.record_count &gt; ds.p99 THEN 'EXTREME_HOT'\n        WHEN ks.record_count &gt; ds.p95 THEN 'VERY_HOT'\n        WHEN ks.record_count &gt; ds.q3 * 1.5 THEN 'HOT'\n        ELSE 'NORMAL'\n    END as hot_key_category\nFROM key_stats ks\nCROSS JOIN distribution_stats ds\nWHERE ks.record_count &gt; ds.mean_count \n</code></pre></p>"},{"location":"coding/flink-sql-2/#confluent-flink-query-profiler","title":"Confluent Flink Query Profiler","text":"<p>This is a specific, modern implementation of the Flink WebUI, used to monitor the performance of the query.</p> <p></p> <ul> <li>Query Profiler Product documentation</li> <li>See how to use QP to debug statement, in the techno chapter.</li> </ul>"},{"location":"coding/flink-sql-clients/","title":"Flink SQL Client","text":"Updates <p>Created 12/25</p> <p>SQL Client helps developers to write and submit table programs to a Flink cluster without a single line of Java code. From this client, it is possible to get the results of the query in real-time.</p>"},{"location":"coding/flink-sql-clients/#source-of-information","title":"Source of Information","text":"<ul> <li>Apache Flink SQL client</li> </ul>"},{"location":"coding/flink-sql-clients/#getting-started-with-a-apache-flink-sql-client","title":"Getting started with a Apache Flink SQL client","text":"<ul> <li>Lets start:     <pre><code># Under the Apache Flink folder. (e.g. deployment/product-tar), start the cluster\n./bin/start_cluster.sh\n# Start the client in embedded mode, which means address local machine \n./bin/sql-client.sh\n</code></pre></li> <li>See product documentation on running SQL queries.</li> <li>Use the <code>-f</code> option to execute a SQL file in a Session Cluster     <pre><code># In this mode, the client will not open an interactive terminal.\n./bin/sql-client.sh -f query_file.sql\n</code></pre></li> <li> <p>Recall that within the SQL client we can see the changelog of what happen to the table, using [M]</p> </li> <li> <p>Play with some first queries (code/flink-sql/00-basis-sql)</p> </li> </ul>"},{"location":"coding/flink-sql-clients/#basic-commands","title":"Basic commands","text":"<ul> <li> <p>Show catalogs, tables... By default there is a default catalog, <code>default_catalog</code>, and database, <code>default_database</code>, without any table.</p> <pre><code>SHOW CATALOGS;\nUSE CATALOG `examples`;\nSHOW DATABASES;\nUSE `marketplace`;\nSHOW TABLES;\nSHOW TABLES LIKE '*_raw'\nSHOW JOBS;\nDESCRIBE tablename;\nDESCRIBE EXTENDED table_name;\n</code></pre> </li> </ul>"},{"location":"coding/flink-sql-clients/#confluent-cloud-sql-client-via-cli","title":"Confluent Cloud SQL Client via CLI","text":"<p>Confluent Cloud enables users to write Flink SQL statements through the web console or a CLI shell.</p> <p>See quick start product documentation which is summarized as:</p> <ul> <li> <p>Connect to Confluent Cloud with CLI, then get the environment and compute pool identifiers. (change the name of your env)     <pre><code>confluent login --save\nexport ENV_NAME=j9r-env\nexport ENV_ID=$(confluent environment list -o json | jq -r '.[] | select(.name == \"'$ENV_NAME'\") | .id')\nexport COMPUTE_POOL_ID=$(confluent flink compute-pool list -o json | jq -r '.[0].id')\necho $COMPUTE_POOL_ID\n</code></pre></p> </li> <li> <p>Start local SQL client - using the <code>aws-west</code> environment.     <pre><code>confluent flink shell --compute-pool $COMPUTE_POOL_ID --environment $ENV_ID\n</code></pre></p> </li> <li> <p>Be sure to set catalog and database:     <code>sql      use catalog `j9r-env`;      use database `j9r-kafka`;</code></p> </li> <li> <p>Write SQL statements, get the results in the active session.</p> </li> </ul>"},{"location":"coding/flink-sql-clients/#confluent-cloud-flink-workspace","title":"Confluent Cloud Flink Workspace","text":"<p>There is already a lot of tutorials and videos on how to use the Workspace. Things to keep in mind:</p> <ul> <li>The Workspace is great to implement a SQL query step by step, CTE by CTE.</li> <li>A workspace is linked to a compute pool, add more compute pools.</li> <li>The query needs to be copy/paste into a file to be managed as software in git, for example.</li> <li>Run in a cell, a query 'inserting into', will run forever and it is easy, while developing queries, to forget about them.</li> <li>Use the left navigation tree to access list of tables, views, models, external datasources..</li> </ul> <p>See also the Terraform to create Confluent Cloud resources in this note.</p> <p>All the SQL studies in code/flink-sql folder include SQL queries for Confluent Cloud or CP Flink.</p>"},{"location":"coding/flink-sql-clients/#confluent-platform-for-flink","title":"Confluent Platform for Flink","text":"<p>When using Kubernetes deployment, it is recommended to package the SQL script with a Java program . This can be by using the Flink TableAPI, or use a Java program, called SQL Runner. The java jar is deployed as a Flink Application using a FlinkDeployment descriptor.</p> <p>Use one of the following approaches:</p> <ul> <li> <p>When using Flink with docker compose: the SQL client in the docker container runs against local Flink cluster (see deployment/custom-flink-image folder to build a custom image using the dockerfile with the <code>sql-client</code> service and any specific connector jars). To interact with Flink using the SQL client, open a bash in the running container, or in the flink bin folder:     <pre><code># be sure to mount the folder with sql scripts into the container\n\n# Using running job manager running within docker\ndocker exec -ti sql-client bash\n# in kubernetes pod\nkubectl exec -ti pod_name -n namespace -- bash\n# in the shell /opt/flink/bin \n./sql-client.sh\n</code></pre></p> </li> <li> <p>Run SQL in Kubernetes application: </p> <ul> <li>Write SQL statements and test them with Java SQL runner. The Class is in flink-studies/code/flink-java/sql-runner folder. Then package the java app and sql script into a docker image then use a FlinkDeployment  descriptor; (see this git doc). </li> <li>As another solution write Table API code that can also include SQL.</li> </ul> </li> </ul>"},{"location":"coding/getting-started/","title":"Flink Getting Started Guide","text":"Updates <ul> <li>Created 2018 </li> <li>Updated 2/14/2025 - improve note, on k8s deployment and get simple demo reference, review done. </li> <li>03/30/25: converged the notes and update referenced links</li> <li>09/25: Review - simplications.</li> </ul> <p>There are four different approaches to deploy and run Apache Flink / Confluent Flink:</p> <ol> <li>Local Binary Installation</li> <li>Docker-based local deployment</li> <li>Kubernetes Deployment Colima, AKS, EKS, GKS, King or minicube</li> <li>Confluent Cloud Managed Service</li> </ol>"},{"location":"coding/getting-started/#pre-requisites","title":"Pre-requisites","text":"<p>Before getting started, ensure you have some of the following dependant components:</p> <ol> <li>Java 11 or higher installed (OpenJDK) for developing Java based solution.</li> <li>Docker Engine and Docker CLI (for Docker and Kubernetes deployments)</li> <li>kubectl and Helm (for Kubernetes deployment)</li> <li>Confluent Cloud account (for Confluent Cloud deployment, and Flink SQL development)</li> <li>Confluent cli installed for both Confluent Cloud and Confluent Platform</li> <li>Git clone this repository, to access code and deployment manifests and scripts</li> </ol>"},{"location":"coding/getting-started/#1-open-source-apache-flink-local-binary-installation","title":"1. Open Source Apache Flink Local Binary Installation","text":"<p>This approach is ideal for development and testing on a single machine.</p>"},{"location":"coding/getting-started/#installation-steps","title":"Installation Steps","text":"<ol> <li>Download and extract Flink binary. See in the release page -&gt; downloads the last version.    <pre><code># Using the provided script, and change the version\n./deployment/product-tar/install-local.sh\n\n# Or manually\ncurl https://dlcdn.apache.org/flink/flink-2.1.1/flink-2.1.1-bin-scala_2.12.tgz --output Flink-2.1.1-bin-scala_2.12.tgz\ntar -xzf Flink-2.1.1-bin-scala_2.12.tgz\n</code></pre></li> <li> <p>Download and extract Kafka binary See download versions <pre><code>curl https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz  --output kafka_2.13-3.9.0.tgz \ntar -xvf kafka_2.13-3.9.0.tgz\ncd kafka_2.13-3.9.0\nKAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\nbin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\n</code></pre></p> </li> <li> <p>Set environment variables:    <pre><code>export Flink_HOME=$(pwd)/flink-2.1.1\nexport KAFKA_HOME=$(pwd)/kafka_2.13-3.9.0\nexport PATH=$PATH:$Flink_HOME/bin:$KAFKA_HOME/bin\n</code></pre></p> </li> <li> <p>Start the Flink cluster:    <pre><code>$Flink_HOME/bin/start-cluster.sh\n</code></pre></p> </li> <li> <p>Access the Web UI at http://localhost:8081</p> </li> <li> <p>Submit a job (Java application)    <pre><code>./bin/flink run ./examples/streaming/TopSpeedWindowing.jar\n./bin/flink list\n./bin/flink cancel &lt;id&gt; \n</code></pre></p> </li> <li> <p>Download needed SQL connector for Kafka    <pre><code>cd Flink-2.1.1\nmkdir sql-lib\ncd sql-lib\ncurl https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.4.0-1.20/flink-sql-connector-kafka-3.4.0-1.20.jar --output Flink-sql-connector-kafka-3.4.0-1.20.jar\n</code></pre></p> </li> <li> <p>Stop the cluster:     <pre><code>$Flink_HOME/bin/stop-cluster.sh\n</code></pre></p> </li> <li> <p>See this deployement readme in this repo.</p> </li> </ol>"},{"location":"coding/getting-started/#running-simple-sql-application","title":"Running simple SQL application","text":"<ul> <li>See this sql programming lab 0 in this repo</li> <li>And the lab with Kafka integration</li> </ul>"},{"location":"coding/getting-started/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If port 8081 is already in use, modify <code>$Flink_HOME/conf/flink-conf.yaml</code></li> <li>Check logs in <code>$Flink_HOME/log</code> directory</li> <li>Ensure Java 11+ is installed and JAVA_HOME is set correctly</li> </ul>"},{"location":"coding/getting-started/#2-docker-based-deployment","title":"2. Docker-based Deployment","text":"<p>This approach provides containerized deployment using Docker Compose.</p>"},{"location":"coding/getting-started/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Build a custom Apache Flink image with your own connectors. Verify current docker image tag then use the Dockerfile:    <pre><code>cd deployment/custom-Flink-image\ndocker build -t jbcodeforce/myFlink .\n</code></pre></p> </li> <li> <p>Start Flink session cluster:    <pre><code>cd deployment/docker\ndocker compose up -d\n</code></pre></p> </li> </ol>"},{"location":"coding/getting-started/#docker-compose-with-kafka","title":"Docker Compose with Kafka","text":"<p>To run Flink with Kafka:    <pre><code>cd deployment/docker\ndocker compose -f kafka-docker-compose.yaml up -d\n</code></pre></p>"},{"location":"coding/getting-started/#customization","title":"Customization","text":"<ul> <li>Modify <code>deployment/custom-flink-image/Dockerfile</code> to add required connectors</li> <li>Update <code>deployment/docker/flink-oss-docker-compose.yaml</code> for configuration changes</li> </ul> <p>During development, we can use docker-compose to start a simple <code>Flink session</code> cluster or a standalone job manager to execute one unique job, which has  the application jar mounted inside the docker image. We can use this same environment to do SQL based Flink apps. </p> <p>As Task manager will execute the job, it is important that the container running the Flink code has access to the jars needed to connect to external sources like Kafka or other tools like FlinkFaker. Therefore, in <code>deployment/custom-Flink-image</code>, there is a Dockerfile to get the needed jars to build a custom Flink image that may be used for Taskmanager  and SQL client. Always update the jar version with new Flink version.</p> Docker hub and maven links <ul> <li>Docker Hub Flink link</li> <li>Maven Flink links</li> </ul>"},{"location":"coding/getting-started/#3-kubernetes-deployment","title":"3. Kubernetes Deployment","text":"<p>This approach provides scalable, production-ready deployment using Kubernetes. </p>"},{"location":"coding/getting-started/#apache-flink","title":"Apache Flink","text":"<p>See the K8S deployment deeper dive chapter and the lab readme for Apache Flink OSS details.</p>"},{"location":"coding/getting-started/#confluent-platform-manager-for-flink-on-kubernetes","title":"Confluent Platform Manager for Flink on Kubernetes","text":"<p>See Kubernetes deployment chapter for detailed instructions, and the deployment folder and readme. See also the Confluent operator documentation, submit Flink SQL Statement with Confluent Manager for Apache Flink:</p>"},{"location":"coding/getting-started/#4-confluent-cloud-deployment","title":"4. Confluent Cloud Deployment","text":"<p>This approach provides a fully managed Flink service and very easy to get started quickly without managing Flink clusters or Kafka Clusters. It uses the confluent cli</p> <ol> <li>Upgrade the cli    <pre><code>confluent update\n</code></pre></li> <li> <p>Create a Flink compute pool:    <pre><code>confluent flink compute-pool create my-pool --cloud aws --region us-west-2\n</code></pre></p> </li> <li> <p>Start SQL client:    <pre><code>confluent flink shell\n</code></pre></p> </li> <li> <p>Submit SQL statements:    <pre><code>CREATE TABLE my_table (\n  id INT,\n  name STRING\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'my-topic',\n  'properties.bootstrap.servers' = 'pkc-xxxxx.region.provider.confluent.cloud:9092',\n  'properties.security.protocol' = 'SASL_SSL',\n  'properties.sasl.mechanism' = 'PLAIN',\n  'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"&lt;API_KEY&gt;\" password=\"&lt;API_SECRET&gt;\";',\n  'format' = 'json'\n);\n</code></pre></p> </li> </ol> <p>See Confluent Cloud Flink documentation for more details.</p> <p>Explore the Shift Left project, your dedicated CLI for scaling and organizing Confluent\u202fCloud\u202fFlink projects with an opinionated, streamlined approach.</p>"},{"location":"coding/getting-started/#choosing-the-right-deployment-approach","title":"Choosing the Right Deployment Approach","text":"Approach Use Case Pros Cons Local Binary Development, Testing Simple setup, Fast iteration Limited scalability, or manual configuration and maintenance on distributed computers. Docker Development, Testing Containerized, Reproducible Manual orchestration Kubernetes Production Scalable, Production-ready Complex setup Confluent Cloud Production Fully managed, No ops Vendor Control Plane"},{"location":"coding/getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Apache Flink Documentation</li> <li>Confluent Cloud Documentation</li> <li>Flink Kubernetes Operator</li> <li>Docker Hub Flink Images</li> <li>Shift Left project to manage Flink project at scale.</li> </ul> &lt;&lt; PREVIOUS: Key Concepts &gt;&gt; NEXT: Flink SQL &gt;&gt;"},{"location":"coding/k8s-deploy/","title":"Flink Kubernetes Deployment","text":"This chapter updates <ul> <li>Created 10/2024</li> <li>12/24: move some content to hands-on readme, clean content</li> <li>01/25: sql processing section</li> <li>05/25: merge content, simplify, add some details on deployment - fully test k8s deployment on Colima</li> <li>07/25: Update for Confluent Platform v8</li> <li>09/29: Update to diagrams and doc structure.</li> <li>10/12: update to Minio and snapshot / checkpoint configuration</li> <li>11/16: Reorganize content - integrate new CMF 2.1.0, CP3.1, cmf 2.1.0 - swap to orbstack instead of colima</li> <li>01/2026: update to cmf 2.2 CCC 2.4</li> <li>02/2026: Move some content to cookbook</li> </ul> <p>Apache Flink has defined a Kubernetes Operator (FKO) to deploy and manage custom resources for Flink deployments. Confluent Platform Manager for Flink (CMF) is also deployed on Kubernetes with its own operator, leveraging the FKO. Also as part of the Confluent Platform,CMF is integrated with Confluent Kubernetes Operator (CKO).</p> <p>We assume reader has good understanding and knowledge of Kubernetes, and kubectl, and has read the cookbook considerations chapter which presents the high level concepts and architeture for deployment.</p> <p>In this chapter we will address Confluent Platform manager for Flink deployment. </p>"},{"location":"coding/k8s-deploy/#installation","title":"Installation","text":"<p>See docker hub confluentinc account for the last version and tags</p> <p>The Components to install for each deployment approach:</p> Confluent PlatformOpen Source Approach <p>In the context of a Confluent Platform deployment, the components to install are represented in the following figure from bottom to higher layer:</p> <p></p> <p>For an equivalent open source the components are:</p> <p></p>"},{"location":"coding/k8s-deploy/#prerequisites","title":"Prerequisites","text":"<p>Any Kubernetes deployment should include the following pre-requisites:</p> <ul> <li>kubectl CLI.</li> <li>A Kubernetes cluster. For local deployment use Orbstack with Kubernetes enabled. Start the cluster with <code>make start_obstack</code> under <code>deployment/k8s</code> folder. <ul> <li>For production deployment, see the resource sizing for the two operator pods.</li> </ul> </li> <li> <p>Be sure to have helm cli installed: (see installation instructions)   <pre><code># for mac\nbrew install helm\n# or \nbrew upgrade helm\n# for WSL2 - ubuntu\nsudo apt-get install helm\n</code></pre></p> </li> <li> <p>Install Confluent CLI or update existing CLI with:    <pre><code>confluent update\n</code></pre></p> </li> <li> <p>Install Confluent Platform. See Confluent Platform deployment documentation.. The following table is a recap of what is needed to run CP Flink </p> </li> </ul> Product Version Local command Kubernetes 1.26 - 1.34 Confluent for kubernetes 3.1 under cfk, make deploy CP 7.3.x - 8.1.x CP FKO .130.0 under k8s/cmf folder,  make install_upgrade_fko CCC 2.4.0 under k8s/cfk CMF 2.2.0 under k8s/cmf folder,  make deploy_cmf <ul> <li> <p>To use private image repository see this Confluent kubernetes operator documentation.</p> </li> <li> <p>Helpful commands to work on CRDs once Confluent Platform is deployed:   <pre><code>kubectl get crds | grep confluent\nkubectl describe crd kafkatopics.platform.confluent.io  \nkubectl describe crd cmfrestclasses.platform.confluent.io      \n</code></pre></p> </li> </ul>"},{"location":"coding/k8s-deploy/#1-install-external-components","title":"1 Install External Components","text":"<p>The certificate manager and minio operator may be deployed. There is one make target under <code>deployment/k8s</code> to do so: <pre><code>make deploy\n</code></pre></p> <p>See the certificate manager current releases, and update the CERT_MGR_VERSION=v1.18.1 in the Makefile</p> what it does <ul> <li>install certification manager <pre><code>kubectl create -f https://github.com/jetstack/cert-manager/releases/download/$(CERT_MGR_VERSION)/cert-manager.yaml; \n</code></pre></li> <li>install minio <pre><code>kubectl apply -f ./MinIO/minio-dev.yaml \nkubectl apply -f./MinIO/minio-credentials-secret.yaml\n</code></pre></li> <li>Verify deployment with <code>sh   kubeclt get pods -n cert-manager   # or   make verify_cert_manager</code></li> </ul>"},{"location":"coding/k8s-deploy/#using-minio","title":"Using MinIO","text":"<p>MinIO is an object storage solution that provides an Amazon Web Services S3-compatible API and supports all core S3 features, on k8s. It may be used for Flink checkpoint and snapshot persistenace, or when deploying application jar file to Flink, as a file storage.</p> <ul> <li> <p>First be sure to have the MinIO CLI installed.      <pre><code>brew install minio/stable/mc\n# or to upgrade to a new version\nbrew upgrade minio/stable/mc\n# Verify installation\nmc --help\n</code></pre></p> <p>mc cli command summary</p> </li> <li> <p>Deploy Minio operator under <code>minio-dev</code> namespace, using `Make     <pre><code>make deploy_minio\nmake verify_minio\n</code></pre></p> </li> <li> <p>Access MinIO S3 API and Console     <pre><code>kubectl port-forward pod/minio 9000 9090 -n minio-dev\n# or\nmake port_forward_minio_console\n</code></pre></p> </li> <li> <p>Log in to the Console with the credentials <code>minioadmin | minioadmin</code></p> </li> <li>Setup a minio client with credential saved to  $HOME/.mc/config.json     <pre><code>mc alias set dev-minio http://localhost:9000 minioadmin minioadmin\n# make a bucket\nmc mb dev-minio/flink\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#using-persistence-volume","title":"Using Persistence Volume","text":"<p>Network file system, SAN and any distributed storage can be used to persist Flink checkpoints and savepoints. The storage class needs to be defined.</p>"},{"location":"coding/k8s-deploy/#2-confluent-for-kubernetes-installation","title":"2 Confluent For Kubernetes Installation","text":"<p>See the Confluent Platform product installation documentation for details, which is summarized as: </p> <ul> <li>The deployment leverages Kubernetes native API to configure, deploy, and manage Kafka cluster, Connect workers, Schema Registry, Confluent Control Center, Confluent REST Proxy and application resources such as topics.</li> <li> <p>The following diagram illustrates those components in one namespace.      Confluent Platform Components - k8s deployment </p> </li> <li> <p>The makefile in deployment/k8s/cfk support a simple <code>deploy</code> target to deploy Operator, Kraft Controller, Kafka Cluster, Control Center, Schema Registry, on a local kubernetes cluster. See the corresponding readme for explanations. The  <code>make deploy</code> does not use security by default.    <pre><code>make deploy\nmake status\nmake undeploy\n</code></pre></p> </li> <li> <p>For CP with security deployment the new target is <code>make deploy-with_security</code>, and described in the readme.</p> </li> <li> <p>Verify the CP components run:</p> <ul> <li>Pods are running and healthy: <code>kubectl get pods -n confluent</code> <pre><code>      NAME                                  READY   STATUS      RESTARTS          AGE\nconfluent-operator-764dbdf6f9-6f7gx   1/1     Running     158 (5h41m ago)   89d\ncontrolcenter-ng-0                    3/3     Running     13 (5h41m ago)    32d\nkafka-0                               1/1     Running     4 (5h41m ago)     32d\nkraftcontroller-0                     1/1     Running     4 (5h41m ago)     32d\nschemaregistry-0                      1/1     Running     7 (5h41m ago)     32d\n</code></pre></li> <li>Services are deployed: <code>kubectl get svcs -n confluent</code></li> <li> <p>The console may be accessed via port-forwarding:     <pre><code>kubectl -n confluent port-forward svc/controlcenter-ng 9021:9021 \nchrome localhost:9021\n# or from deployment/k8s/cfk folder\nmake port_forward_cp_console\n</code></pre></p> <p></p> </li> </ul> </li> <li> <p>See also the Confluent Platform releases information, for product interopability.</p> </li> </ul>"},{"location":"coding/k8s-deploy/#3-confluent-manager-for-flink-cmf","title":"3 Confluent Manager for Flink (CMF)","text":"<p>Updated 1.10.2026: For CFK version 2.2.0 and CP v8.1.1</p> <p>See the Makefile under deployment/k8s/cmf which includes a set of targets to simplify the deployment. See Confluent Manager for Flink product documentation for deeper information. The following steps are a summary of what should be done.</p> <ul> <li>Install Confluent Manager for Flink operator, under <code>deployment/k8s/cmf</code> <pre><code>make help\nmake deploy\nmake status\n</code></pre></li> </ul> What it does <ul> <li>Install Flink Kubernetes Operator (OSS updated by Confluent) <pre><code>helm upgrade --install cp-flink-kubernetes-operator --version $(FKO_VERSION) confluentinc/flink-kubernetes-operator --set watchNamespaces=\"{$(NS_LIST)}\n</code></pre></li> <li>Define <code>flink</code> service account, a cluster role and role binding</li> <li>Define specific role and role bidning</li> <li>Install CMF operator using Helm <pre><code>  helm upgrade --install cmf --version $(CMF_VERSION)  confluentinc/confluent-manager-for-apache-flink --namespace $(FLCK_NS)  --values cmf_values.yaml \n</code></pre></li> <li>Install the REST Class for CMF <pre><code>kubectl apply -f cmf-rest-class.yaml\n</code></pre></li> </ul> <p>Next to deploy an application see deploy application section for SQL or Java app deployment</p>"},{"location":"coding/k8s-deploy/#security","title":"Security","text":"<p>Confluent Platform Flink security principals are summarized in this section.</p>"},{"location":"coding/k8s-deploy/#4-create-an-environment-for-flink","title":"4 Create an Environment for Flink","text":"<p>Flink environment is used to control access, and to group Flink applications</p> <p>Normally running <code>make deploy</code> under <code>cmf folder</code> will create a dev environment. The creation of an environment can be done via the confluent cli</p> <pre><code>confluent flink environment create $(ENV_NAME) --url $(CONFLUENT_CMF_URL) --kubernetes-namespace $(FLCK_NS) \n</code></pre> <p>From the Console Center UI, create new environment</p> <p>which creates this:</p> <p>Or via REST API - do a port forward on port 8084</p> <pre><code># under deployment/k8s/cmf\nmake port_forward_cmf\n# which is doing\nkubectl port-forward svc/cmf-service 8084:8080 -n $(FLCK_NS)\n# Use an environment definition as json\ncurl -v -H \"Content-Type: application/json\" -X POST http://localhost:8084/cmf/api/v1/environments -d @staging_env.json\n</code></pre> <p>Now we have two environments</p>"},{"location":"coding/k8s-deploy/#5-define-a-sql-catalog","title":"5 Define a SQL Catalog","text":"<p>A <code>KafkaCatalog</code> exposes Kafka topics as tables and derives their schema from Schema Registry. Define a Flink Catalog as json file: (see cmf/dev_catalog.json). The catalog is configured with connection properties to the Schema Registry clients.</p> <pre><code># under deployment/k8s/cmf\nmake create_kafka_catalog\n# OR using curl\ncurl -v -H \"Content-Type: application/json\" -X POST http://localhost:8084/cmf/api/v1/catalogs/kafka -d@./dev_catalog.json\n</code></pre>"},{"location":"coding/k8s-deploy/#documentations","title":"Documentations","text":"<ul> <li>Confluent Platform for Flink has another operator integrated with FKO. See my CP Flink summary.</li> <li>Confluent Flink operator documentation</li> <li>Getting started with Flink OSS Standalone Kubernetes Setup.</li> <li>Apache Flink Native Kubernetes deployment.</li> <li>A Confluent Platform demonstration git repo: confluentinc/confluent-demo</li> <li></li> </ul>"},{"location":"coding/k8s-deploy/#apache-flink-oss","title":"Apache Flink OSS","text":"<p>As seen in previous section, Apache Flink has implemented a Kubernetes Operator for managing application. You can get the list of stable versions here.</p> <ul> <li>The prerequisites include getting Certificat Manager deployed. (Optional with Minio for local Object Storage - S3 protocol).</li> <li>Get the list of Apache Flink releases and tags here </li> <li> <p>Add the Apache Flink Helm repositories:      <pre><code>helm repo add flink-operator-repo https://downloads.apache.org/flink/flink-Kubernetes-operator-1.13.0\n# Verify help repo entries exist\nhelm repo list\n# Be sure to change the repo as the URL may not be valid anymore\nhelm repo remove  flink-operator-repo\n# try to update repo content\nhelm repo update\n</code></pre></p> </li> <li> <p>This repository includes a Makefile to simplify deployment, of Apache Flink, (local orbstack or colima) see the deployment/k8s/flink-oss folder.    <pre><code>make prepare\nmake verify_flink\nmake deploy_basic_flink_deployment  \n</code></pre></p> </li> <li>Smoke Test with one of the pre-packaged app:   <pre><code>kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.13/examples/basic.yaml\nkubectl logs -f deploy/basic-example\n# Access WebUI\nkubectl port-forward svc/basic-example-rest 8081\n</code></pre></li> <li>Access Flink UI</li> <li>Stop the job   <pre><code>kubectl delete flinkdeployment/basic-example\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#streaming-processing-deployment","title":"Streaming Processing Deployment","text":"<p>TO BE MOVED</p>"},{"location":"coding/k8s-deploy/#deploy-datastream-table-api-applications","title":"Deploy DataStream / Table API Applications","text":"<p>For java application, once the jar is built, we need to define a manifest to deploy the application. See Apache Flink Kubernetes Operator - FlinkDeployment documentation.</p> <ul> <li> <p>An example of manifest is the smoke test for Apache Flink</p> </li> <li> <p>An example of a DataStream deployment on CP Flink</p> </li> </ul>"},{"location":"coding/k8s-deploy/#flink-sql-statement","title":"Flink SQL Statement","text":""},{"location":"coding/k8s-deploy/#apache-flink-open-source","title":"Apache Flink Open Source","text":"<p>There are multiple choices to run Flink SQL: using the SQL client, or package the SQL scripts in a docker container with the java SQL runner executing the SQL statements from a file, or use the Table API. The application deployment is Java based even if SQL scripts are used for stream processing.</p> <p>With Apache Flink OSS, Flink Session Cluster is the most suitable deployment mode for the SQL Client. This is a long-running Flink cluster (JobManager and TaskManagers) on which you can submit multiple jobs to. The sql client is a long-running, interactive application that submits jobs to an existing cluster.</p> <p>TBC</p>"},{"location":"coding/k8s-deploy/#confluent-flink-sql-statement","title":"Confluent Flink SQL Statement","text":"<p>A catalog is a top-level resource and references a Schema Registry instance. A database is a sub-resource of a catalog, references a Kafka cluster and exposes all topics of its Kafka cluster as queryable tables. </p> <ul> <li>Define Database which maps to a Kafka Cluster and is created within a catalog: see product documentation for example <pre><code>curl -v -H \"Content-Type: application/json\" -X POST http://localhost:8084/cmf/api/v1/catalogs/kafka/dev-catalog/databases -d@./dev_database.json\n</code></pre></li> </ul> <p>The creation of the catalog and database may be done one time. The Data engineers will deploy multiple statements to work on tables within the database. * To be able to run ay SQL queries we need to define one to many compute pools. Product Doc <pre><code>curl -v -H \"Content-Type: application/json\"  -X POST http://localhost:8084/cmf/api/v1/environments/dev-env/compute-pools  -d @./compute_pool.json\n</code></pre></p> <ul> <li>Deploy one to many Flink SQL Statement. The C3 version 2.4 includes the Worspace user interface. Each statement is associated with exactly one compute pool.</li> </ul>"},{"location":"coding/k8s-deploy/#confluent-manager-for-flink","title":"Confluent Manager for Flink","text":"<p>As seen previously in Confluent Manager for Flink the method is to create an Environment and Compute pool to run the SQL statements in a pool. Those concepts and components are the same as the Confluent Cloud for Flink.</p> <ul> <li> <p>Define a compute pool (verify current docker image tag) and see the compute_pool.json <pre><code>make create_compute_pool\n</code></pre></p> </li> <li> <p>Flink SQL uses the concept of Catalogs to connect to external storage systems. CMF features built-in KafkaCatalogs to connect to Kafka and Schema Registry. </p> </li> <li> <p>Define secret to access Kafka Cluster See this secret and the mapping <pre><code>make create_kafka_secret\nmake create_env_secret_mapping\n</code></pre></p> </li> <li> <p>Use the confluent cli to start a Flink  SQL shell   <pre><code>confluent --environment dev --compute-pool pool1 flink shell --url http://localhost:8084\n</code></pre></p> </li> </ul>"},{"location":"coding/k8s-deploy/#apache-flink-oss-flink-sql","title":"Apache Flink (OSS) - Flink SQL","text":"<p>You can run the SQL Client in a couple of ways:</p> <ul> <li>As a separate Docker container: The Flink Docker images include the SQL Client. You can run a container and connect to the JobManager. You will need to mount a volume to persist SQL scripts and other data.   <pre><code>kubectl exec -it &lt;sql-client-pod-name&gt; -- /opt/flink/bin/sql-client.sh\n</code></pre></li> </ul> <p>When running the SQL Client as a pod within the same Kubernetes cluster, you can use the internal DNS name of the JobManager service to connect. The format is typically ..svc.cluster.local <ul> <li>Locally: Download the Flink distribution, extract it, and run the SQL Client from your local machine.   <pre><code># port forwarding\nkubectl port-forward svc/&lt;jobmanager-service-name&gt; 8081:8081\n\n./bin/sql-client.sh -s &lt;jobmanager-service-name&gt;:8081\n</code></pre></li> </ul>"},{"location":"coding/k8s-deploy/#confluent-flink","title":"Confluent Flink","text":"<ul> <li>See manage Flink app using Confluent for Flink</li> </ul>"},{"location":"coding/k8s-deploy/#fault-tolerance","title":"Fault tolerance","text":"<p>For Flink job or application that are stateful and for fault tolerance, it is important to enable checkpointing and savepointing:</p> <pre><code>job:\n  jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar\n\n  parallelism: 2\n  upgradeMode: savepoint\n  #savepointTriggerNonce: 0\n  # initialSavepointPath: file:///\n</code></pre> <p>The other upgradeMode is ``</p> How to validate checkpointing? <p>Checkpointing let Flink to periodically save the state of a job into local storage.  Look at the pod name of the task manager and stop it with <code>kubectl delete pod/....</code> Flink should automatically restart the job and recover from the latest checkpoint. Use the Flink UI or CLI to see the job status.</p> How to validate savepointing? <p>Savepoints are manually triggered snapshots of the job state, which can be used to upgrade a job or to perform manual recovery. To trigger a savepoint we need to set a value into <code>savepointTriggerNonce</code> in the FlinkDeployment descriptor and then apply the changes.  Get the location of the save point and then add to the yaml <code>initialSavepointPath</code> to redeploy the applicationL: it will reload its state from the savepoint. There is a custom resource definition (FlinkStateSnapshotSpec) to trigger savepoints. </p> <p><code>flinkConfiguration</code> is a hash map used to define the Flink configuration, such as the task slot, HA and checkpointing parameters.</p> <pre><code>  flinkConfiguration:\n    high-availability.type: org.apache.flink.Kubernetes.highavailability.KubernetesHaServicesFactory\n    high-availability.storageDir: 'file:///opt/flink/volume/flink-ha'\n    restart-strategy: failure-rate\n    restart-strategy.failure-rate.max-failures-per-interval: '10'\n    restart-strategy.failure-rate.failure-rate-interval: '10 min'\n    restart-strategy.failure-rate.delay: '30 s'\n    execution.checkpointing.interval: '5000'\n    execution.checkpointing.unaligned: 'false'\n    state.backend.type: rocksdb\n    state.backend.incremental: 'true'\n    state.backend.rocksdb.use-bloom-filter: 'true'\n    state.checkpoints.dir: 'file:///opt/flink/volume/flink-cp'\n    state.checkpoints.num-retained: '3'\n    state.savepoints.dir: 'file:///opt/flink/volume/flink-sp'\n    taskmanager.numberOfTaskSlots: '10'\n    table.exec.source.idle-timeout: '30 s'\n</code></pre> <p>The application jar needs to be in a custom Flink docker image built using the Dockerfile as in e-com-sale-demo, or uploaded to a MinIO bucket. </p> <p>The following Dockerfile is used for deploying a solution in application mode, which packages the Java Flink jars with the app, and any connector jars needed for the integration and starts the <code>main()</code> function.</p> <pre><code>FROM confluentinc/cp-flink:1.19.1-cp2\nRUN mkdir -p $FLINK_HOME/usrlib\nCOPY /path/of/my-flink-job-*.jar $FLINK_HOME/usrlib/my-flink-job.jar\n</code></pre> <ul> <li> <p>With Confluent Platform for Flink:</p> <pre><code>  # First be sure the service is expose\n  kubectl port-forward svc/cmf-service 8080:80 -n flink\n  # Deploy the app given its deployment\n  confluent flink application create k8s/cmf_app_deployment.yaml  --environment $(ENV_NAME) --url http://localhost:8080 \n</code></pre> </li> </ul> Access to user interface <p>To forward your jobmanager\u2019s web ui port to local 8081.</p> <pre><code>kubectl port-forward ${flink-jobmanager-pod} 8081:8081 \n# Or using confluent cli CP for Flink command:\nconfluent flink application web-ui-forward $(APP_NAME) --environment $(ENV_NAME) --port 8081 --url http://localhost:8080\n</code></pre> <p>And navigate to http://localhost:8081.</p>"},{"location":"coding/k8s-deploy/#using-minio-for-app-deployment","title":"Using MinIO for app deployment","text":"<ul> <li> <p>Upload an application to minio bucket:</p> <pre><code>mc cp ./target/flink-app-0.1.0.jar dev-minio/flink/flink-app-0.1.0.jar\nmc ls dev-minio/flink\n</code></pre> </li> <li> <p>Start the application using confluent cli:</p> <pre><code>confluent flink application create --environment env1 --url http://localhost:8080 app-deployment.json\n</code></pre> </li> <li> <p>Open Flink UI:</p> <pre><code>confluent flink application web-ui-forward --environment env1 flink-app --url http://localhost:8080\n</code></pre> </li> <li> <p>Produce messages to kafka topic</p> <pre><code>echo 'message1' | kubectl exec -i -n confluent kafka-0 -- /bin/kafka-console-producer --bootstrap-server kafka.confluent.svc.cluster.local:9092 --topic in\n</code></pre> </li> <li> <p>Cleanup     <pre><code># the Flink app\nconfluent flink application delete kafka-reader-writer-example --environment development --url http://localhost:8080\n# the Kafka cluster\n# the operators\n</code></pre></p> </li> </ul> <p>To REWORK  --- To REWORK  --- To REWORK --- To REWORK  --- To REWORK</p>"},{"location":"coding/k8s-deploy/#ha-configuration","title":"HA configuration","text":"<p>Within Kubernetes, we can enable Flink HA in the ConfigMap of the cluster configuration that will be shared with deployments:</p> <pre><code>  flinkConfiguration:\n    taskmanager.numberOfTaskSlots: \"2\"\n    state.backend: rockdb\n    state.savepoints.dir: file:///flink-data/savepoints\n    state.checkpoints.dir: file:///flink-data/checkpoints\n    high-availability.type: Kubernetes\n    high-availability.storageDir: file:///flink-data/ha\n    job.autoscaler.enabled: true\n</code></pre> <p>This configuration settings is supported via FKO.  See product documentation, and the autoscaler section for deeper parameter explanations. The Flink autoscaler monitors the number of unprocessed records in the input (pending records), and will allocate more resources to absorb the lag. It adjusts parallelism at the flink operator level within the DAG. </p> <p>JobManager metadata is persisted in the file system specified by <code>high-availability.storageDir</code> . This <code>storageDir</code> stores all metadata needed to recover a JobManager failure.</p> <p>JobManager Pods, that crashed, are restarted automatically by the Kubernetes scheduler, and as Flink persists metadata and the job artifacts, it is important to mount pv to the expected paths.</p> <pre><code>podTemplate:\n  spec:\n    containers:\n      - name: flink-main-container\n        volumeMounts:\n        - mountPath: /flink-data\n          name: flink-volume\n    volumes:\n    - name: flink-volume\n      hostPath:\n        # directory location on host\n        path: /tmp/flink\n        # this field is optional\n        type: Directory\n</code></pre> <p>Recall that <code>podTemplate</code> is a base declaration common for job and task manager pods. Can be overridden by the jobManager and taskManager pod template sub-elements (spec.taskManager.podTemplate). The previous declaration will work for local k8s with hostPath access, for Kubernetes cluster with separate storage class then the volume declaration is:</p> <pre><code>volumes:\n  - name: flink-volume\n    persistenceVolumeClaim:\n      claimName: flink-pvc\n</code></pre> <p>podTemplate can include nodeAffinity to allocate taskManager to different node characteristics:</p> <pre><code>  podTemplate:\n      spec:\n        affinity:\n          nodeAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n              nodeSelectorTerms:\n                - matchExpressions:\n                    - key: cfk-cr\n                      operator: In\n                      values:\n                        - flink\n        tolerations:\n          - key: cfk-cr\n            operator: Equal\n            value: flink\n            effect: NoSchedule\n</code></pre> <p>TO UPDATE </p>"},{"location":"coding/k8s-deploy/#durable-storage","title":"Durable Storage","text":"<p>Durable storage is used to store consistent checkpoints of the Flink state. Review the state management section in the concept chapter. The checkpoints are saved to object storage compatible with S3, or HDFS protocol. The FlinkConfiguration can be set at the Application, ComputePool or Environment level.</p> <p>Two important elements to configure:  1. the environment variable ENABLE_BUILT_IN_PLUGINS 1. The <code>state.checkpoints.dir</code> to the location of S3 bucket.</p> <p>The following is a configuration using minio and the presto S3FileSystem which is a specific implementation (created by Presto) of the file system interface within Apache Flink. (See the S3FileSystemFactory class). </p> <pre><code>\"flinkConfiguration\": {\n        \"pipeline.operator-chaining.enabled\": \"false\",\n        \"execution.checkpointing.interval\": \"10s\",\n        \"taskmanager.numberOfTaskSlots\": \"4\",\n        \"fs.s3.impl\": \"org.apache.flink.fs.s3presto.S3FileSystem\",\n        \"presto.s3.endpoint\": \"http://minio.minio-dev.svc.cluster.local:9000\",\n        \"presto.s3.path.style.access\": \"true\",\n        \"presto.s3.connection.ssl.enabled\": \"false\",\n        \"presto.s3.access-key\": \"admin\",\n        \"presto.s3.secret-key\": \"admin123\",\n        \"state.checkpoints.dir\": \"s3://flink/stateful-flink/checkpoints\",\n        \"state.savepoints.dir\": \"s3://flink/stateful-flink/savepoints\",\n        \"state.checkpoints.interval\": \"10000\",\n        \"state.checkpoints.timeout\": \"600000\"\n\n      },\n</code></pre> <p>For Minio settings:</p> <pre><code>  s3.endpoint: http://minio.minio-dev.svc.cluster.local:9000\n  s3.path.style.access: \"true\"\n  s3.connection.ssl.enabled: \"false\"\n  s3.access-key: minioadmin\n  s3.secret-key: minioadmin\n  state.checkpoints.dir: s3://flink/stateful-flink/checkpoints\n  state.savepoints.dir: s3://flink/stateful-flink/savepoints\n  state.checkpoints.interval: \"10000\"\n  state.checkpoints.timeout: \"600000\"\n</code></pre> <p>TO BE CONTINUED</p> <p>A RWX, shared PersistentVolumeClaim (PVC) for the Flink JobManagers and TaskManagers provides persistence for stateful checkpoint and savepoint of Flink jobs. </p> <p>A flow is a packaged as a jar, so developers need to define a docker image with the Flink API and any connector jars. Example of Dockerfile and FlinkApplication manifest.</p> <p>Also one solution includes using MinIO to persist application jars.</p>"},{"location":"coding/k8s-deploy/#flink-config-update","title":"Flink Config Update","text":"<ul> <li> <p>If a write operation fails when the pod creates a folder or updates the Flink config, verify the following:</p> <ul> <li>Assess PVC and R/W access. Verify PVC configuration. Some storage classes or persistent volume types may have restrictions on directory creation</li> <li>Verify security context for the pod. Modify the pod's security context to allow necessary permissions.</li> <li>The podTemplate can be configured at the same level as the task and job managers so any mounted volumes will be available to those pods. See basic-reactive.yaml from Flink Operator examples.</li> </ul> </li> </ul> <p>See PVC and PV declarations</p>"},{"location":"coding/k8s-deploy/#flink-session-cluster","title":"Flink Session Cluster","text":"<p>For Session cluster, there is no jobSpec. See this deployment definition. Once a cluster is defined, it has a name and can be referenced to submit SessionJobs.</p> <p>A SessionJob is executed as a long-running Kubernetes Deployment. We may run multiple Flink jobs on a Session cluster. Each job needs to be submitted to the cluster after the cluster has been deployed. To deploy a job, we need at least three components:</p> <ul> <li>a Deployment which runs a JobManager</li> <li>a Deployment for a pool of TaskManagers</li> <li>a Service exposing the JobManager\u2019s REST and UI ports</li> </ul> <p>For a deployment select the execution mode: <code>application, or session</code>. For production it is recommended to deploy in <code>application</code> mode for better isolation, and using a cloud native approach. We can just build a dockerfile for our application using the Flink jars.</p>"},{"location":"coding/k8s-deploy/#session-deployment","title":"Session Deployment","text":"<p>Flink has a set of examples like the Car top speed computation with simulated record. As this code is packaged in a jar available in maven repository, we can declare a job session.</p> <p>Deploy a config map to define the <code>log4j-console.properties</code> and other parameters for Flink (<code>flink-conf.yaml</code>)</p> <p>The diagram below illustrates the standard deployment of a job on k8s with session mode:</p> <p></p> <p>src: apache Flink site</p> <pre><code>apiVersion: flink.apache.org/v1beta1\nkind: FlinkSessionJob\nmetadata:\n  name: car-top-speed-job\nspec:\n  deploymentName: flink-session-cluster\n  job:\n    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.17.2/flink-examples-streaming_2.12-1.17.2-TopSpeedWindowing.jar\n    parallelism: 4\n    upgradeMode: stateless\n</code></pre> <p>Before deploying this job, be sure to deploy a session cluster using the following command:</p> <pre><code># under deployment/k8s\nkubectl apply -f basic-job-task-mgrs.yaml \n</code></pre> <p>Once the job is deployed we can see the pod and then using the user interface the job continuously running:</p> <ul> <li> <p>Example of deploying Java based SQL Runner to interpret a Flink SQL script: package it as docker images, and deploy it with a Session Job. There is a equivalent for Python using Pyflink.</p> <ul> <li>See the ported code for Java</li> <li>And for the Python implementation</li> </ul> </li> </ul>"},{"location":"coding/k8s-deploy/#flink-state-snapshot","title":"Flink State Snapshot","text":"<p>To help managing snapshots, there is another CR called FlinkStateSnapshot</p>"},{"location":"coding/k8s-deploy/#practices","title":"Practices","text":"<ul> <li>It is not recommended to host a Flink Cluster across multiple Kubernetes clusters. Flink node exchanges data between task managers and so better to run in same region, and within same k8s. </li> </ul>"},{"location":"coding/pyflink/","title":"PyFlink","text":"<p>FLIP 541</p>"},{"location":"coding/stateful-func/","title":"Stateful function","text":"<p>Update 1/2025: it seems the OSS project has less traction.</p> <p>Stateful Functions is an open source framework that reduces the complexity of building and orchestrating distributed stateful applications at scale. It brings together the benefits of stream processing with Apache Flink\u00ae and Function-as-a-Service (FaaS) to provide a powerful abstraction for the next generation of event-driven architectures.</p> <p></p> <p>The Flink worker processes (TaskManagers) receive the events from the ingress systems (Kafka, Kinesis, etc.) and route them to the target functions. They invoke the functions and route the resulting messages to the next respective target functions. Messages designated for egress are written to an egress system.</p>"},{"location":"coding/table-api/","title":"Table API","text":"Update <p>Created 10/2024 - Updated 11/03/24. Reorganize in improve documentation 10/2025</p>"},{"location":"coding/table-api/#concepts","title":"Concepts","text":"<p>The TableAPI serves as the lower-level API for executing Flink SQL, allowing for stream processing implementations in Java and Python. The Table API encapsulates a stream or a physical table, enabling developers to implement streaming processing by programming against these tables.</p> <p>See the main concepts and APIs. The structure of a program looks mostly the same:</p> <ol> <li>Create a TableEnvironment for batch or streaming execution     <pre><code>import org.apache.flink.table.api.EnvironmentSettings;\nimport org.apache.flink.table.api.TableEnvironment;\n\nEnvironmentSettings settings = EnvironmentSettings\n    .newInstance()\n    .inStreamingMode()\n    //.inBatchMode()\n    .build();\n\nTableEnvironment tEnv = TableEnvironment.create(settings);\n</code></pre></li> <li>Create one or more source table(s)</li> <li>Create one or more sink Tables(s) or use the print sink</li> <li>Create processing logic using SQL string or Table API functions</li> </ol> <p>Summary of important concepts:</p> <ul> <li>The main function is a Flink client, that will compiles the code into a dataflow graph and submots to the JobManager.</li> <li>A TableEnvironment maintains a map of catalogs of tables </li> <li>Tables can be either virtual (VIEWS) or regular TABLES which describe external data.</li> <li>Tables may be temporary (tied to the lifecycle of a single Flink session), or permanent ( visible across multiple Flink sessions and clusters).</li> <li>Temportary table may shadow a permanent table.</li> <li>Tables are always registered with a 3-part identifier consisting of catalog, database, and table name.</li> <li>TableSink is a generic interface to to write results to. A batch Table can only be written to a <code>BatchTableSink</code>, while a streaming Table requires either an <code>AppendStreamTableSink</code>, a <code>RetractStreamTableSink</code>, or an <code>UpsertStreamTableSink</code>.</li> <li>A pipeline can be explained with <code>TablePipeline.explain()</code> and executed invoking <code>TablePipeline.execute()</code>.</li> <li>Recall that High-Availability in Application Mode is only supported for single-execute() applications.</li> </ul> <p>It is important to note that Table API and SQL queries can be easily integrated with and embedded into DataStream programs.</p>"},{"location":"coding/table-api/#packaging","title":"Packaging","text":"<p>TBD</p>"},{"location":"coding/table-api/#confluent-specifics","title":"Confluent Specifics","text":"<p>In Confluent Manager for Flink deployment, only Flink Application mode is supported. A Flink Application is any user's program that spawns one or multiple Flink jobs from its <code>main()</code> method. The execution of these jobs can happen in a local JVM (LocalEnvironment) or on a remote setup of clusters with multiple machines (kubernetes).</p> <p>In the context of Confluent Cloud, the Table API program acts as a client-side library for interacting with the Flink engine hosted in the cloud. It enables the submission of <code>Statements</code> and retrieval of <code>StatementResults</code>. The provided Confluent plugin integrates specific components for configuring the TableEnvironment, eliminating the need for a local Flink cluster. By including the <code>confluent-flink-table-api-java-plugin</code> dependency, Flink's internal components\u2014such as CatalogStore, Catalog, Planner, Executor, and configuration, are managed by the plugin and fully integrated with Confluent Cloud. This integration is via the REST API, so Confluent Table API plugin is an higher emcapsulation of the CC REST API. </p>"},{"location":"coding/table-api/#getting-started","title":"Getting Started","text":"<p>The development approach includes at least the following steps:</p> <ol> <li> <p>Create a maven project with a command like:     <pre><code>mvn archetype:generate -DgroupId=j9r.flink -DartifactId=my-app -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.5 -DinteractiveMode=false\n</code></pre></p> </li> <li> <p>Add the flink table api, and Kafka client dependencies:</p> Open Source LibrariesConfluent Cloud for FlinkConfluent Platform for Flink <p><pre><code>&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n\n&lt;artifactId&gt;flink-java&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-clients&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-connector-base&lt;/artifactId&gt;\n&lt;!-- Depending of the serialization needs --&gt;\n&lt;artifactId&gt;flink-json&lt;/artifactId&gt;\n&lt;artifactId&gt;flink-avro&lt;/artifactId&gt;\n&lt;!-- when using schema registry --&gt;\n&lt;artifactId&gt;flink-avro-confluent-registry&lt;/artifactId&gt;\n</code></pre> (see an example of pom.xml).  Use <code>provided</code> dependencies to get the Flink jars from the deployed product. </p> <p><pre><code> &lt;dependency&gt;\n    &lt;groupId&gt;io.confluent.flink&lt;/groupId&gt;\n    &lt;artifactId&gt;confluent-flink-table-api-java-plugin&lt;/artifactId&gt;\n    &lt;version&gt;${confluent-plugin.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> For Confluent Cloud for Flink deployment see the pom.xml in table-api/ccf-table-api folder.</p> <pre><code>\n</code></pre> </li> <li> <p>Implement and unit test the flow. See best practices for code structure.</p> </li> <li> <p>Depending of the target Flink runtime, there will be different steps: </p> For Confluent Platform for Flink:For Confluent Cloud for Flink: <ol> <li>Define a FlinkApplication CR</li> <li>Package the jar, and build a docker image using Confluent Platform for Flink base image with a copy of the app jar. (See example of Dockerfile)</li> <li>Deploy to Kubernetes using the Flink kubernetes operator</li> <li>Monitor with the web ui.</li> </ol> <ol> <li> <p>Add the  <code>io.confluent.flink.confluent-flink-table-api-java-plugin</code> into the maven dependencies and use the following Environment settings: </p> Confluent Cloud access via environment variables<pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport org.apache.flink.table.api.EnvironmentSettings;\nEnvironmentSettings settings = ConfluentSettings.fromGlobalVariables();\nTableEnvironment env = TableEnvironment.create(settings);\n</code></pre> Confluent Cloud access via properties file<pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport org.apache.flink.table.api.EnvironmentSettings;\nEnvironmentSettings settings = ConfluentSettings.fromResource(\"/cloud.properties\");\nTableEnvironment env = TableEnvironment.create(settings);\n</code></pre> <p>When using private endpoint, there is a need to be at the 1.20.50 version of the plugin (<code>confluent_flink_table_api_python_plugin=\"&gt;=1.20.50\"</code>) and use code (python example) like:</p> <pre><code>settings = ConfluentSettings.new_builder_from_file(CLOUD_PROPERTIES_PATH) \n.set_context_name(\"my-context\") \n.set_rest_endpoint(\"private.confluent.cloud\") \n.build()\n</code></pre> </li> <li> <p>Set environment variables to specify API key and secrets, Confluent Cloud environment, compute pool... </p> Confluent Cloud related  environment variables<pre><code>export CLOUD_PROVIDER=\"aws\"\nexport CLOUD_REGION=\"us-west-2\"\nexport FLINK_API_KEY=\"&lt;your-flink-api-key&gt;\"\nexport FLINK_API_SECRET=\"&lt;your-flink-api-secret&gt;\"\nexport ORG_ID=\"&lt;your-organization-id&gt;\"\nexport ENV_ID=\"&lt;your-environment-id&gt;\"\nexport COMPUTE_POOL_ID=\"&lt;your-compute-pool-id&gt;\"\n</code></pre> </li> <li> <p>Execute the java program</p> </li> </ol> </li> </ol> <p>See the Confluent Flink cookbook for more Table API and DataStream examples.</p>"},{"location":"coding/table-api/#java","title":"Java","text":"<p>Any main function needs to connect to the Flink environment. Confluent API offers a way to read cloud client properties so the running Flink application can access the Job and Task managers running in the Confluent Cloud compute pools as a service (see code example above).</p> <p>A table environment is the base class, entry point, and central context for creating Table and SQL API programs. </p> <p>TableEnvironment uses an <code>EnvironmentSettings</code> that define the execution mode. The following is a template code to run Table API program submitted to a Job Manager deployed locally or on Kubernetes (OSS Flink or CP for Flink):</p> Deployment for Kubernetes or local<pre><code>settings= EnvironmentSettings.newInstance()\n   .inStreamingMode()\n   .withBuiltInCatalogName(\"default_catalog\")\n   .withBuiltInDatabaseName(\"default_database\")\n   .build();\n\nTableEnvironment tableEnv = TableEnvironment.create(settings);\n        tableEnv.get_config().set(\"parallelism.default\", \"4\");\n</code></pre> <p>Once packaged with maven as a uber-jar the application may be executed locally to send the dataflow to the Confluent Cloud for Flink JobManager or can be deployed as a <code>FlinkApplication</code> within a k8s cluster. </p>"},{"location":"coding/table-api/#confluent-cloud-for-flink-execution","title":"Confluent Cloud for Flink execution","text":"<p>When using remote Confluent Cloud for Flink, it is possible to directly execute the java jar and it will send the DAG to the remote job manager:</p> <pre><code># set all environment variable or use /cloud.properties in resource folder\njava -jar target/flink-app-ecom-0.1.0.jar \n</code></pre> <p>See code sample: Main_00_JoinOrderCustomer.java</p> <p>Get the catalog and databases, and use the environment to get the list of tables. In Confluent Cloud, there is a predefined catalog with some table samples: <code>examples.marketplace</code>.</p> <pre><code># using a sql string\nenv.executeSql(\"SHOW TABLES IN `examples`.`marketplace`\").print();\n# or using the api\nenv.useCatalog(\"examples\");\nenv.useDatabase(\"marketplace\");\nArrays.stream(env.listTables()).forEach(System.out::println);\n# work on one table\nenv.from(\"`customers`\").printSchema();\n</code></pre>"},{"location":"coding/table-api/#python","title":"Python","text":""},{"location":"coding/table-api/#code-samples","title":"Code Samples","text":"<p>The important classes are:</p> <ul> <li>TableEnvironment</li> <li>Table</li> <li>Row</li> <li>Expressions contains static methods for referencing table columns, creating literals, and building more complex Expression chains. See below.</li> <li>Confluent Table API Tutorial</li> </ul>"},{"location":"coding/table-api/#code-structure","title":"Code structure","text":"<p>Clearly separate the creation of sources, sinks, workflow in different methods. References those methods in the main().</p>"},{"location":"coding/table-api/#create-some-test-data","title":"Create some test data","text":"<p>Use one of the TableEnvironment fromValues() methods,</p> From collectionWith Schema and list of row <pre><code>env.fromValues(\"Paul\", \"Jerome\", \"Peter\", \"Robert\")\n                .as(\"name\")\n                .filter($(\"name\").like(\"%e%\"))\n                .execute()\n                .print();\n</code></pre> <pre><code>import org.apache.flink.table.api.DataTypes;\nTable customers = env.fromValues(\n                        DataTypes.ROW(\n                                DataTypes.FIELD(\"customer_id\", DataTypes.INT()),\n                                DataTypes.FIELD(\"name\", DataTypes.STRING()),\n                                DataTypes.FIELD(\"email\", DataTypes.STRING())),\n                        row(3160, \"Bob\", \"bob@corp.com\"),\n                        row(3107, \"Alice\", \"alice.smith@example.org\"),\n                        row(3248, \"Robert\", \"robert@someinc.com\"));\n</code></pre>"},{"location":"coding/table-api/#joining-two-tables","title":"Joining two tables","text":"<p>See the example in 00_join_order_customer.java. The statements may run forever. </p>"},{"location":"coding/table-api/#a-deduplication-example","title":"A deduplication example","text":"<p>The deduplication of record over a time window is a classical pattern. See this SQL query with the following Table API implementation</p> <pre><code>Table source;\n</code></pre>"},{"location":"coding/table-api/#confluent-tools-for-printing-and-stop-statement","title":"Confluent tools for printing and stop statement","text":"<p>See this git repository</p>"},{"location":"coding/table-api/#examples-of-github-repo","title":"Examples of github repo","text":"<ul> <li>Timo Walther's flink api examples in Java</li> </ul>"},{"location":"coding/table-api/#define-data-flows","title":"Define data flows","text":"<p>A TablePipeline describes a flow of data from source(s) to sink. We can also use </p> <p>The pipeline flow can use different services, defined in separate Java classes. Those classes may be reusable. The environment needs to be passed to each service, as this is the environment which includes all the Table API functions.</p> <p>Some code is in this folder.</p>"},{"location":"coding/table-api/#how-to","title":"How to","text":"Create a data generator <p>There is the FlinkFaker tool that seems to be very efficient to send different types of data. It is using Datafaker Java library, which can be extended to add our own data provider. FlinkFaker jar is added to the custom flink image in the Dockerfile. The challenges will be to remote connect to the compute-pool as defined in Confluent Cloud.</p> Connect to Confluent Cloud remotely <p>Define the cloud.properties and then use the Confluent API</p> <pre><code>    import io.confluent.flink.plugin.ConfluentSettings;\n    import org.apache.flink.table.api.EnvironmentSettings;\n    import org.apache.flink.table.api.TableEnvironment;\n\n    public static void main(String[] args) {\n        EnvironmentSettings settings = ConfluentSettings.fromResource(\"/cloud.properties\");\n        TableEnvironment env = TableEnvironment.create(settings);\n</code></pre> <p>With environment variables</p> <pre><code>ConfluentSettings settings = ConfluentSettings.fromGlobalVariables();\n</code></pre> Create a table with Kafka topic as persistence in Confluent Cloud? <pre><code>import io.confluent.flink.plugin.ConfluentSettings;\nimport io.confluent.flink.plugin.ConfluentTableDescriptor;\n//...\nenv.createTable(\n        TARGET_TABLE1,\n        ConfluentTableDescriptor.forManaged()\n            .schema(\n                    Schema.newBuilder()\n                            .column(\"user_id\", DataTypes.STRING())\n                            .column(\"name\", DataTypes.STRING())\n                            .column(\"email\", DataTypes.STRING())\n                            .build())\n            .distributedBy(4, \"user_id\")\n            .option(\"kafka.retention.time\", \"0\")\n            .option(\"key.format\", \"json-registry\")\n            .option(\"value.format\", \"json-registry\")\n            .build());\n</code></pre> Access to the schema of an existing topic / table? <pre><code>import org.apache.flink.table.api.DataTypes;\n//...\nDataType productsRow = env.from(\"examples.marketplace.products\")\n                .getResolvedSchema()\n                .toPhysicalRowDataType();\nList&lt;String&gt; columnNames = DataType.getFieldNames(productsRow);\nList&lt;DataType&gt; columnTypes = DataType.getFieldDataTypes(productsRow);\n// use in the schema function to create a new topic ...\n        Schema.newBuilder()\n                .fromFields(columnNames, columnTypes)\n                .column(\"additionalColumn\", DataTypes.STRING())\n                .build()\n</code></pre> How to split records to two topic, using StatementSet? <pre><code>StatementSet statementSet = env.createStatementSet()\n                    .add(\n                        env.from(\"`examples`.`marketplace`.`orders`\")\n                           .select($(\"product_id\"), $(\"price\"))\n                           .insertInto(\"PricePerProduct\"))\n                    .add(\n                        env.from(\"`examples`.`marketplace`.`orders`\")\n                           .select($(\"customer_id\"), $(\"price\"))\n                           .insertInto(\"PricePerCustomer\"));\n</code></pre>"},{"location":"coding/table-api/#deeper-dive","title":"Deeper dive","text":"<ul> <li>See this git repo: Learn-apache-flink-table-api-for-java-exercises. </li> <li>See the Table API in Java documentation.</li> <li>Connecting the Apache Flink Table API to Confluent Cloud with matching github which part of this code was ported into flink-sql-demos/02-table-api-java</li> </ul> <p>TO WORK ON</p>"},{"location":"coding/table-api/#lower-level-java-based-programming-model","title":"Lower level Java based programming model","text":"<ul> <li>Start Flink server using docker (start with docker compose or on Kubernetes). </li> <li>Start by creating a java application (quarkus create app for example or using maven) and a Main class. See code in flink-sql-quarkus folder.</li> <li>Add dependencies in the pom</li> </ul> <pre><code>      &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n      &lt;/dependency&gt;\n        &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n        &lt;scope&gt;provided&lt;/scope&gt;\n      &lt;/dependency&gt;\n      &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;\n        &lt;artifactId&gt;flink-table-planner-loader&lt;/artifactId&gt;\n        &lt;version&gt;${flink-version}&lt;/version&gt;\n        &lt;scope&gt;provided&lt;/scope&gt;\n      &lt;/dependency&gt;\n</code></pre> <pre><code>public class FirstSQLApp {\n public static void main(String[] args) {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);\n</code></pre> <p>The <code>TableEnvironment</code> is the entrypoint for Table API and SQL integration. See Create Table environment</p> <p>A TableEnvironment maintains a map of catalogs of tables which are created with an identifier. Each identifier consists of 3 parts: catalog name, database name and object name.</p> <pre><code>    // build a dynamic view from a stream and specifies the fields. here one field only\n    Table inputTable = tableEnv.fromDataStream(dataStream).as(\"name\");\n\n    // register the Table object in default catalog and database, as a view and query it\n    tableEnv.createTemporaryView(\"clickStreamsView\", inputTable);\n</code></pre> <p></p> <p>Tables can be either temporary, tied to the lifecycle of a single Flink session, or permanent, making them visible across multiple Flink sessions and clusters.</p> <p>Queries such as SELECT ... FROM ... WHERE which only consist of field projections or filters are usually stateless pipelines. However, operations such as joins, aggregations, or deduplications require keeping intermediate results in a fault-tolerant storage for which Flink\u2019s state abstractions are used.</p>"},{"location":"coding/table-api/#etl-with-table-api","title":"ETL with Table API","text":"<p>See code: TableToJson</p> <pre><code>public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv);\n\n        final Table t = tableEnv.fromValues(\n\n            row(12L, \"Alice\", LocalDate.of(1984, 3, 12)),\n            row(32L, \"Bob\", LocalDate.of(1990, 10, 14)),\n            row(7L, \"Kyle\", LocalDate.of(1979, 2, 23)))\n        .as(\"c_id\", \"c_name\", \"c_birthday\")\n        .select(\n                jsonObject(\n                JsonOnNull.NULL,\n                    \"name\",\n                    $(\"c_name\"),\n                    \"age\",\n                    timestampDiff(TimePointUnit.YEAR, $(\"c_birthday\"), currentDate())\n                )\n        );\n\n        tableEnv.toChangelogStream(t).print();\n        streamEnv.execute();\n    }\n</code></pre>"},{"location":"coding/table-api/#join-with-a-kafka-streams","title":"Join with a kafka streams","text":"<p>Join transactions coming from Kafka topic with customer information.</p> <pre><code>    // customers is reference data loaded from file or DB connector\n    tableEnv.createTemporaryView(\"Customers\", customerStream);\n    // transactions come from kafka\n    DataStream&lt;Transaction&gt; transactionStream =\n        env.fromSource(transactionSource, WatermarkStrategy.noWatermarks(), \"Transactions\");\n    tableEnv.createTemporaryView(\"Transactions\", transactionStream\n    tableEnv\n        .executeSql(\n            \"SELECT c_name, CAST(t_amount AS DECIMAL(5, 2))\\n\"\n                + \"FROM Customers\\n\"\n                + \"JOIN (SELECT DISTINCT * FROM Transactions) ON c_id = t_customer_id\")\n        .print();\n</code></pre>"},{"location":"coding/udf_sql/","title":"User Defined Functions","text":"<p>User-defined functions (UDFs) are extension to Flink SQL and Table API for frequently used logic and custom integration. It can be written in Java or PyFlink.</p> <p>If an operation cannot be expressed directly using Flink's standard SQL syntax or built-in functions (e.g., integrating a third-party library, implementing a proprietary business logic, or performing a complex machine learning inference), a UDF provides the necessary capability to execute that custom code within the stream or batch job.</p> <p>Developers can leverage ny existing libraries like Geospatial calculation, Math computation, to implement the UDF.</p>"},{"location":"coding/udf_sql/#four-types-of-udf","title":"Four Types of UDF","text":"UDF Type Description Input to Output Mapping Example Use Case Scalar Function Maps a set of scalar input values to a single, new scalar output value. 1 row -&gt; 1 row Formatting a string, calculating an encryption key. Table Function Maps a set of scalar input values to one or more rows (a new table). 1 row -&gt; N rows Splitting a single column into multiple rows. Aggregate Function Maps the values of multiple input rows to a single scalar aggregate value. N rows -&gt; 1 row Calculating a custom weighted average or variance. Table Aggregate Function Maps the values of multiple input rows to multiple output rows. N rows -&gt; M rows Calculating a running \"top-N\" list for each group."},{"location":"coding/udf_sql/#implementation-approach","title":"Implementation approach","text":"<p>See Apache flink UDF implementation guide.</p> <p>For developer the steps are:</p> <ol> <li>Develop a Class to extends a <code>org.apache.flink.table.functions.ScalarFunction</code> or <code>org.apache.flink.table.functions.TableFunction</code></li> <li>Implement one of the eval function</li> <li>Add constructor with empty parameters and more constructors if needed</li> <li>Prefer specifying the parameter types and function return type, specially for TableFunction</li> <li>Build a uber jar</li> <li>Deploy to Confluent Cloud or into the lib folder of CP Flink Application or into the lib folder of the OSS Flink distribution.</li> </ol> <p>On Confluent Cloud, be sure to use log4j to get function logs and wrap code into try .. .catch. See log debug messages documentation.</p>"},{"location":"coding/udf_sql/#iterate-on-udf-development","title":"Iterate on UDF development","text":"<p>In some case we need to iterate on the deployment of new UDF version. It is possible to deploy UDF with different version.</p> <ul> <li>Need to drop the function:     <pre><code>drop function USERS_IN_GROUPS\n</code></pre></li> <li>Delete the artifacts</li> <li>Upload the new jar as new artifact</li> <li>Then recreate it with the new artifact id</li> </ul>"},{"location":"coding/udf_sql/#examples","title":"Examples","text":"<ul> <li>See this repository to get a set of reusable UDFs implemented as solution for generic problems asked by our customers.</li> <li>See also the Confluent documentation on UDF and a Confluent git repo with some sample UDFs.</li> </ul>"},{"location":"coding/udf_sql/#table-api-examples","title":"Table API examples","text":"<ul> <li>Confluent Flink table api example with UDF.</li> </ul>"},{"location":"coding/udf_sql/#extending-base-apis","title":"Extending base APIs","text":""},{"location":"coding/udf_sql/#scalar-function","title":"Scalar function","text":"<p>Scalar function generates a unique value.</p> <p>The product documentation has all the details.</p> <p>Use AsyncScalarFunction when interacting with external systems. Use thread pools, initialized in constructor, to manage connection multiplexing.</p>"},{"location":"coding/udf_sql/#table-function","title":"Table function","text":"<p>Table function returns an arbitrary number of rows (or structured types) as output. Single scalar value can be emitted and will be implicitly wrapped into a row by the runtime.</p> <pre><code>import org.apache.flink.table.annotation.DataTypeHint;\nimport org.apache.flink.table.annotation.FunctionHint;\n\n@FunctionHint(output = @DataTypeHint(\"ROW&lt;word STRING, length INT&gt;\"))\npublic static class SplitFunction extends TableFunction&lt;Row&gt; {\n\n    public void eval(String str) {...\n</code></pre> <p>See details in the product documentation.</p> <p>AsyncTableFunction should be used to generate n rows when integrating with external systems.</p> <p>The function is used with SQL <code>LATERAL TABLE(&lt;TableFunction&gt;) with JOIN or LEFT JOIN</code> with an ON TRUE join condition. See Lateral table section</p>"},{"location":"coding/udf_sql/#aggregate-function","title":"Aggregate Function","text":"<p>Aggregate user defined function, maps scalar values of multiple rows to a new scalar value, using accumulator. The accumulator is an intermediate data structure that stores the aggregated values until a final aggregation result is computed. </p> <p>The <code>accumulate()</code> method is called for each input row to update the accumulator.</p> <p>For more detail, see the product documentation.</p>"},{"location":"coding/udf_sql/#deeper-considerations","title":"Deeper considerations","text":"<ul> <li>Scalar UDFs in the Table API/SQL are generally expected to be stateless</li> <li>Stateful with hashmap to keep state is risky as the map will be wiped if the Task Manager restarts. We need a distributed cache, or use KeyedProcessFunction with the ValueState or ListState. If the UDTF's output depends on internal state that changes over time, it can sometimes lead to unexpected results in complex joins or aggregations. For inner join lateral,  when the logic decides not to call collect(), the entire row is filtered out of the result. This effectively acts as a stateful filter. For left join lateral, the non call to collect() will return null, and it is possible to keep the input record from the left table. </li> <li>Process Table Function may be needed to define those complex stateful processing as PTF has access to Flink\u2019s managed state, event-time and timer services, and underlying table changelogs. When invoking a PTF, the system automatically adds implicit arguments for state and time management alongside the user-defined input arguments. </li> </ul>"},{"location":"coding/udf_sql/#deploying-to-confluent-cloud","title":"Deploying to Confluent Cloud","text":"<ul> <li>Get FlinkDeveloper RBAC to be able to manage workspaces and artifacts</li> <li> <p>Use the Confluent CLI to upload the jar file. Example from GEO_DISTANCE:     <pre><code>confluent environment list\n# then in your environment\nconfluent flink artifact create geo_distance --artifact-file target/geo-distance-udf-1.0-0.jar --cloud aws --region us-west-2 --environment env-nk...\n</code></pre></p> <pre><code>+--------------------+--------------+\n| ID                 | cfa-nx6wjz   |\n| Name               | geo_distance |\n| Version            | ver-nxnnnd   |\n| Cloud              | aws          |\n| Region             | us-west-2    |\n| Environment        | env-nknqp3   |\n| Content Format     | JAR          |\n| Description        |              |\n| Documentation Link |              |\n+--------------------+--------------+\n</code></pre> <p>Also visible in the Artifacts menu </p> </li> <li> <p>UDFs are registered inside a Flink database. It may take some time.     <pre><code>CREATE FUNCTION GEO_DISTANCE\nAS\n'io.confluent.udf.GeoDistanceFunction'\nUSING JAR 'confluent-artifact://cfa-...';\n</code></pre></p> </li> <li> <p>Use the function to compute distance between Paris and London:     </p> </li> </ul>"},{"location":"coding/udf_sql/#runtime-explanation","title":"Runtime explanation","text":"<ul> <li>UDF invocations are batched (may wait up to 500ms), the runtime tries to accumulate several records before it calls them. </li> <li>In Confluent Cloud the UDFs actually run in a separate pod for security isolation, which increases the consumption.</li> </ul>"},{"location":"concepts/","title":"Flink Key Concepts","text":"<p>windowing# Apache Flink - Core Concepts</p> Version <ul> <li>Update 07/2025 - Review done with simplification and avoid redundancies.</li> <li>Update - revision 11/23/25</li> <li>2/2026: Refactor content as part of the new cookbok chapter</li> </ul>"},{"location":"concepts/#quick-reference","title":"Quick Reference","text":"<ul> <li>Core Concepts</li> <li>Stream Processing</li> <li>Architecture</li> <li>State Management</li> <li>Time Handling</li> </ul>"},{"location":"concepts/#why-flink","title":"Why Flink?","text":"<p>Traditional data processing faces key challenges:</p> <ul> <li>Transactional Systems: Monolithic applications with shared databases create scaling challenges</li> <li>Analytics Systems: ETL pipelines create stale data and require massive storage and often duplicate data across systems.  ETLs extract data from a transactional database, transform it into a common representation (including validation, normalization, encoding, deduplication, and schema transformation), and then load the new records into the target analytical database. These processes are run periodically in batches.</li> </ul> <p>Flink enables real-time stream processing with three application patterns:</p> <ol> <li>Event-Driven Applications: Reactive systems using messaging</li> <li>Data Pipelines: Low-latency transformation and enrichment  </li> <li>Real-Time Analytics: Immediate computation and action on streaming data</li> </ol> <p>Flink Apps bring stateful processing to serverless. Developers write event handlers in Java (similar to serverless functions) but with annotations for state, timers, and multi-stream correlation. State is automatically partitioned, persisted, and restored. Event-time processing handles late-arriving data correctly. Exactly-once guarantees ensure critical business logic executes reliably. </p>"},{"location":"concepts/#overview-of-apache-flink","title":"Overview of Apache Flink","text":"<p>Apache Flink is a distributed stream processing engine for stateful computations over bounded and unbounded data streams. It's  become an industry standard due to its performance and comprehensive feature set.</p> <p>Key Features:</p> <ul> <li>Low Latency Processing: Offers event time semantics for consistent and accurate results, even with out-of-order events.</li> <li>Exactly-Once Consistency: Ensures reliable state management to avoid duplicates and not loosing message.</li> <li>High Throughput: Achieves millisecond latencies while processing millions of events per second.</li> <li>Powerful APIs: Provides APIs for operations such as map, reduce, join, window, split, and connect.</li> <li>Fault Tolerance and High Availability: Supports failover for task manager nodes, eliminating single points of failure.</li> <li>Multilingual Support: Enables streaming logic implementation in Java, Scala, Python, and SQL.</li> <li>Extensive Connectors: Integrates seamlessly with various systems, including Kafka, Cassandra, Pulsar, Elasticsearch, File system, JDBC complain  Database, HDFS and S3.</li> <li>Kubernetes Native: Supports containerization and deployment on Kubernetes with dedicated k8s operator to manage session job or application as  well as job and task managers.</li> <li>Dynamic Code Updates: Allows for application code updates and job migrations across different Flink clusters without losing application state.</li> <li>Batch Processing: Also transparently support traditional batch processing workloads as reading at rest table becomes a stream in Flink</li> </ul>"},{"location":"concepts/#stream-processing-concepts","title":"Stream Processing Concepts","text":"<p>A Flink application runs as a job - a processing pipeline structured as a directed acyclic graph (DAG) with:</p> <ul> <li>Sources: Read from streams (Kafka, Kinesis, Queue, CDC etc.)</li> <li>Operators: Transform, filter, enrich data</li> <li>Sinks: Write results to external systems</li> </ul> Data flow as directed acyclic graph <p>Operations can run in parallel across partitions. Some operators (like Group By) require data reshuffling or repartitioning.</p>"},{"location":"concepts/#bounded-and-unbounded-data","title":"Bounded and unbounded data","text":"<p>A Stream is a sequence of events, bounded or unbounded:</p> Bounded and unbounded event sequence <p>Apache Flink supports batch processing by processing all the data in one job with bounded dataset. It is used when we need all the data, to assess trend, develop AI model, and with a focus on throughput instead of latency. Jobs are run when needed, on input that can be pre-sorted by time or by any other key.</p> <p>The results are reported at the end of the job execution. Any failure means to do of full restart of the job.</p> <p>Hadoop was designed to do batch processing. Flink has capability to replace the Hadoop map-reduce processing.</p> <p>When latency is a major requirements, like monitoring and alerting, fraud detection then streaming is the only choice.</p>"},{"location":"concepts/#dataflow","title":"Dataflow","text":"<p>In Flink 2.1.x, applications are composed of streaming dataflows. Dataflow can consume from Kafka, Kinesis, Queue, and any data sources. A typical high level view of Flink app is presented in figure below:</p> A Flink application -  src: apache Flink product doc <p>Stream processing includes a set of functions to transform data, and to produce a new output stream. An operator in Flink is a component that performs a specific operation on the data stream. Operations can be transformations (e.g., map, filter, reduce); an action (e.g., print, save); or, a source or sink.  Intermediate steps compute rolling aggregations like min, max, mean, or collect  and buffer records in time window to compute metrics on a finite set of events. </p> Streaming Dataflow  src: apache Flink product doc <p>Data is partitioned for parallel processing.Flink performs computations using tasks, subtasks and operators. Each stream has multiple partitions, and each operator has multiple tasks for scalability. Tasks are the basic unit of execution in Flink. A task represents a piece of work that gets scheduled and executed by the Flink runtime. </p> <p>Each task is responsible for executing a specific part of the data processing logic defined by Flink. Tasks are parallelizable, meaning you can have multiple instances of a task running in parallel to process data streams more efficiently.  A subtask in Flink is a parallel instance of a task. A task can be divided into multiple subtasks that can all be running at the same time. Each subtask processes a portion of the data leading to more efficient data processing. </p> Distributed processing  src: apache Flink product doc <p>Operations like GROUP BY require data reshuffling across network, which can be costly but enables distributed aggregation.</p> <pre><code>INSERT INTO results\nSELECT key, COUNT(*) FROM events\nWHERE color &lt;&gt; blue\nGROUP BY key;\n</code></pre>"},{"location":"concepts/#runtime-architecture","title":"Runtime architecture","text":"<p>Flink consists of a Job Manager and <code>n</code> Task Managers deployed on <code>k</code> hosts. </p> Main Flink Components <p>Client applications compile batch or streaming applications into a dataflow graph. Client submits the DAG to the JobManager. The JobManager controls the execution of one or more applications. Developers submit their application (jar file or SQL statements) via CLI, REST call or k8s manifest. Job Manager receives the Flink application for execution and builds a Task Execution Graph from the defined JobGraph. It manages job submission which parallelizes the job and distributes slices of the Data Stream flow, the developers have defined. Each parallel slice of the job is a task that is executed in a task slot.  </p> <p>Once the job is submitted, the Job Manager is scheduling the job to different task slots within the Task Manager. The Job manager may create resources from a computer pool, or when deployed on kubernetes, it creates pods. </p> <p>The Resource Manager manages Task Slots and leverages an underlying orchestrator, like Kubernetes or Yarn (deprecated).</p> <p>A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager:</p> Sequence flow from job submission <p>The Dispatcher exposes API to submit applications for execution. It hosts the user interface too.</p> <p>Once the job is running, the Job Manager is responsible to coordinate the activities of the Flink cluster, like checkpointing, and restarting task manager that may have failed.</p> <p>Tasks are loading the data from sources, do their own processing and then send data among themselves for repartitioning and rebalancing, to finally push results out to the sinks.</p> <p>When Flink is not able to process a real-time event, it may have to buffer it, until other necessary data has arrived. This buffer has to be persisted in longer storage, so data are not lost if a task manager fails and has to be restarted. In batch mode, the job can reload the data from the beginning. In batch the results are computed once the job is done (count the number of record like <code>select count(*) AS</code>count<code>from bounded_pageviews;</code> return one result), while in streaming mode, each event may be the last one received, so results are produced incrementally, after every events or after a certain period of time based on timers.</p> Parameters <ul> <li>taskmanager.numberOfTaskSlots: 2</li> </ul> <p>Only one Job Manager is active at a given point of time, and there may be <code>n</code> Task Managers. It is a single point of failure, but it startes quickly and can leverage the checkpoints data to restart its processing.</p> <p>There are different deployment models: </p> <ul> <li>Deploy on executing cluster, this is the session mode. Use session cluster to run multiple jobs: we need a separate JobManager container for that. </li> <li>Per job mode: spin up a cluster per job submission. This provides better resource isolation. </li> <li>Application mode: creates a cluster per app with the <code>main()</code> function executed on the JobManager. It can include multiple jobs but they run inside the app. It allows for saving the required CPU cycles, but also save the bandwidth required for downloading the dependencies locally.</li> </ul> <p>Flink can run on any common resource manager like Hadoop Yarn, Mesos, or Kubernetes. For development purpose, we can use docker images to deploy a Session or Job cluster.</p> <p>See also deployment to Kubernetes</p> <p>The new K8s operator, deploys and monitors Flink Application and Session deployments.</p>"},{"location":"concepts/#state-management","title":"State Management","text":"<p>In Flink, state consists of information that an operator remembers about past events, which is used to influence the processing of future events.</p>"},{"location":"concepts/#core-concept-of-state","title":"Core Concept of State","text":"<ul> <li> <p>Stateful operations are required for many common use cases, such as:</p> <ul> <li>Windowing: Aggregating events over time (e.g., sum of sales per minute).</li> <li>Pattern Detection: Tracking a sequence of events to find specific patterns.</li> <li>Machine Learning: Updating model parameters based on a stream of data.</li> <li>Analytics: Maintaining counters or profiles for unique users.</li> </ul> </li> <li> <p>We can dissociate different type of operations:</p> <ul> <li>Stateless Operations process each event independently without retaining information:<ul> <li>Basic operations: <code>INSERT</code>, <code>SELECT</code>, <code>WHERE</code>, <code>FROM</code> </li> <li>Scalar/table functions, projections, filters</li> </ul> </li> <li>Stateful Operations maintain state across events for complex processing:<ul> <li><code>JOIN</code> operations (except <code>CROSS JOIN UNNEST</code>)</li> <li><code>GROUP BY</code> aggregations (windowed/non-windowed)</li> <li><code>OVER</code> aggregations and <code>MATCH_RECOGNIZE</code> patterns</li> </ul> </li> </ul> </li> </ul>"},{"location":"concepts/#types-of-state","title":"Types of State","text":"<p>Flink distinguishes between two main categories of state:</p> <ul> <li>Managed State: Handled by the Flink runtime. Flink manages the storage, rescaling, and fault tolerance of this state.</li> <li>Raw State: Handled by the user in their own data structures. It is generally not recommended as Flink cannot automatically manage it during rescaling.</li> </ul> <p>Within Managed State, there are several sub-types:</p> <ul> <li> <p>Keyed State: Tied to a specific key (e.g., a user ID). It is partitioned across the cluster so that each key's state is handled by exactly one parallel task. Flink maintains one state instance per key value and Flink partitions all records with the same key to the operator task that maintains the state for this key. The key-value map is sharded across all parallel tasks:</p> <p> Keyes states </p> <p>Each task maintains its state locally to improve latency. For small state, the state backends will use JVM heap, but for larger state RocksDB is used. A state backend takes care of checkpointing the state of a task to a remote and persistent storage.</p> </li> <li> <p>Operator State: Tied to a parallel operator instance rather than a key. It is often used for source/sink connectors (e.g., tracking Kafka offsets).</p> </li> <li>Broadcast State: A special type of operator state where the state is duplicated across all parallel tasks of an operator.</li> </ul> <p>Flink keeps state of its processing for Fault tolerance. It fact, Flink uses stream replay and checkpointing. </p> <p>All data maintained by a task and used to compute the results of a function belong to the state of the task. Function may use  pairs to store values, and may implement the ChekpointedFunctions to make local state fault tolerant. <p>While processing the data, the task can read and update its state and computes its results based on its input data and state.</p> <p>State management may address very large states, and no state is lost in case of failures.</p> <p>Within a DAG, each operator needs to register its state. Operator state is scoped to an operator task: all records processed by the same parallel task have access to the same state.</p> <p>State can grow over time. Local state persistence improves latency while remote checkpointing ensures fault tolerance.</p> Flink and Kafka integration with state management <p>Flink ensures fault tolerance through checkpoints and savepoints that persistently store application state.</p>"},{"location":"concepts/#fault-tolerance-and-consistency","title":"Fault Tolerance and Consistency","text":"<ul> <li> <p>Checkpoints: Flink periodically takes distributed snapshots of the state and stores them in durable storage.</p> </li> <li> <p>Exactly-once Semantics: By combining checkpoints with replayable data sources, Flink guarantees that each event affects the state exactly once, even in the event of a failure.</p> </li> <li> <p>Savepoints: These are manually triggered snapshots used for operational tasks like application upgrades, A/B testing, or migrating to a different cluster.</p> </li> </ul> <p>See deeper explanations in the cookbook chapter</p>"},{"location":"concepts/#state-backends","title":"State Backends","text":"<p>State backends determine how the state is physically stored. Options typically include:</p> <ul> <li>HashMap State Backend: Stores state as objects on the JVM heap (very fast, but limited by memory).</li> <li>Embedded RocksDB: Stores state in an embedded database on local disk. RocksDB is a key-value store based on Log-Structured Merge-Trees (LSM Trees). Flink organizes state into \"Key Groups.\" Each RocksDB instance on a TaskManager handles a specific set of these groups. It saves asynchronously. Serializes using bytes. But there is a limit to the size per key and valye of 2^31 bytes. Supports incremental checkpoints which is key for maintaining performance as state grows into the terabytes.<ul> <li>The process: When an operator updates state, it writes to the RocksDB MemTable and a Write-Ahead Log (WAL). Once the MemTable is full, it is flushed to disk as a static SST file.</li> <li>ForStState uses tree structured k-v store. May use object storage for remote file systems. Allows Flink to scale the state size beyond the local disk capacity of the TaskManager. </li> <li><code>HashMapStateBackend</code> use Java heap to keep state, as java object. So unsafe to reuse!.</li> </ul> </li> <li>ForSt (Disaggregated): The 2.x preference for cloud-native scaling and fast recovery, by using Distributed File Systems (DFS).</li> </ul>"},{"location":"concepts/#sources-of-knowledge","title":"Sources of knowledge","text":"<ul> <li>Stateful processing - Apache Flink documentation.</li> <li>Confluent state management documentation.</li> <li>See 'working with state' from Flink documentation.</li> </ul>"},{"location":"concepts/#windowing","title":"Windowing","text":"<p>Windows group stream events into finite buckets for processing. Flink provides window table-valued functions (TVF): Tumbling, Hop, Cumulate, Session.</p>"},{"location":"concepts/#tumbling-windows","title":"Tumbling windows","text":"<ul> <li> <p>Tumbling window assigns events to non-overlapping buckets of fixed size. Records are assigned to the window based on an event-time attribute field, specified by the DESCRIPTOR() function. Once the window boundary is crossed, all events within that window are sent to an evaluation function for processing. </p> </li> <li> <p>Count-based tumbling windows define how many events are collected before triggering evaluation. </p> </li> <li> <p>Time-based tumbling windows define time interval (e.g., n seconds) during which events are collected. The amount of data within a window can vary depending on the incoming event rate.      <pre><code>.keyBy(...).window(TumblingProcessingTimeWindows.of(Time.seconds(2)))\n</code></pre></p> <p>in SQL:</p> <pre><code>-- computes the sum of the price in the orders table within 10-minute tumbling windows\nSELECT window_start, window_end, SUM(price) as `sum`\nFROM TABLE(\n    TUMBLE(TABLE `examples`.`marketplace`.`orders`, DESCRIPTOR($rowtime), INTERVAL '10' MINUTES))\nGROUP BY window_start, window_end;\n</code></pre> </li> </ul> Tumbling window concept <ul> <li>See example TumblingWindowOnSale.java in my-fink folder and to test it, do the following:</li> </ul> <pre><code># Start the SaleDataServer that starts a server on socket 9181 and will read the avg.txt file and send each line to the socket\njava -cp target/my-flink-1.0.0-SNAPSHOT.jar jbcodeforce.sale.SaleDataServer\n# inside the job manager container started with \n`flink run -d -c jbcodeforce.windows.TumblingWindowOnSale /home/my-flink/target/my-flink-1.0.0-SNAPSHOT.jar`.\n# The job creates the data/profitPerMonthWindowed.txt file with accumulated sale and number of record in a 2 seconds tumbling window\n(June,Bat,Category5,154,6)\n(August,PC,Category5,74,2)\n(July,Television,Category1,50,1)\n(June,Tablet,Category2,142,5)\n(July,Steamer,Category5,123,6)\n...\n</code></pre>"},{"location":"concepts/#sliding-windows","title":"Sliding windows","text":"<ul> <li> <p>Sliding windows allows for overlapping periods, meaning an event can belong to multiple buckets. This is particularly useful for capturing trends over time. The window sliding time parameter defines the duration of the window and the interval at which new windows are created. For example, in the following code snippet defines a new 2-second window is created every 1 second:</p> <pre><code>.keyBy(...).window(SlidingProcessingTimeWindows.of(Time.seconds(2), Time.seconds(1)))\n</code></pre> <p>As a result, each event that arrives during this period will be included in multiple overlapping windows, enabling more granular analysis of the data stream.</p> </li> </ul> Sliding window concept"},{"location":"concepts/#session-window","title":"Session window","text":"<p>Session window begins when the data stream processes records and ends when there is a defined period of inactivity. The inactivity threshold is set using a timer, which determines how long to wait before closing the window.</p> <pre><code>.keyBy(...).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))\n</code></pre> <p>The operator creates one window for each data element received.  If there is a gap of 5 seconds without new events, the window will close. This makes session windows particularly useful for scenarios where you want to group events based on user activity or sessions of interaction, capturing the dynamics of intermittent data streams effectively.</p> Session window concept"},{"location":"concepts/#global","title":"Global","text":"<ul> <li>Global: One window per key, requires explicit triggers</li> </ul> <p>See Windowing TVF documentation.</p> <pre><code>.keyBy(0)\n.window(GlobalWindows.create())\n.trigger(CountTrigger.of(5))\n</code></pre> <p>See Windowing Table-Valued Functions details in Confluent documentation.</p>"},{"location":"concepts/#trigger","title":"Trigger","text":"<p>A Trigger in Flink, determines when a window is ready to be processed. </p> <p>Each window has a default trigger associated with it. For example, a tumbling window might have a default trigger set to 2 seconds, while a global window requires an explicit trigger definition.</p> <p>You can implement custom triggers by creating a class that implements the Trigger interface, which includes methods such as onElement(..), onEventTime(..), and onProcessingTime(..).</p> <p>Flink provides several default triggers::</p> <ul> <li>EventTimeTrigger fires based upon progress of event time</li> <li>ProcessingTimeTrigger fires based upon progress of processing time</li> <li>CountTrigger fires when # of elements in a window exceeds a specified parameter.</li> <li>PurgingTrigger is used for purging the window, allowing for more flexible management of state.</li> </ul>"},{"location":"concepts/#eviction","title":"Eviction","text":"<p>Evictor is used to remove elements from a window either after the trigger fires or before/after the window function is applied. The specific logic for removing elements is application-specific and can be tailored to meet the needs of your use case.</p> <p>The predefined evictors: </p> <ul> <li>CountEvictor removes elements based on a specified count, allowing for fine control over how many elements remain in the window.</li> <li>DeltaEvictor evicts elements based on the difference between the current and previous counts, useful for scenarios where you want to maintain a specific change threshold.</li> <li>TimeEvictor removes elements based on time, allowing you to keep only the most recent elements within a given time frame.</li> </ul>"},{"location":"concepts/#event-time","title":"Event time","text":"<p>Time is a central concept in stream processing and can have different interpretations based on the context of the flow or environment:</p> <ul> <li>Processing Time refers to the system time of the machine executing the task. It offers the best performance and lowest latency since it relies on the local clock. But it may lead to no deterministic results due to factors like ingestion delays, parallel execution, clock synch, backpressure...</li> <li>Event Time is the timestamp embedded in the record at the event source level. Using event-time ensures consistent and deterministic results, regardless of the order in which events are processed. This is crucial for accurately reflecting the actual timing of events.</li> <li>Ingestion Time denotes the time when an event enters the Flink system. It captures the latency introduced during the event's journey into the processing framework.</li> </ul> <p>In any time window, the order of arrival may not be guarantee, and some events with an older timestamp may fall outside of the time window boundaries. To address this challenge, particularly when computing aggregates, it's essential to ensure that all relevant events have arrived within the intended time frame.</p> <p>The watermark serves as a heuristic for this purpose.</p>"},{"location":"concepts/#watermarks","title":"Watermarks","text":"<p>Watermarks are special markers indicating event-time progress in streams to keep track of how time progress and to handle out-of-order records. This is the core mechanims to trigger computation at <code>event-time</code>. They determine when windows can safely close by estimating when all events for a time period have arrived.</p>"},{"location":"concepts/#key-concepts","title":"Key Concepts","text":"<ul> <li>Generated in the data stream at regular intervals, they are part of the source operator processing or immediately after it. Each parallel subtask of the source typically generates its watermarks independently, based on the events it processes. This is especially important for partitioned sources like Kafka, where each source subtask might read from one or more partitions. </li> <li>Watermark generation logic is defined using a WatermarkStrategy. This strategy is typically applied directly when you define the data source. It tells Flink how to extract the event time timestamp from each incoming data record. And it determines how to generate the actual watermark based on those timestamps.     <pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n    // 1. Define the Watermark Strategy\n    WatermarkStrategy&lt;MyEvent&gt; watermarkStrategy = \n        WatermarkStrategy\n            // This strategy is ideal for out-of-order data streams.\n            // It allows events up to 5 seconds late (out-of-order) to be processed.\n            .&lt;MyEvent&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5))\n\n            // 2. Define how to extract the event timestamp from the record\n            .withTimestampAssigner((event, timestamp) -&gt; event.getEventTime());\n\n    // 3. Apply the Strategy directly to the Source Connector\n    DataStream&lt;MyEvent&gt; stream = env\n        .fromSource(\n            // In a real application, this would be a KafkaSource, FileSource, etc.\n            // Here, we use a simple collection source for demonstration.\n            new DummyEventSource(), // Assuming a custom Flink Source implementation\n            watermarkStrategy, \n            \"My Event Source\"\n        );\n</code></pre></li> <li>Watermarks flow downstream alongside the data records</li> <li>Watermark timestamp = largest seen timestamp - estimated out-of-orderness. This timestamp are always increasing. </li> <li>Events arriving after watermarks are considered late and typically discarded</li> <li>The default strategy is designed for large-scale production workloads, requiring a significant volume of data (around 250 events per partition) before advancing the watermark and emitting results.</li> <li>Essential for triggering window computations in event-time processing</li> </ul> Watermark concept <p>Within a window, states are saved on disk and need to be cleaned once the window is closed. The watermark is the limit from where the Java garbage collection may occur. </p> <p>The out-of-orderness estimation serves as an educated guess and is defined for each individual stream. Watermarks are essential for comparing timestamps of events, allowing the  system to assert that no earlier events will arrive after the watermark's timestamp.</p> <p>Watermarks are crucial when dealing with multiple sources. In scenarios involving IoT devices and network latency, it's possible to receive an event with an earlier timestamp even  after the operator has already processed events with that timestamp from other sources. Importantly, watermarks are applicable to any timestamps and are not limited to window  semantics.</p> <p>When working with Kafka topic partitions, the absence of watermarks may represent some challenges. Watermarks are generated independently for each stream and partition. When two  partitions are combined, the resulting watermark will be the oldest of the two (min value), reflecting the point at which the system has complete information. If one partition stops receiving  new events, the watermark for that partition will not progress. To ensure that processing continues over time, an idle timeout configuration can be implemented.</p> <p>Each task has its own watermark, and at the arrival of a new watermark, it checks if it needs to advance its own watermark. When it is advanced, the task performs all triggered computations and emits all result records. The watermark is broadcasted to all output of the task.</p> <p>The watermark of a task is the mininum of all per-connection watermarks. Task with multiple input, like JOINs or UNIONs maintains a single watermark, which is the minimum between the input watermarks.</p> <p>Additionally, it is possible to configure the system to accept late events by specifying an <code>allowed lateness</code> period. This defines how late an element can arrive before it is discarded. Flink maintains the state of the window until the allowed lateness time has expired, allowing for flexible handling of late-arriving data while ensuring that the processing remains efficient and accurate.</p> <p>When using processing time, the watermark advances at regular intervals, typically every second. Events within the window are emitted for processing, once the watermark surpasses  the end of that window.</p> <p>Parallel watermarking is an example of getting data from 4 partitions with 2 kafka consumers and 2 windows:</p> Parallel watermarking <p>Shuffling is done as windows are computing some COUNT or GROUP BY operations. Event A arriving at 3:13, and B[3:20] on green partitions, and are processed by Window 1 which considers 60 minutes time between 3:00 and 4:00. </p> <p>The source connector sends a Watermark for each partition independently. If the out-of-orderness is set to be 5 minutes, a watermark is created with a timestamp 3:08 = 3:13 - 5 (partition 0) and at 3:15 (3:20 - 5) for partition 1. The generator sends the minimum of both. The timestamp reflects how complete the stream is so far: it could not be no more completed than the further behind which was event at 3:13, </p> <p>In the case of a partition does not get any events, as there is no watermark generated for this partition, it may mean the watermark does no advance, and as a side effect it prevents windows from producing events. To avoid this problem, we need to balance kafka partitions so none are empty or idle, or configure the watermarking to use idleness detection.</p>"},{"location":"concepts/#source-of-information","title":"Source of information","text":"<ul> <li>Confluent documentation</li> <li>Interesting enablement from Confluent, David Anderson</li> <li>An animated webapp to explain the Watermark concepts.</li> </ul>"},{"location":"concepts/#classical-issue-due-to-watermark","title":"Classical issue due to watermark","text":"<ul> <li> <p>Records may not being seen in output table or topic. When testing with only a few events, this fails to meet the initial \"safety margin\" of 250 events per partition. This causes the system to apply a massive 7-day default margin, which stalls the watermark indefinitely and prevents time windows from ever closing and producing a result.</p> </li> <li> <p>Stalled Joins with Idle Sources: When joining two streams, if one stream is idle or has very old data, its watermark remains far in the past. The join operator's watermark becomes the minimum of the two, effectively stalling the entire query and preventing any new join results from being produced, even when one stream is active.</p> </li> <li>Losing the Last Message: In a sparse stream of events, the very last event is correctly placed in its time window but remains buffered. Because no new event ever arrives to advance the watermark past the end of that final window, the window never closes, and the result for the last message is never produced, making it seem like Flink lost data.</li> </ul>"},{"location":"concepts/#monitoring-watermark","title":"Monitoring watermark","text":"<p>The following metrics are used at the operator and task level</p> <ul> <li><code>currentInputWatermark</code>: the last watermark received by the operator in its n inputs.</li> <li><code>currentOutputWatermark</code>: last emitted watermark by the operator</li> <li><code>watermarkAlignmentDrift</code>: current drift from the minimal watermakr emitted by all sources beloging to the same watermark group.</li> </ul> <p>Watermarks can be seen in Apache flink Console. </p> Confluent Cloud for Flink <ul> <li>The default watermark strategy is set to 180ms.</li> <li>There is a support for configurable late data handling to DLQ to avoid data drop. Developers choose between three options: \"pass,\" \"drop,\" or \"send to a dead letter queue (DLQ)</li> </ul>"},{"location":"concepts/#identify-which-watermark-is-calculated","title":"Identify which watermark is calculated","text":"<p>The approach is to add a virtual column to keep the Kafka partition number:</p> <pre><code>ALTER TABLE &lt;table_name&gt; ADD _part INT METADATA FROM 'partition' VIRTUAL;\n</code></pre> <p>then assess if there is a value on the \"Operator Watermark\" column with</p> <pre><code>SELECT\n  *,\n  _part AS `Row Partition`,\n  $rowtime AS `Row Timestamp`,\n  CURRENT_WATERMARK($rowtime) AS `Operator Watermark`\nFROM  &lt;table_name&gt;;\n</code></pre> <p>If not all partitions are included in the result, it may indicate a watermark issue with those partitions. We need to ensure that events are sent across all partitions. To test a statement, we can configure it to avoid being an unbounded query by consuming until the latest offset. This can be done by setting: <code>SET 'sql.tables.scan.bounded.mode' = 'latest-offset';</code></p> <p>Flink statement consumes data up to the most recent available offset at the job submission moment. Upon reaching this time, Flink ensures that a final watermark is propagated, indicating that all results are complete and ready for reporting. The statement then transitions into a 'COMPLETED' state.\"</p> <p>The table alteration can be undone with:</p> <pre><code>ALTER TABLE &lt;table_name&gt; DROP _part;\n</code></pre>"},{"location":"concepts/#data-skew","title":"Data Skew","text":"<p>This section assumes a lot of knowledge on sql, joins, stateful state. It is in the concept chapter because I am not sure where to put it otherwise.</p> <p>When dealing with large scale dataset and state, keys used for upsert operation, joins or aggregrations may be subject to data skew.  Hot keys are sent to the same Flink subtask. Those operator workers receive most of the records while others are idle. Scaling the number of task manager will not help, as the majority of records go to the same task. </p> <p>It is important to compute the number of keys found in left and right tables. NULL key may be found, and may be also very common. </p> <p>The following query is a standard approach to assess the percent allocation of all the data groups:</p> <pre><code>SELECT \n    column_name, \n    COUNT(*) AS rows_count,\nFROM \n    your_table_name\nGROUP BY \n    column_name;\n</code></pre> <p>If for example one value accounts for more than 30% of the rows then we face data skew.</p> <p>For join, Flink distributes rows based on key used in the on condition.</p> <p>It is then necessary to use a 'salting' key technique, by spreading the hot key to multiple processing tasks. The original join looks like:</p> <pre><code>select \n    u.*,\n    g.group_name\nfrom src_users u\njoin src_groups g on u.group_id = g.id\n</code></pre> <p>For that we need to add a column (the salt) to the skewed table and the smaller tllbe, and append a sequence number between 0 to N-1, where N is the number of buckets to use to repartition the data. See SEQUENCE UDTF</p> <p>Below is an example to create 3 buckets for each key of the slow table:</p> <pre><code>-- using the SEQUENCE UDF\ncreate view groups_salted as select\n   g.*,\n  S.salt_id as salt_id\nfrom `src_groups` as g\ncross join lateral table(SEQUENCE(1,3)) as S(salt_id)\n-- using the UNNEST\nCROSS JOIN UNNEST(ARRAY[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) as S(salt_id)\n</code></pre> <p>Same approach applies to the big table, if we can do it with a view <pre><code>create view users_salted as select\n  u.*,\n  S.salt_id as salt_id\nfrom `src_users` as u\ncross join lateral table(SEQUENCE(1,3)) as S(salt_id)\n</code></pre></p> <p>The joins now taking into account the combined key: <pre><code>select \n    u.*,\n    g.group_name\nfrom users_salted u\njoin groups_salted g on u.group_id = g.id and u.salt_id = g.salt_id\n</code></pre></p> <p>When the join needs to be temporal, we may need tables and not a view.</p> <pre><code>\n</code></pre> <p>To demonstrate the partitioning, use a sink topic with 3 partitions, and a partition key based on group_id with the first approach a lot of records are going one partition, while with the salty key, the sink  partition key will be group_id and salt_id and so will be spread against the 3 partitions.</p> <p>See matching demo scripts in flink-sql/04-joins/data_skew.</p>"},{"location":"concepts/#from-batch-to-real-time","title":"From batch to real-time","text":"<p>In business analytics there is a need to differentiate data tables according to their usage and reusability. There are two important concepts of this practice:</p> <ul> <li>The Dimensions, which provide the \u201cwho, what, where, when, why, and how\u201d context surrounding a business process event. Dimension tables contain the descriptive attributes used by BI applications for \ufb01ltering and grouping the facts. </li> <li>The Facts, which are the measurements that result from a business process event and are almost always numeric. The design of a fact table is entirely based on a physical activity, and not by the reports to produce from those facts. A fact table always contains foreign keys for each of its associated dimensions, as well as optional degenerate dimension keys and date/time stamps.</li> </ul>"},{"location":"concepts/#the-star-schema","title":"The star schema","text":"<p>The star schema, was defined at the end of the 80s, as a multi-dimensional data model to organize data in Date warehouse, to maintain history and by reducing the data duplication. A star schema is used to denormalize business data into dimensions and facts. The fact table connects to multiple other dimension tables along \"dimensions\" like time, or product.</p> <p></p> <p>The following project illustrates how to implement the star schema using Flink:</p> <ul> <li>Customer 360</li> <li>Transaction analytics</li> </ul> <p>In Flink a dimension may created via a SQL statement, and persisted as table with Kafka topic, JDBC table or files. When less reusable a Dimension can be a CTE within a bigger flink statement.</p> How to support Type 2 slowly changing dimension (SCD) table? <p>Type 2 SCDs are designed to maintain a complete history of all changes to dimension data. When a change occurs, a new row is inserted into the table, representing the updated record, while the original record remains untouched. Each record in the table is typically assigned a unique identifier (often a surrogate key) to distinguish between different versions of the same dimension member. </p>"},{"location":"concepts/#source-of-knowledge","title":"Source of knowledge","text":"<ul> <li> Apache Flink Product documentation. </li> <li> Official Apache Flink training.</li> <li> Confluent \"Fundamentals of Apache Flink\" training- David Anderson.</li> <li> Anatomy of a Flink Cluster - product documentation.</li> <li> Jobs and Scheduling - Flink product documentation.</li> <li> Confluent Cloud Flink product documentation</li> <li> Confluent Plaform for Flink product documentation</li> <li>Base docker image is: https://hub.docker.com/_/flink</li> <li>Flink docker setup and the docker-compose files in this repo.</li> <li>FAQ</li> <li> Cloudera flink stateful tutorial: very good example for inventory transaction and queries on item considered as stream</li> <li>Building real-time dashboard applications with Apache Flink, Elasticsearch, and Kibana</li> </ul>"},{"location":"concepts/flink-sql/","title":"Flink SQL Concepts","text":"Updates <ul> <li>Created 02/2021 </li> <li>Updated 12/20/24 - Update 11/2025</li> </ul>"},{"location":"concepts/flink-sql/#introduction","title":"Introduction","text":"<p>Flink SQL is an ANSI-compliant SQL engine designed for processing both batch and streaming data on distributed computing servers managed by Flink.</p> <p>Built on Apache Calcite, Flink SQL facilitates the implementation of SQL-based streaming logic. With Flink SQL, developers work with dynamic tables, a concept similar to materialized views in DB, while abstracting away the stream construct from the developers. </p> <p>Developers may write SQL statement, or use the Table API, a language-integrated query API for Java, Scala and Python that enables the composition of queries using relational operators such as selection, filtering, and joining. </p> <p>Table API is a client SDK, once submitted to the Job Manager, the code is translated to a Directed Acyclic Graph. SQL or Table API efficiently handle both bounded and unbounded streams within a unified and highly optimized system inspired by traditional databases and SQL.</p> <p>Both the Table API and SQL operate on top of a lower-level stream operator API, which runs on the dataflow runtime:</p> Flink APIs working together <p>The optimizer and planner APIs transform SQL statements for execution across distributed nodes, leveraging the lower-level stream operator API. </p> <p>To write and execute SQL queries, developers may package the code in a Java Table API program or use the Flink SQL client: an interactive tool for submitting SQL queries to Flink and visualizing the results. With Confluent Platform or Cloud managed service, SQL is a first language of choices and queries may be submitted via API or CLI as a Statement.</p> <p>Streams or bounded data are mapped to Tables. The following SQL loads data from a csv file and creates a dynamic table in Flink. The WITH section specifies the properties to external connector to read the stream from:</p> <pre><code>CREATE TABLE car_rides (\n    cab_number VARCHAR,\n    plate VARCHAR, \n    cab_type VARCHAR,\n    driver_name VARCHAR, \n    ongoing_trip VARCHAR, \n    pickup_location VARCHAR, \n    destination VARCHAR, \n    passenger_count INT\n) WITH ( \n    'connector' = 'filesystem',\n    'path' = '/home/flink-sql-quarkus/data/cab_rides.txt',\n    'format' = 'csv'\n);\n</code></pre> <p>Show how the table is created:</p> <pre><code>show create table orders;\n</code></pre> <p>See the getting started chapter to run Flink open-source locally with the Flink SQL shell client connected to a Job Manager and Task manager running in container. </p> <p>See also the SQL coding practices chapters and this one.</p>"},{"location":"concepts/flink-sql/#main-use-cases","title":"Main use cases","text":"<p>Flink SQL can be used in two main categories of application:</p> <ol> <li>Reactive application, event-driven function</li> <li>Data as a product with real-time white box ETL pipeline: schematizing, cleaning, enriching for data lake, lake house, feature store or vector store. </li> </ol> <p>Data engineers are experts on writing SQL for preparing Data as a product, so implementing the same logic on real-time streaming will be easier with an ANSI SQL engine like Flink SQL.</p>"},{"location":"concepts/flink-sql/#parallel-with-database-sql","title":"Parallel with Database SQL","text":"<p>Database applications are typically classified into two domains: Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP), which are used for business reporting.</p> <p>Databases consist of catalogs, databases, tables, views, and materialized views. The most critical component is the query processor, which receives queries, plans their execution using metadata from the catalog (including information about tables and functions), and then executes the query via the storage engine to access the data and generate results.</p> <p>Views are virtual tables derived from the results of SQL queries. Some databases also support materialized views, which cache the results in a physical table. For example, a GROUP BY operation on an aggregate can store the results based on grouping elements and aggregates in a new table. Materialized views can be updated through a full refresh (by re-executing the query) or through incremental updates.</p> <p>Flink SQL utilizes dynamic tables derived from data streams and employs materialized views with incremental updates. While it is not a traditional database, Flink functions as a query processor. The processor runs continuous queries.</p> <p>In Confluent (Cloud or Platform for Flink), the catalog defines access to the schema registry, and Database reference a Kafka cluster, so Flink SQL engine can access topic message structure to map to Table schema and being able to retrieves records from topics, for processing.</p> <p>Flink can provide either \"exactly once\" or \"at least once\" guarantees (with the possibility of duplicates), depending on the configuration and the external systems used for input and output sources.</p> <p>For effective \"exactly once\" processing, the source must be replayable, and the sink must support transactions and being idempotent. Kafka topics support both of these requirements, and the consumer protocol adheres to the read-committed semantics. However, transaction scope in Kafka is at the single-key level, whereas ACID transactions in databases maintain integrity across multiple keys.</p>"},{"location":"concepts/flink-sql/#changelog-mode","title":"Changelog mode","text":"<p>In traditional DB, changelog or transaction log records all modification operations in the database. </p> <p>For Flink stream is the changelog of the table, and every dynamic table in Flink is backed by a stream.  Result of a query in Flink is also a dynamic table, which is continuously updated.</p> <p>Flink SQL generates changelog data: changelogs that contain only INSERT-type events is an append streams, while for UPDATE-type events, the stream is an update mode.</p> <p>Here is an animation of the append-only table</p> <p>Some Flink operations (stateful ones) such as group by, aggregation and deduplication will produce update events. An update means Flink produces two update events -U (update before) to retract previous value, and +U to keep last value. +I for insertion and -D to delete the record. Primary key needs to be define. With retract mode, +I, +U, -U, -D update events are present. For update mode, +I, -D and +U.</p> <p>The changelog <code>normalize()</code> function allows Flink to track the precise sequence of changes to data, which is essential for all stateful operations. In Kafka, records provide the current state of the records without information of what changed. In Flink, without this state tracking, any stateful operation would produce incorrect or inconsistent results when source records are updated or deleted, as operators would lack the context of how the data has changed. </p> <p>Those messages are inside the Flink cluster and exchanged between tasks. Message with the same key will reach the same task/state.</p> State size <p>Queries that make update changes, usually have to maintain more state. See Flink table to stream conversion documentation.</p> <p>To keep the semantic end-to-end the sink connector needs to support upsert. JDBC Sink is the most obvious one.</p> <p>For Kafka, there are three different modes to persist table rows in a append log like a Kafka topic: append, retract or upsert. </p> <ul> <li> <p>append is the simplest mode where records are only added to the result stream, never updated or retracted. It means that every insertion can be treated as an independent immutable fact. Like a write-once data. Records can be distributed using round robin to the different partitions.There is no need to specify a primary key with append mode, as windowing or aggregation will produce undefined, and may be wrong, results.  Temporal join may be possible. Some queries will create append output, like window aggregation, or any operation using the watermark.     <pre><code>create table if not exists orders (\n        order_id STRING primary key not enforced,\n        product_id STRING,\n        quantity INT\n    ) DISTRIBUTED into '1' BUCKETS \n    with (\n        'changelog.mode' = 'append',\n        'value.fields-include' = 'all'\n    );\n</code></pre></p> <p>The outcome includes records in topics that are insert, immutable records. The order_id 7 is duplicated as we can see by running:  <pre><code>SELECT * FROM `orders`;\n</code></pre></p> <p> Changelog mode: insert events </p> </li> <li> <p>upsert means that all rows with the same primary key are related and must be partitioned together. Events are only Upsert or Delete for a primary key. A select in a Job session will returns the good result set, for example, without any duplicates. </p> </li> <li>retract means a fact can be undone. Flink emits pairs of retraction (-U) and addition (+U) records. When updating a value, it first sends a retraction of the old record (negative record) followed by the addition of the new record (positive record). It means, a fact can be undone, and the combination of +U and -U are related and must be partitioned together. With retract mode a consumer outside of Flink needs to interpret the Kafka header to implement the good semantic. </li> </ul> <p>Some operations, such as group by, aggregation and deduplication with ROW_NUMBER() produce update events. Use the <code>EXPLAIN</code> feature to analyze the physical execution plan of a query to see the changelog mode of each operator.</p> <p>See this lab in this repository, for the a study of the different changelog modes.</p> <p>Also be sure to get the key as part of the values, using the <code>'value.fields-include' = 'all'</code> option, if not, it will not be possible to group by the key.</p> <p>See the concept of changelog and dynamic tables in Confluent's documentation.</p>"},{"location":"concepts/flink-sql/#non-deterministic-functions-with-upsertretract-tables","title":"Non-Deterministic Functions with Upsert/Retract Tables","text":"<p>When working with upsert or retract changelog modes, Flink requires deterministic computations to correctly process update messages (<code>UB</code>/<code>UA</code>/<code>D</code> operations). Using non-deterministic functions can cause deployment failures.</p> <p>Common Error Messages:</p> <pre><code>generated by non-deterministic function: CURRENT_DATE ) $f10(generated by non-deterministic function: CURRENT_DATE ) can not satisfy the determinism requirement for correctly processing update message('UB'/'UA'/'D' in changelogMode, not 'I' only), this usually happens when input node has no upsertKey(upsertKeys=[{}]) or current node outputs non-deterministic update messages. \nPlease consider removing these non-deterministic columns or making them deterministic by using deterministic functions.\n</code></pre> <ul> <li>Non-deterministic functions like <code>CURRENT_DATE</code>, <code>CURRENT_TIMESTAMP</code>, <code>NOW()</code>, <code>LOCALTIME</code>, <code>LOCALTIMESTAMP</code> return different values each time they're evaluated.</li> <li>With upsert/retract changelog modes, Flink may need to reprocess the same logical record multiple times (for updates, retractions). If the computation produces different results on reprocessing, the changelog stream becomes inconsistent. This breaks the determinism requirement needed for correct stateful operations.</li> </ul> <p>Replace processing time functions with event time attributes like <code>$rowtime</code> or explicit event time columns.</p> <p>As an example: <pre><code>SELECT \n  customer_id,\n  COUNT(CASE WHEN TIMESTAMPDIFF(DAY, transaction_date, CURRENT_DATE) &lt;= 90 \n        THEN 1 END) as transactions_last_90d,\n  COUNT(CASE WHEN TIMESTAMPDIFF(DAY, transaction_date, CURRENT_DATE) &lt;= 30 \n        THEN 1 END) as transactions_last_30d\nFROM upsert_transactions\nGROUP BY customer_id;\n</code></pre></p> <p>may better work if using the event time: <pre><code>SELECT \n  customer_id,\n  COUNT(CASE WHEN TIMESTAMPDIFF(DAY, transaction_date, \n        DATE($rowtime)) &lt;= 90 THEN 1 END) as transactions_last_90d,\n  COUNT(CASE WHEN TIMESTAMPDIFF(DAY, transaction_date, \n        DATE($rowtime)) &lt;= 30 THEN 1 END) as transactions_last_30d\nFROM upsert_transactions\nGROUP BY customer_id;\n</code></pre></p> <p>As TIMESTAMPDIFF works on DATE, important to use the DATE($rowtime) function to transform the event time to a Date.</p>"},{"location":"concepts/flink-sql/#other-best-practices","title":"Other Best Practices","text":"<ol> <li>Always use event time for time-based calculations in streaming pipelines</li> <li>Extract <code>$rowtime</code> early in your pipeline if you need date comparisons:    <pre><code>-- In intermediate table\nSELECT \n  *,\n  DATE($rowtime) as almost_processing_date\nFROM source_table;\n</code></pre></li> </ol>"},{"location":"concepts/flink-sql/#when-processing-time-is-actually-needed","title":"When Processing Time Is Actually Needed","text":"<p>In rare cases where you genuinely need processing time (e.g., for monitoring, debugging), ensure your table uses append-only mode (<code>changelog.mode = 'append'</code>), not upsert/retract mode.</p> <p>Related Considerations:</p> <ul> <li>This error only occurs with stateful operations (GROUP BY, JOIN, etc.) that produce upsert/retract streams</li> <li>Append-only tables (no primary key, append changelog) can safely use non-deterministic functions</li> <li>Event time provides better correctness for streaming analytics and supports late data handling</li> </ul>"},{"location":"concepts/flink-sql/#sql-programming-basics","title":"SQL Programming Basics","text":"<p>Flink SQL tables are dynamic, meaning they change over time; some tables act more like changelog streams than static tables.</p> <p>The following diagram illustrates the main processing concepts: the <code>shipments</code> table tracks product shipments, while the <code>inventory</code> table maintains the current quantity of each item. The INSERT statement processes streams to update the inventory based on new shipment records. This SQL statement uses the SUM aggregation operation to count each item, with the records grouped by item name.</p> <p>SQL is applied directly to the stream of data; data is not stored within Flink. </p> <p>Events can represent INSERT, UPDATE, or DELETE operations in the table. The diagram shows that, at the sink level, the initial events reflect the addition of items to the inventory. When the inventory for the \"Card\" item is updated, a record is first created to remove the current stock of the \"Card\" and then a new message is sent with the updated stock value (2 cards). This behavior arises from the GROUP BY semantics, where the right table is an update-only table while the left is append-only. </p> <p>Dynamic tables can also be persisted in Kafka topics, meaning the table definition includes statements on how to connect to Kafka. In batch processing, the sink can be a database table or a CSV file in the filesystem. </p> <p>Note that the SQL Client executes each INSERT INTO statement as a separate Flink job. The STATEMENT SET BEGIN .. END construct can be used to group multiple insert statements into a single set. </p> <p>As the job manager schedules these jobs to the task managers, SQL statements are executed asynchronously. For batch processing, developers can set the set <code>table.dml-sync</code> option to <code>true</code>.</p> <p>In streaming, the \"ORDER BY\" statement applies only to timestamps in ascending order, while in batch processing, it can be applied to any type of column.</p>"},{"location":"concepts/flink-sql/#data-lifecycle","title":"Data lifecycle","text":"<p>In a pure Kafka integration architecture, the data lifecycle follows these steps:</p> <ul> <li>Data is read from a Kafka topic to a Flink SQL table in memory</li> <li>Data is processed using Flink SQL statements and may be distributed between multiple hosts.</li> <li>Results are returned as result sets in interactive mode, or to a table (mapped to a topic) in continuous streaming mode.</li> </ul>"},{"location":"concepts/flink-sql/#sql-operators-state","title":"SQL operators state","text":"Type Operators Comments Stateless SELECT {projection, transformation} WHERE {filter}; UNION ..., CROSS JOIN UNNEST or CROSS JOIN LATERAL Can be distributed Materialized GROUP BY , OVER, JOINS or MATCH_RECOGNIZE Dangerously Stateful, keep an internal copy of the data related to the query Temporal Time windowed operations, interval joins, time-versioned joins Stateful but constrained in size <p>As elements are stored for computing materialized projections, it's crucial to assess the number of elements to retain. Millions to billions of small items are possible. However, if a query runs indefinitely, it may eventually overflow the data store. In such cases, the Flink task will ultimately fail.</p> <p>In a join any previously processed records can be used potentially to process the join operation on new arrived records, which means keeping a lot of records in memory. As memory will be bounded, there are other mechanisms to limit those joins or aggregation, for example using time windows and time to live parameters.</p>"},{"location":"concepts/flink-sql/#flink-sql-high-level-faq","title":"Flink SQL High Level FAQ","text":""},{"location":"concepts/flink-sql/#why-the-watermark-is-set-7-days-in-the-past","title":"Why the watermark is set 7 days in the past?","text":"<ul> <li> <p>Creating a table with the <code>$rowtime</code> as watermark like:     <pre><code>user_id STRING  NULL,\n-- more columns      \n$rowtime    TIMESTAMP_LTZ(3) *ROWTIME*  NOT NULL    METADATA VIRTUAL, WATERMARK AS `SOURCE_WATERMARK`() SYSTEM\n</code></pre></p> </li> <li> <p>and then checking the watermark value with something like:     <pre><code>SELECT *, CURRENT_WATERMARK($rowtime) AS current_watermark, $rowtime FROM `table_name`;\n</code></pre></p> <p>the current_watermark is 7 days behind. </p> </li> </ul> <p>Altering the table to use <code>$rowtime</code> as watermark addresses the issues. But the default strategy is to emit the first watermark 7 days in the past, if not enough records (1000 records in each partition) are emitted. </p> <p>Time and Watermarks video</p>"},{"location":"concepts/flink-sql/#how-we-are-going-to-setup-full-load-of-a-table","title":"How we are going to setup full load of a table?","text":"<p>For Flink SQL stream processing, we assume raw-topics are created from a change data capture systems and continuously write records to the topics. When history is important the raw-topic content become the implementation of the event-sourcing patttern and we need to be able to reprocess from the earliest offset. So when defining the table view associated to this topic, using Flink Kafka connector, the CREATE TABLE has, in the WITH section, a parameter to set the kafka reading strategy:      <pre><code>CREATE TABLE raw_user (\n      `user_id` BIGINT,\n      `name` STRING\n)\nWITH (\n      'scan.startup.mode' = 'earliest-offset',\n)\n</code></pre></p> <p>In Confluent Cloud the connector set this property to the earliest by default. Altering the table can change to the latest-offset.</p>"},{"location":"concepts/flink-sql/#how-we-are-going-to-merge-the-reference-data-and-cdc-data","title":"How we are going to merge the reference data and CDC data?","text":"<p>This is a standard left or right joins on the fields needed to identify records on both table. The reference data in the context of Kafka is a topic with retention set to infinite, and will all the records in one partition, sorted or not. In Flink a primary key may be defined in the reference table and the changelog model sets to upsert so the last value is kept per key.</p>"},{"location":"concepts/flink-sql/#deeper-dive","title":"Deeper dive","text":"<ul> <li>SQL and Table API overview</li> <li>Table API</li> <li>Introduction to Apache Flink SQL by Timo Walther</li> <li>Flink API examples presents how the API solves different scenarios:<ul> <li>as a batch processor,</li> <li>a changelog processor,</li> <li>a change data capture (CDC) hub,</li> <li>or a streaming ETL tool</li> </ul> </li> </ul>"},{"location":"cookbook/","title":"A Production Cookbook for Managing Apache Flink","text":"Chapter Version <ul> <li>Creation 01/2026</li> </ul>"},{"location":"cookbook/#executive-summary","title":"Executive Summary","text":"<p>A \u201ccookbook\u201d for managing Flink in production is essentially a curated set of short, repeatable runbooks (\u201crecipes\u201d) that cover the lifecycle of Flink applications: deploying, operating, diagnosing, and evolving them safely.</p> <ul> <li>It is organized by operational scenario (e.g., \u201cRoll a Flink job upgrade with savepoints,\u201d \u201cRecover from failed checkpointing,\u201d \u201cBackfill data for a job\u201d) rather than by API or feature.</li> <li>Each recipe follows a standard template: Context, Preconditions, Steps, Validation, Rollback, and Gotchas, so SREs and data engineers know exactly what to do during incidents or changes.</li> <li>The cookbook spans cluster management, job lifecycle &amp; state, resource management &amp; scaling, observability &amp; SLOs, data quality &amp; compatibility, and common incident handling.</li> <li>We may want to start small (10\u201315 critical recipes) and evolve it over time as new classes of incidents or operational patterns emerge.</li> <li>If you\u2019re running Flink on Kubernetes/Flink Operator or on a managed platform (e.g., Confluent Cloud Flink), the core recipes are similar; the concrete commands and UIs differ, but the cookbook structure remains the same.</li> </ul>"},{"location":"cookbook/#audience","title":"Audience","text":"<p>A Flink production cookbook is written primarily for:</p> <ul> <li>Platform / SRE teams who own the Flink clusters or managed Flink accounts.</li> <li>Data engineers / stream app owners who own the logic of the jobs and must deploy, tune, and troubleshoot them.</li> <li>On-call responders who need precise, low-ambiguity steps during incidents.</li> </ul>"},{"location":"cookbook/#scope-of-the-cookbook","title":"Scope of the Cookbook","text":"<p>The cookbook is not documentation for \u201chow to write Flink code.\u201d It assumes jobs already exist and focuses on Day-2 operations:</p> <ul> <li>Provisioning / Cluster ops: Spinning up clusters, upgrading Flink versions, adjusting HA settings, etc.</li> <li>Job lifecycle &amp; state: Deploying jobs, restarting, upgrading with/without state, managing savepoints and checkpoints.</li> <li>Resources &amp; scaling: Tuning parallelism, memory, slots, auto-scaling strategies, backpressure handling.</li> <li>Observability: Metrics, logs, traces, alerts, SLOs, debugging performance issues.</li> <li>Data &amp; schema: Handling schema evolution (e.g., with Kafka/Confluent Schema Registry), reprocessing, backfills.</li> <li>Failure handling: Job failures, checkpoint failures, state corruption, external system outages.</li> </ul> <p>For each of the chapter / section we will try to address Apache Flink, Confluent Platform and Confluent Cloud for Flink.</p>"},{"location":"cookbook/cluster_mgt/","title":"Cluster &amp; Environment Management","text":""},{"location":"cookbook/cluster_mgt/#provisioning-and-scaling-clusters","title":"Provisioning and Scaling Clusters","text":"<ul> <li>Bring up new cluster/environment.</li> <li>Adjust cluster resources (more TaskManagers, more slots).</li> </ul>"},{"location":"cookbook/cluster_mgt/#upgrading-flink-and-cluster-components","title":"Upgrading Flink and Cluster Components","text":"<ul> <li>Upgrade Flink minor/patch versions.</li> <li>Rolling upgrades and compatibility considerations.</li> </ul>"},{"location":"cookbook/cluster_mgt/#disaster-recovery-multi-region-strategies","title":"Disaster Recovery &amp; Multi-Region Strategies","text":"<ul> <li>Backup/restore of state backend.</li> <li>Active-active / active-passive patterns.</li> </ul>"},{"location":"cookbook/considerations/","title":"Deployment Model and Considerations","text":""},{"location":"cookbook/considerations/#deployment-models","title":"Deployment Models","text":"<p>Flink may be deployed as standalone servers, within Kubernetes and Flink Operator, YARN or as managed services. </p> <p>As seen in the architecture chapter, the components to consider are Job manager and Task managers. But in production deployment other services are very important to address.</p> Figure 1: Flink Components &amp; Services <ul> <li>Job Manager in application mode runs the cluster exclusively for one application. This is the recommended mode for resource isolation and load balancing.</li> <li>HA service: Flink's JobManager can be run in high availability mode which allows Flink to recover from JobManager faults. In order to failover faster, multiple standby JobManagers can be started to act as backups. Such service may be Zookeeper or Kubernetes job scheduler.</li> <li>Storage service for checkpoints management</li> <li>Resource service: kubernetes or Yarn for process scheduling and resource management</li> <li>Sink or source connectors for read/write operators: Kafka, Object Storage (S3), ElasticSearch,</li> </ul> <p>In application mode, the user jars are bundled with the Flink distribution, and most likely packaged as a container image.</p>"},{"location":"cookbook/considerations/#apache-flink-standalone","title":"Apache Flink Standalone","text":"<p>The key points to work on: * The processes presented above are executed within the host operating system. * SREs have to restart failed processes, or allocation and de-allocation of resources during operation * For getting HA, Zookeeper needs to be started, and the Flink configuration needs to be changed to support multiple Job Managers. * Local directory is configured to store important information, like job state and metadata, to be used during recovery. Job identity helps to find persisted information.</p> Figure 2: Apache Flink Bare Metal Components for Production <p>See product chapter for the scripts to use.</p>"},{"location":"cookbook/considerations/#apache-flink-within-kubernetes","title":"Apache Flink within Kubernetes","text":"<p>For Kubernetes deployment, we should distinct three different mode of deployment:</p> <ol> <li>Standalone: the barebone deployment on kubernetes</li> <li>Native Kubernetes</li> <li>Flink Kubernetes Operator</li> </ol> <p>Apache Flink has defined a Kubernetes Operator (FKO) to deploy and manage custom resources for Flink deployments.</p> <p>Apache Flink Kubernetes Operator(FKO) acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications. </p> Figure 3: Apache Flink Kubernetes Operator to manage Flink Job and Task managers <p>The operator takes care of submitting, savepointing, upgrading and generally managing Flink jobs using the built-in Flink Kubernetes integration. The operator fully automates the entire lifecycle of the job manager, the task managers, and the applications. A FlinkDeployment is a manifest to define what needs to be deployed, and then the FKO manages the deployment by using Kubernetes deployments, pods... It supports query on the custom resources it manages. </p> <p>Failures of Job Manager pod are handled by the Deployment Controller which takes care of spawning a new Job Manager.</p> <p>As any Kubernetes operators, FKO can run namespace-scoped, to get multiple versions of the operator in the same Kubernetes cluster, or cluster-scoped for highly distributed  deployment. The operator maps its custom resources to existing Kubernetes resources of deployments, replica sets, config maps, secrets, service accounts...</p> <p>The following figure represents a simple deployment view of a Flink Cluster, in parallel of a Kafka cluster running on a Kubernetes platform:</p> Figure 4: Flink and Kafka OSS - K8S deployment <p>The FKO may have two instances running in parallel. A Flink application may run on its own namespace and will be one job manager and n task managers.  The Kafka cluster runs in its own namespace. PVC or File services are needed for Flink to persist checkpoints and savepoints. </p>"},{"location":"cookbook/considerations/#apache-flink-custom-resources","title":"Apache Flink Custom Resources","text":"<p>Once the Flink for Kubernetes Operator is running, we can submit jobs using  <code>FlinkDeployment</code> (for Flink Application or for Job manager and task manager for session cluster) and <code>FlinkSessionJob</code> for Flink Session. The following figure represents those concepts: </p> FKO main Custom Resources Definitions <p>On the left, a <code>FlinkSessionJob</code> references an existing FlinkDeployment as multiple session jobs can run into the same Flink cluster. The <code>job</code> declaration specifies the code to run with its specific configuration. While on the right, the application mode, has the job definition as part of the FlinkDeployment, as the JobManager and TaskManager mininum resource requirements.</p> <p>The Apache Flink FlinkDeployment spec is here and is used to define Flink application (will have a job section) or session cluster (only job and task managers configuration).</p> <p>The custom resource definition that describes the schema of a FlinkDeployment is a cluster wide resource. The Operator continuously tracks cluster events relating to the <code>FlinkDeployment</code> and <code>FlinkSessionJob</code> custom resources. The operator control flow is described in this note.. The important points to remember are:</p> <ul> <li>The operator control flow is: 1/ Observes the status of the currently deployed resource. 2/ Validates the new resource spec, 3/ Reconciles any required changes based on the new spec and the observed status.</li> <li>The Observer module assesses the current stateus of any deployed Flink resources. </li> <li>Observer is responsible for application upgrade.</li> <li>The job manager is validated via a call to its REST api and the status is recorded in the <code>jobManagerDeploymentStatus</code></li> <li>A Job cannot be in running state without a healthy jobmanager.</li> </ul> <p>A Flink Application is any user's program that spawns one or multiple Flink jobs from its <code>main()</code> method and is deploying a JobManager and n Task managers. They may run in their own namespace. </p> <p>The Flink Kubernetes Operator is looking at different <code>Flink Deployment</code>, so it can be isolated within its own namespace.</p> Practices <p>When deploying the FKO it is important to specify the namespaces to watch for future deployments. The following command modify this list: <pre><code>helm upgrade --install cp-flink-Kubernetes-operator --version \"~1.130.0\"  confluentinc/flink-Kubernetes-operator --set watchNamespace=\"{flink, confluent, el-demo, rental}\" -n flink\n</code></pre></p> <p>It is important to delete the operator pod and let Kubernetes restarts the FKO pod with the new config.</p> <p>It is important to note that <code>FlinkDeployment</code> and <code>FlinkApplication</code> CRDs have a podTemplate, so ConfigMap(s) and Secret(s) can be used to configure environment variables for the Flink app. (Be sure to keep the container name as <code>flink-main-container</code>)</p> <pre><code>spec:\n  podTemplate:\n    spec:\n      containers:\n        - name: flink-main-container\n          envFrom:\n            - configMapRef:\n                name: flink-app-cm\n</code></pre>"},{"location":"cookbook/considerations/#confluent-cloud-manager-services","title":"Confluent Cloud - Manager Services","text":""},{"location":"cookbook/considerations/#confluent-manager-for-flink","title":"Confluent Manager for Flink","text":"<p>The deployments for Confluent Platform and Confluent Manager for Flink, may look like in the following figure:</p> Figure 3: K8S deployment <p>Confluent for Kubernetes Operator (CFK) is the control plane for deploying and managing Confluent in your Kubernetes private cloud environment. It defines custom resource definitions to support Kafka based resources like brokers, kraft controllers, topics, schema registry, connectors, cmfrestclass,...</p> <p>Confluent Manager for Apache Flink\u00ae (CMF) is a Kubernetes operator, to manage Confluent Flink Applications, Environments, Compute pools, SQL Catalogs, we will detail those in a later section. CMF integrates with FKO to support Flink native custom resources.</p> <p>The following figure illustrates the relationships between those kubernetes operators:</p> Figure 4: The operators playing together <p>It is important to note that all Kubernetes Custom Resource deployments via <code>kubectl</code> go to the CFK with the <code>apiVersion: platform.confluent.io/v1beta1</code>. CRs touching Flink resources are delegated to the CMF operator. While deploying Flink components via the Confluent CLI or via REST API, the CMF CRDs use different apiVersion. Therefore it is possible to run CMF without CFK. Any CR with <code>cmf.confluent.io/v1</code> as apiVersion needs to be created with confluent CLI, as using <code>kubectl</code> will not work because the CRDs are not known by Kubernetes.</p> <p>Confluent Platform Console (also named C3) is integrated with CMF. CMF exposes a REST API and cli integration for managing Flink statements.</p> <p>The following figure illustrates the current (Oct 2025) configuration of Flink solution deployment using the different CRs apiVersion.</p> Figure 5: CFK, CMF and CKO <ul> <li> <p>Confluent FlinkEnvironments may be created with Manifests and uploaded via the Confluent CLI. Confluent Flink Environment differs from the Apache Flink Environment: it specifies which kubernetes namespace to use, which cmf REST class to connect to.</p> </li> <li> <p>The metadata are persisted in an embedded database. </p> </li> <li>The 'Environment' concept is to group multiple Flink applications together. This is an isolation layer for RBAC, and used to define Flink Configuration cross compute pools and applications deployed within an environment. Flink Configuration may include common observability and checkpointing storage (HDFS or S3) definitions. See one definition of FlinkEnvironment.</li> <li>a REST API supports all the external integration to the operator. Confluent Control Center and  the <code>confluent</code> cli are using this REST end point.</li> <li>CMF manages FlinkDeployment resources internally</li> </ul> <p>It is still possible to do pure OSS FlinkDeployment CRs but this strongly not recommended to leverage the full power of Confluent Platform and get Confluent Support.</p> Versioning <p>There is a Confluent version and interoperability document that should be updated at each release. But each time there is a new release you need to be sure to modify the references for:</p> <ul> <li>Confluent Platform (e.g. 8.1)</li> <li>Confluent Flink image (e.g. confluentinc/cp-flink-sql:1.19-cp4) in compute pool manifests</li> </ul>"},{"location":"cookbook/considerations/#confluent-flink-specific-crs","title":"Confluent Flink Specific CRs","text":"<p>First an important document to read: The Confluent Operator API references.</p> <p>Confluent Managed for Flink only manages Flink application mode and is using its own CRDs to define <code>FlinkEnvironment</code> and <code>FlinkApplication</code>. The CRDs are defined here. To be complete, it also define KafkaCatalog and ComputePool CRDs to defne SQL catalog, and other components introduced by Confluent Cloud.</p> <ul> <li>The new CRs for Environment, Application,  Compute pool, and Flink Catalog:</li> </ul> Confluent Manager for Flink - Custom Resources Definitions <ul> <li>An FlinkEnvironment CRD define access control to flink resources and may define FlinkConfigurations cross applications. Environment level has precedence over Flink configuration for individual Flink applications. See one example in deployment/k8s/cmf.    <pre><code>apiVersion: platform.confluent.io/v1beta1\nkind: FlinkEnvironment\nmetadata:\n  name: dev-env\n  namespace: confluent\nspec:\n  kubernetesNamespace: el-demo\n  flinkApplicationDefaults: \n    metadata:\n      labels:\n        env: dev-env\n    spec:\n      flinkConfiguration:\n        taskmanager.numberOfTaskSlots: '1'\n        state.backend.type: rocksdb\n        state.checkpoints.dir: 's3a://flink/checkpoints'\n        state.savepoints.dir: 's3a://flink/savepoints'\n        state.backend.incremental: 'true'\n        state.backend.rocksdb.use-bloom-filter: 'true'\n        state.checkpoints.num-retained: '3'\n        ...\n      podTemplate:\n        metadata:\n          name: flink-pod-template\n        spec:\n          containers:\n          - name: flink-main-container\n            env:\n            - name: S3_ENDPOINT\n              valueFrom:\n                secretKeyRef:\n                  name: minio-s3-credentials\n                  key: s3.endpoint\n          ...\n  cmfRestClassRef:\n    name: default\n    namespace: confluent\n</code></pre></li> </ul> <p>Some important elements to consider are: </p> <ul> <li> <p><code>kubernetesNamespace</code> is the namespace where the Flink deployment(s) will be deployed. So one environment establishes foundations for those Flink applications. It can define default Flink configuration for all applications and add common labels, like specifying the environment name they run in. <code>FlinkApplication</code> is referencing back the Flink Environment which is not what Flink OSS Application does. The last piece is the <code>cmfRestClassRef</code> to reference the Kubernetes object/resource used to define access point to the CMF REST api.</p> </li> <li> <p><code>CMFRestClass</code> defines the client configuration to access CMF Rest APIs. This resource is referenced by other CMF resources (ex FlinkEnvironment, FlinkApplication) to access CMF Rest APIs. It alos supports security configuration, like the authentication mechanism and mTLS to access the REST api.   <pre><code>apiVersion: platform.confluent.io/v1beta1\nkind: CMFRestClass\nmetadata:\n  name: default\n  namespace: confluent\nspec:\n  cmfRest:\n    endpoint: http://cmf-service.confluent.svc.cluster.local\n</code></pre></p> </li> <li> <p><code>FlinkApplication</code>, in the context of Confluent Manager for Flink, is the same as Apache Flink but adds references to Environment and to the CMFRestClass. Every application runs on its own cluster, providing isolation between all applications.</p> </li> <li>Service Account: Service accounts provide a secure way for applications (like Flink jobs deployed via CMF) to interact with Confluent platform resources (e.g., Kafka clusters, Schema Registry) without relying on individual user credentials. Service accounts are central to the RBAC system. Need one service account per application or most likely per environment. The SA, cluster role, role and the role bindings need to be defined in the target namespace where the Flink app will be deployed. See this example for one application or the rental demo based on Table API app.</li> <li>KafkaCatalog is used to expose Kafka Topics as Tables for Flink. This CRD defines a Kafka Catalog object to connect to a Schema Registry. See catalog definition for the rental demo:   <pre><code>{\n  \"apiVersion\": \"cmf.confluent.io/v1\",\n  \"kind\": \"KafkaCatalog\",\n  \"metadata\": {\n    \"name\": \"rental\"\n  },\n  \"spec\": {\n    \"srInstance\": {\n      \"connectionConfig\": {\n        \"schema.registry.url\": \"http://schemaregistry.confluent.svc.cluster.local:8081\"\n      }\n    }\n  }\n}\n</code></pre></li> <li>Create a database to reference a Kafka cluster: See product documentation, one example of database definition</li> <li>ComputePools are used in the context of Flink SQL to execute SQL queries or statements.  The ComputePool will only be used when the statement is deployed which happens after the compilation. It is a second level of Flink configuration for Flink cluster settings. See the kafka catalog example in external lookup demo. One important element is to specify the <code>image</code> attribute to referent a flink with SQL like <code>confluentinc/cp-flink-sql:1.19-cp4</code>. See docker hub for last tags available.</li> </ul> <p>The configuration flexibility:</p> <ul> <li>FlinkConfiguration defined  at the environment level can apply to all compute pools of this environment, and applications</li> <li>Compute pool configuration can apply to all SQL statements executed within the compute pool</li> <li>Flink Application has its own configuration, knowing that an application can be done with DataStream, or TableAPI.</li> </ul>"},{"location":"cookbook/considerations/#source-of-information","title":"Source of information","text":"<p>The examples in Confluent github provides scenario workflows to deploy and manage Confluent on Kubernetes including Flink and this article: How to Use Confluent for Kubernetes to Manage Resources Outside of Kubernetes covers part of the deployment. </p>"},{"location":"cookbook/considerations/#high-availability","title":"High Availability","text":"<p>With Task managers running in parallel, if one fails the number of available slots drops, and the JobManager asks the Resource Manager to get new processing slots. </p> Figure : Task Manager host failure - HA  <p>The application's restart strategy determines how often the JobManager restarts the application and how long it waits between restarts.</p> <p>Flink OSS uses Zookeeper to manage multiple JobManagers and select the leader to control the execution of the streaming jobs. </p> Figure : Job Manager host failure - HA  <p>Application's tasks checkpoints and other states are saved in a local and remote storages, but metadata are saved in Zookeeper. When a JobManager fails, all tasks that belong to its application are automatically cancelled. A new JobManager that takes over the work by getting information of the storage from Zookeeper, and then restarts the process with the JobManager.</p>"},{"location":"cookbook/considerations/#fault-tolerance","title":"Fault Tolerance","text":"<p>The central part of Flink\u2019s fault tolerance mechanism is drawing consistent snapshots of the distributed data stream and operator state. The two major Flink features to support fault tolerance are the checkpoints and savepoints. </p>"},{"location":"cookbook/considerations/#checkpointing","title":"Checkpointing","text":"<p>Checkpoints are snapshots of the input data stream, capturing the state of each operator, of the DAG, at a specific point in time. They are created automatically and periodically by Flink. The saved states are used to recover from failures, and checkpoints are optimized for quick recovery.</p> <p>Checkpoints allow maintaining consistency through exactly-once processing semantics, when the data stream source is able to support rewind to a defined recent point. When a failure occurs, Flink can restore the state of the operators and replay the records starting from the checkpoint.</p> <p>In the event of a failure in a parallel execution, Flink halts the stream flow and restarts the operators from the most recent checkpoints. During data partition reallocation for processing, the associated states are also reallocated. States are stored in distributed file systems, and when Kafka is used as the data source, the committed read offsets are included in the checkpoint data.</p> <p>Checkpointing is coordinated by the Job Manager, it knows the location of the latest completed checkpoint which will get important later on. This checkpointing and recovery mechanism can provide exactly-once consistency for application state, given that all operators checkpoint and restore all of their states and that all input streams are reset to the position up to which they were consumed when the checkpoint was taken. This will work perfectly with Kafka, but not with sockets or queues where messages are lost once consumed. Therefore exactly-once state consistency can be ensured only if all input streams are from reset-able data sources.</p> <p>During the recovery and depending on the sink operators of an application, some result records might be emitted multiple times to downstream systems. Downstream systems need to be idempotent.</p> <p>Flink utilizes the concept of Checkpoint Barriers to delineate records. These barriers separate records so that those received after the last snapshot are included in the next checkpoint, ensuring a clear and consistent state transition.</p> <p>Barrier can be seen as a mark, a tag, in the data stream and aims to close a snapshot. </p> Checkpointing concepts <p>Checkpoint barriers flow with the stream, allowing them to be distributed across the system. When a sink operator \u2014 located at the end of a streaming Directed Acyclic Graph (DAG) \u2014 receives <code>barrier n</code> from all its input streams, it acknowledges <code>snapshot n</code> to the checkpoint coordinator.</p> <p>Once all sink operators have acknowledged a snapshot, it is considered completed. After <code>snapshot n</code> is finalized, the job will not request any records from the source prior to that snapshot, ensuring data consistency and integrity.</p> <p>State snapshots are stored in a state backend, which can include options such as in-memory storage, HDFS, object storage or RocksDB. This flexibility allows for optimal performance and scalability based on the application\u2019s requirements.</p> <p>When Flink triggers a checkpoint, it doesn't copy the whole database. It identifies which SST files are new since the last successful checkpoint. These new files are uploaded to durable storage (like S3 or HDFS). The checkpoint metadata simply \"points\" to these files.</p> <p>With stateful distributed processing, scaling stateful operators, enforces state repartitioning and assigning to more or fewer parallel tasks. Keys are organized in key-groups, and key groups are assigned to tasks. Operators with operator list state are scaled by redistributing the list entries. Operators with operator union list state are scaled by broadcasting the full list of state entries to each task.</p> <p>In the context of a KeyedStream, Flink functions as a key-value store where the key corresponds to the key in the stream. State updates do not require transactions, simplifying the update process.</p> <p>For Batch processing there is no checkpoint, so in case of failure the stream is replayed from the beginning.</p>"},{"location":"cookbook/considerations/#savepoints","title":"Savepoints","text":"<p>Savepoints are user triggered snapshot at a specific point in time. It is used during system operations like product upgrades. The Flink operator for kubernetes has custom resource definition to support the savepoint process. See also the end to end demo for savepoint in this folder.</p>"},{"location":"cookbook/considerations/#checkpoints-impact-throughput","title":"Checkpoints impact throughput","text":"<ul> <li>The persistence to remote storage is done asynchronously, but at the level of a task. So too frequent checkpointing will impact throughput. Now it also depends if the tasks are compute or IO intensive. </li> </ul>"},{"location":"cookbook/considerations/#interuption-while-writing-checkpoints","title":"Interuption while writing checkpoints","text":"<ul> <li>The processing will restart from the last persisted checkpoints so no data loss. Specially true when source of the data are coming from Kafka topics. The checkpoint points to last read-commited offset within topic/partition so Flink will reload from there</li> </ul>"},{"location":"cookbook/considerations/#when-flink-cluster-has-10-nodes-what-happen-in-one-node-failure","title":"When Flink cluster has 10 nodes what happen in one node failure","text":"<p>It will depend of the operator allocation to the task to the task manager and what the operator needs (as state). At worse case the full DAG needs to be restored, every operator needs to rebuild their state so multiple task managers in the cluster.</p> <p>It can take sometime to recover. Reread data and reprocess it, will take many seconds, or minutes. </p> <p>With hot-hot deployment, it is possible to get the same running application running in parallel, and then switch the output sink / topic for the consumer. For real-time payment we can achieve around 3 to 7 seconds recovery time, with million of records per second.  </p>"},{"location":"cookbook/considerations/#can-we-set-one-task-manager-one-task-to-run-all-a-dag-to-make-it-simple","title":"Can we set one task manager one task to run all a DAG to make it simple?","text":"<p>It will depend of the application state size and logic to operate. If all state stays in memory, yes this is a common pattern to use. If state are bigger than physical memory of the computer running the task manager, then the processing needs more computers, so more task managers and need to distribute data. Then it needs distributed storage to persist states. </p>"},{"location":"cookbook/considerations/#eactly-once-processing","title":"Eactly-once processing","text":"<p>When addressing exactly once processing, it is crucial to consider the following steps:</p> <ul> <li>Read Operation from the Source: Ensuring that the data is read exactly once is foundational. Flink's source connectors are designed to handle this reliably through mechanisms like checkpointing.</li> <li>Apply Processing Logic which involves operations such as window aggregation or other transformations, which can also be executed with exactly-once semantics when properly configured.</li> <li>Generate Results to a Sink introduces more complexity. While reading from the source and applying processing logic can be managed to ensure exactly-once semantics, generating a unique result to a sink depends on the target technology and its capabilities. Different sink technologies may have varying levels of support for exactly-once processing, requiring additional strategies such as idempotent writes or transactional sinks to achieve the desired consistency.</li> </ul> End-to-end exactly once <p>After reading records from Kafka, processing them, and generating results, if a failure occurs, Flink will revert to the last committed read offset. This means it will reload the records from Kafka and reprocess them. As a result, this can lead to duplicate entries being generated in the sink:</p> End-to-end recovery <p>Since duplicates may occur, it is crucial to assess how downstream applications handle idempotence. Many distributed key-value stores are designed to provide consistent results even after retries, which can help manage duplicate entries effectively.</p> <p>To achieve end-to-end exactly-once delivery, it is essential to utilize a sink that supports transactions and implements a two-phase commit protocol. In the event of a failure, this allows for the rollback of any output generated, ensuring that only successfully processed data is committed. However, it's important to note that implementing transactional outputs can impact overall latency.</p> <p>Flink takes checkpoints periodically \u2014 typically every 10 seconds \u2014 which establishes the minimum latency we can expect at the sink level. This periodic checkpointing is a critical aspect of maintaining state consistency while balancing the need for timely data processing.</p> <p>For Kafka Sink connector, as kafka producer, we need to set the <code>transactionId</code>, and the delivery guarantee type:</p> <pre><code>new KafkaSinkBuilder&lt;String&gt;()\n    .setBootstrapServers(bootstrapURL)\n    .setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE)\n    .setTransactionalIdPrefix(\"store-sol\")\n</code></pre> <p>With transaction ID, a sequence number is sent by the Kafka producer API to the broker, and so the partition leader will be able to remove duplicate retries.</p> End-to-end with Kafka transaction id <p>When the checkpointing period is set, we need to also configure <code>transaction.max.timeout.ms</code> of the Kafka broker and <code>transaction.timeout.ms</code> for the producer (sink connector) to a higher timeout than the checkpointing interval plus the max expected Flink downtime. If not the Kafka broker will consider the connection has failed and will remove its state management.</p> Event-driven microservice <p>The evolution of microservice is to become more event-driven, which are stateful streaming applications that ingest event streams and process the events with application-specific business logic. This logic can be done in flow defined in Flink and executed in the clustered runtime.</p> <p> Event-driven application as a sequence of Flink apps </p>"},{"location":"cookbook/considerations/#state-backends-and-storage","title":"State Backends and Storage","text":""},{"location":"cookbook/considerations/#slos","title":"SLOs","text":"<ul> <li>SLOs for latency, availability, recovery time.</li> <li>Definition of \u201ccritical job,\u201d \u201cnon-critical job.\u201d</li> <li>Naming conventions and tagging for jobs and clusters.</li> </ul>"},{"location":"cookbook/governance/","title":"Governance, Change Management &amp; Best Practices","text":""},{"location":"cookbook/governance/#data-quality-schema-evolution-incidents","title":"Data Quality &amp; Schema Evolution Incidents","text":"<p>Upstream schema changes breaking deserialization. Late data spikes and watermark misconfigurations.</p>"},{"location":"cookbook/governance/#change-management","title":"Change Management","text":"<p>Required pipeline for moving from dev \u2192 staging \u2192 prod. Required checks before deploying risky changes.</p>"},{"location":"cookbook/governance/#security-auditing-and-access-control","title":"Security, Auditing, and Access Control","text":"<p>Who can deploy/stop jobs. Secrets management.</p>"},{"location":"cookbook/job_lifecycle/","title":"Job Lifecycle &amp; State Management (App Owners + Platform)","text":""},{"location":"cookbook/job_lifecycle/#1-deploying-new-jobs","title":"1- Deploying New Jobs","text":"<p>From zero to running: required configs, resource requests, restart strategies.</p> <p>There two types of Jobs/Flink application to consider for deployment:  * the java/python application (DataStream or TableAPI) * the SQL Statements. </p> <p>Then the target platform will have different mechanism and packaging depending if it is:</p> <ul> <li>Confluent Cloud for Flink</li> <li>Confluent Platform for Flink</li> <li>Apache Flink OSS</li> </ul>"},{"location":"cookbook/job_lifecycle/#11-packaged-application-deployment-oss-or-cp-flink","title":"1.1 Packaged Application Deployment (OSS or CP-Flink)","text":""},{"location":"cookbook/job_lifecycle/#context","title":"Context","text":"<p>The deployment of java packaging is the same between OpenSource and Confluent Platform Flink. So any existing DataStream application will run the same way.</p> <p>There is only yaml manifest to deploy application that will take into account environment, as applications are grouped within environment.</p>"},{"location":"cookbook/job_lifecycle/#12-sql-query-deployment-on-cp-flink","title":"1.2 SQL Query Deployment on CP-Flink","text":""},{"location":"cookbook/job_lifecycle/#context_1","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist","title":"Preconditions / Checklist","text":"<ul> <li>Be sure to have access to the CMF REST end point: could be localhost</li> <li>An environment is defined. (See this note)</li> <li>A Catalog is defined - See this note, and see example from this repository</li> </ul>"},{"location":"cookbook/job_lifecycle/#inputs-parameters","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure","title":"Procedure","text":"<ul> <li>Define a database - A database is created within a catalog and references a Kafka cluster. See product documentation</li> </ul>"},{"location":"cookbook/job_lifecycle/#rollback","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas","title":"Gotchas","text":"<ul> <li>For end-to-end validation of CP Flink with the employee demo, see code/flink-sql/00-basic-sql and run <code>cp_flink_employees_demo.py</code>.</li> </ul>"},{"location":"cookbook/job_lifecycle/#2-upgrading-jobs-safely","title":"2- Upgrading Jobs Safely","text":"<p>With compatible changes (resume from savepoint).</p>"},{"location":"cookbook/job_lifecycle/#context_2","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_1","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_1","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_1","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_1","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_1","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#21-recipe-safely-upgrade-a-flink-job-using-savepoints","title":"2.1- Recipe: Safely Upgrade a Flink Job Using Savepoints","text":"<p>Upgrade a Production Flink Job with Savepoint (Minimal Downtime)</p>"},{"location":"cookbook/job_lifecycle/#context_3","title":"Context","text":"<p>Use this when you need to:</p> <ul> <li>Deploy a new version of an existing job that must preserve state (e.g., aggregations, keyed state).</li> <li>Make compatible changes to the job graph (e.g., logic changes without breaking state schemas).</li> </ul>"},{"location":"cookbook/job_lifecycle/#preconditions-checklist_2","title":"Preconditions / Checklist","text":"<ul> <li>You understand whether the change is state compatible:<ul> <li>No removal/renaming of stateful operators or registered state names.</li> <li>No incompatible serialization changes for keyed state / operator state.</li> </ul> </li> <li>You have:<ul> <li>Access to Flink\u2019s Web UI and/or CLI (or corresponding managed-service UI).</li> <li>Permissions to trigger savepoints and cancel/start jobs.</li> </ul> </li> <li>Checkpointing is healthy:<ul> <li>Latest checkpoints successful.</li> <li>Checkpoint duration and size stable.</li> </ul> </li> </ul>"},{"location":"cookbook/job_lifecycle/#inputs-parameters_2","title":"Inputs / Parameters","text":"<ul> <li>JOB_ID or stable job name.</li> <li>SAVEPOINT_DIR (e.g., s3://my-bucket/flink/savepoints/...).</li> <li>New artifact reference (e.g., Docker image tag, jar path).</li> <li>Desired parallelism for the new version.</li> </ul>"},{"location":"cookbook/job_lifecycle/#procedure_2","title":"Procedure","text":"<ol> <li>Trigger a Savepoint<ul> <li>From UI/CLI, trigger a savepoint for the running job, specifying SAVEPOINT_DIR if required.</li> <li>Wait until the savepoint finishes successfully and record the savepoint path.</li> </ul> </li> <li>Cancel the Job with Savepoint (Optional Depending on Platform)<ul> <li>Either:     Cancel-with-savepoint in one operation, or     After savepoint completion, cancel the job gracefully.</li> <li>Confirm the job is no longer running.</li> </ul> </li> <li> <p>Deploy New Job Version from Savepoint</p> <ul> <li>Configure the new deployment with: Same job name (if your infra relies on it).</li> <li>fromSavepoint  (or equivalent UI option). <li>Updated artifact version.</li> <li>Make sure parallelism choices are valid for the state (e.g., beware of keyed state repartitioning).</li> <li> <p>Monitor Startup</p> <ul> <li>Watch logs and the Flink UI:  The job transitions to RUNNING.  No StateMigrationException or deserialization errors.</li> <li>Confirm that checkpointing restarts successfully.</li> </ul> </li> <li>Post-Deploy Validation: For at least 10\u201330 minutes (depending on SLAs):<ul> <li>Check key metrics: input rate, end-to-end latency, checkpoint status, backpressure.</li> <li>Validate downstream data (sanity checks, dashboards, or data-quality rules).</li> </ul> </li>"},{"location":"cookbook/job_lifecycle/#rollback_2","title":"Rollback","text":"<p>If you detect errors, anomalies, or instability: * Cancel the new job. * Restart the previous version from the same savepoint (or the last known good one). * Confirm successful restore and checkpointing before considering a new upgrade attempt.</p>"},{"location":"cookbook/job_lifecycle/#gotchas_2","title":"Gotchas","text":"<ul> <li>Incompatible changes to keyed state serialization often only show up at restore time; always test in staging with a copy of prod state before running this recipe in production.</li> <li>If your platform supports \u201cupgrade in place\u201d semantics (e.g., via an operator or managed UI), integrate those flows but preserve this mental model: take consistent state \u2192 deploy new logic from that state \u2192 validate \u2192 rollback if needed.</li> </ul>"},{"location":"cookbook/job_lifecycle/#22-with-incompatible-state-changes-state-migration-strategies","title":"2.2 With incompatible state changes (state migration strategies).","text":""},{"location":"cookbook/job_lifecycle/#context_4","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_3","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_3","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_3","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_3","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_3","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#3-scaling-jobs","title":"3- Scaling Jobs","text":""},{"location":"cookbook/job_lifecycle/#31-recipe-scale-a-flink-job-to-handle-increased-load","title":"3.1- Recipe: Scale a Flink Job to Handle Increased Load","text":""},{"location":"cookbook/job_lifecycle/#context_5","title":"Context","text":"<p>You see sustained backpressure in the Flink UI or high operator utilization, and the job is falling behind (increasing end-to-end latency, growing Kafka lag, etc.).</p>"},{"location":"cookbook/job_lifecycle/#preconditions-checklist_4","title":"Preconditions / Checklist","text":"<p>Check that: * The upstream system can support higher parallelism (e.g., Kafka topic partition count). * The Flink cluster has or can get enough resources (TaskManagers, CPU/memory).</p>"},{"location":"cookbook/job_lifecycle/#inputs-parameters_4","title":"Inputs / Parameters","text":"<ul> <li>Current job parallelism.</li> <li>Target parallelism.</li> <li>Environment details (e.g., Kubernetes deployment spec or managed configuration).</li> </ul>"},{"location":"cookbook/job_lifecycle/#procedure_4","title":"Procedure","text":"<ol> <li> <p>Identify Bottleneck Operators     In Flink UI, look at:         * Backpressure tab (which subtasks are under pressure).         * Operator utilization and busy time.         * Confirm where the bottleneck actually is (source, transformation, sink).</p> </li> <li> <p>Verify External Constraints</p> <ul> <li>For Kafka sources:<ul> <li>Ensure partitions \u2265 desired parallelism.</li> </ul> </li> <li>For sinks (databases, external systems):<ul> <li>Check they can handle increased concurrency.</li> </ul> </li> </ul> </li> <li> <p>Plan Parallelism Change. Because job state is keyed, changing parallelism will usually require restart-from-savepoint:</p> <ul> <li>Trigger a savepoint.</li> <li>Cancel job (if required by your environment).</li> <li>Redeploy job with higher parallelism, restoring from that savepoint.</li> </ul> </li> <li> <p>Adjust Cluster Resources</p> <ul> <li>If needed, scale out TaskManagers or underlying nodes/pods so that the job\u2019s new parallelism can be scheduled.</li> <li>Update resource requests/limits to avoid CPU starvation or frequent OOMs.</li> </ul> </li> <li>Deploy and Monitor<ul> <li>Start the job from the savepoint with increased parallelism.</li> <li>Watch:<ul> <li>Backpressure metrics.</li> <li>Throughput and lag.</li> <li>Checkpoint times (may change).</li> </ul> </li> </ul> </li> </ol>"},{"location":"cookbook/job_lifecycle/#validation","title":"Validation","text":"<ul> <li>Backpressure should reduce or disappear on the previously hot operators.</li> <li>End-to-end latency should drop or stabilize within target limits.</li> <li>No new bottleneck should appear elsewhere (e.g., sinks).</li> </ul>"},{"location":"cookbook/job_lifecycle/#rollback_4","title":"Rollback","text":"<p>If errors appear or performance worsens: * Cancel the new deployment. * Restore the previous parallelism from the same savepoint. * Reevaluate resource allocation or code hot spots before attempting scale-out again.</p>"},{"location":"cookbook/job_lifecycle/#32-handling-backpressure-and-hot-keys","title":"3.2  Handling backpressure and hot keys.","text":""},{"location":"cookbook/job_lifecycle/#context_6","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_5","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_5","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_5","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_5","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_4","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#4-backfills-and-reprocessing","title":"4- Backfills and Reprocessing","text":""},{"location":"cookbook/job_lifecycle/#41-recipe-replaying-kafka-topics-from-older-offsets","title":"4.1- Recipe: Replaying Kafka topics from older offsets.","text":""},{"location":"cookbook/job_lifecycle/#context_7","title":"Context","text":"<p>You need to recompute results for a historical period (e.g., due to a code bug or schema issue), typically from Kafka-based sources.</p>"},{"location":"cookbook/job_lifecycle/#preconditions-checklist_6","title":"Preconditions / Checklist","text":"<ul> <li>Kafka (or equivalent) retains the data for the desired backfill window.</li> <li>Downstream systems can accept re-ingestion or you have a separate backfill sink.</li> <li>You know whether: Backfill should coexist with prod job, or should stop prod job, run backfill, then resume.</li> </ul>"},{"location":"cookbook/job_lifecycle/#inputs-parameters_6","title":"Inputs / Parameters","text":"<ul> <li>Source topics and partitions.</li> <li>Time/offset range for backfill.</li> <li>Desired output location (same sinks or separate backfill tables/topics).</li> <li>Expected data volume and runtime.</li> </ul>"},{"location":"cookbook/job_lifecycle/#procedure_6","title":"Procedure","text":"<ol> <li> <p>Choose Backfill Strategy</p> <ul> <li>Separate backfill job: Same code base but different job name, reading from earlier offsets and writing to separate sinks.</li> <li>Repoint prod job: Temporarily rewind offsets and run against main sinks (riskier, requires idempotent sinks or dedupe).</li> </ul> </li> <li> <p>Configure Source Start Position</p> <ul> <li>For Kafka, configure a start offset or timestamp (e.g., \u201cstart from timestamp T0\u201d).</li> <li>Ensure the backfill job won\u2019t auto-reset to latest if it hits errors.</li> </ul> </li> <li>Isolate or Protect Downstream<ul> <li>Use dedicated output topics/tables for backfill where possible.</li> <li>If writing to prod sinks, ensure: Idempotency or deduplication and clear communication with consumers.</li> </ul> </li> <li>Deploy Backfill Job<ul> <li>Use a job configuration tuned for throughput (more parallelism, possibly looser latency constraints).</li> <li>Ensure checkpointing is still enabled (for large backfills) to allow restart.</li> </ul> </li> <li>Monitor Runtime<ul> <li>Monitor progress via Kafka lag and job metrics.</li> <li>Ensure not to starve the production job\u2019s resources.</li> </ul> </li> <li>Finalize<ul> <li>When backfill completes: Either swap backfill data into prod tables (if using separate sinks) or mark backfilled period as complete.</li> <li>Turn off the backfill job.</li> </ul> </li> </ol>"},{"location":"cookbook/job_lifecycle/#validation_1","title":"Validation","text":"<ul> <li>Check target sinks: Row counts, key distributions, and sample records match expectations.</li> <li>Check that no prolonged performance impact occurred on prod clusters.</li> </ul>"},{"location":"cookbook/job_lifecycle/#rollback-contingency","title":"Rollback / Contingency","text":"<ul> <li>If backfill misbehaves (e.g., wrong logic), stop job and discard backfill outputs if isolated.</li> <li>For shared sinks, you may need a corrective cleanup step (e.g., delete or overwrite bad window).</li> </ul>"},{"location":"cookbook/job_lifecycle/#42-running-temporary-backfill-jobs-vs-reusing-production-pipelines","title":"4.2- Running temporary backfill jobs vs. reusing production pipelines.","text":""},{"location":"cookbook/job_lifecycle/#context_8","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_7","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_7","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_7","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_6","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_5","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#5-monitoring-alerting","title":"5- Monitoring &amp; Alerting","text":""},{"location":"cookbook/job_lifecycle/#51-key-metrics-to-watch-checkpointing-backpressure-task-failures-jvm","title":"5.1- Key metrics to watch (checkpointing, backpressure, task failures, JVM).","text":""},{"location":"cookbook/job_lifecycle/#context_9","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_8","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_8","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_8","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_7","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_6","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#52-baseline-dashboards-and-alerts","title":"5.2 Baseline dashboards and alerts.","text":""},{"location":"cookbook/job_lifecycle/#context_10","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_9","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_9","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_9","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_8","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_7","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#6-performance-troubleshooting","title":"6- Performance Troubleshooting","text":""},{"location":"cookbook/job_lifecycle/#61-identifying-bottlenecks-sources-network-rocksdb","title":"6.1- Identifying bottlenecks (sources, network, RocksDB).","text":""},{"location":"cookbook/job_lifecycle/#context_11","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_10","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_10","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_10","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_9","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_8","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#62-gcmemory-tuning-networkshuffle-tuning","title":"6.2- GC/memory tuning, network/shuffle tuning.","text":""},{"location":"cookbook/job_lifecycle/#context_12","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_11","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_11","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_11","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_10","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_9","title":"Gotchas","text":""},{"location":"cookbook/job_lifecycle/#7-common-incident-recipes","title":"7- Common Incident Recipes","text":""},{"location":"cookbook/job_lifecycle/#71-job-stuck-in-restarting-or-failing-loop","title":"7.1 Job stuck in \u201crestarting\u201d or \u201cfailing\u201d loop.","text":""},{"location":"cookbook/job_lifecycle/#context_13","title":"Context","text":"<p>A job repeatedly fails and auto-restarts due to its restart strategy, causing instability and potentially thrashing external systems.</p>"},{"location":"cookbook/job_lifecycle/#preconditions-checklist_12","title":"Preconditions / Checklist","text":"<ul> <li>You can modify job configuration or redeploy.</li> <li>You have access to logs and metrics.</li> </ul>"},{"location":"cookbook/job_lifecycle/#inputs-parameters_12","title":"Inputs / Parameters","text":"<ul> <li>JOB_ID.</li> <li>Recent error stack traces.</li> <li>Restart strategy configuration.</li> </ul>"},{"location":"cookbook/job_lifecycle/#procedure_12","title":"Procedure","text":"<ol> <li>Pause/Limit Damage if Needed<ul> <li>If the job is causing harm (e.g., hammering a DB, producing corrupt data), consider:<ul> <li>Lowering restart frequency temporarily (update restart strategy), or</li> <li>Cancelling the job until you understand the issue.</li> </ul> </li> </ul> </li> <li>Identify Root Error<ul> <li>From Flink UI or logs, capture the exception causing the failure.<ul> <li>Is it data-related (bad record)?</li> <li>External system (timeout, 429/500 responses)?</li> <li>Program bug (NPE, etc.)?</li> </ul> </li> </ul> </li> <li>Classify Incident<ul> <li>Data/record issue: maybe a single poison pill message.</li> <li>Infrastructure issue: external system down/slow.</li> <li>Code bug: deterministic exception for some input.</li> </ul> </li> <li>Choose Temporary Mitigation<ul> <li>Data/record issue: Implement dead-letter queue mechanism or filtering, redeploy job.</li> <li>Infrastructure issue: Throttle load, increase timeouts, or temporarily disable sink.</li> <li>Code bug: Hotfix code in staging \u2192 redeploy from last savepoint.</li> </ul> </li> <li>Redeploy from Last Known Good State<ul> <li>Use the most recent successful checkpoint or savepoint before the incident.</li> <li>Ensure that the new version handles the problematic condition.</li> </ul> </li> </ol>"},{"location":"cookbook/job_lifecycle/#validation_2","title":"Validation","text":"<ul> <li>Job stays in RUNNING state over your defined stability window.</li> <li>Alerts for repeated failures clear.</li> </ul>"},{"location":"cookbook/job_lifecycle/#rollback_11","title":"Rollback","text":"<p>If hotfix fails, roll back to last known good version with a mitigation that avoids the triggering condition (e.g., filtering the offending key).</p>"},{"location":"cookbook/job_lifecycle/#72-recipe-handling-persistent-checkpoint-failures","title":"7.2- Recipe: Handling Persistent Checkpoint Failures","text":""},{"location":"cookbook/job_lifecycle/#context_14","title":"Context","text":"<p>You\u2019re seeing alerts that a job\u2019s checkpoints are failing for several consecutive attempts, or the Flink UI shows repeated checkpoint failures. Prolonged failure threatens your ability to recover reliably and meet recovery SLOs.</p>"},{"location":"cookbook/job_lifecycle/#preconditions-checklist_13","title":"Preconditions / Checklist","text":"<p>Access to: * Flink Web UI. * Logs for JobManager and TaskManagers. * State backend storage system (e.g., S3/GCS/HDFS). * Know the job\u2019s criticality and tolerated downtime (can you pause input or not?).</p>"},{"location":"cookbook/job_lifecycle/#inputs-parameters_13","title":"Inputs / Parameters","text":"<ul> <li>Job name / JOB_ID.</li> <li>Time window of failures.</li> <li>Checkpoint directory URI (from job config).</li> </ul>"},{"location":"cookbook/job_lifecycle/#procedure_13","title":"Procedure","text":"<ol> <li>Inspect Failure Reason in Flink UI:  In the job\u2019s Checkpoints tab, open a failed checkpoint and note the error:<ul> <li>Storage-related (e.g., permission denied, quota exceeded).</li> <li>Timeout / slow I/O.</li> <li>Operator-specific errors during snapshot.</li> <li>Serialization errors.</li> </ul> </li> <li>Check Storage Health: If errors mention the state backend or filesystem:<ul> <li>Attempt a small write/read manually to the checkpoint directory from a node or pod with similar permissions.</li> <li>Verify IAM/ACLs, quotas, and recent infra changes.</li> </ul> </li> <li> <p>Check Operator Logs</p> <ul> <li>Look for stack traces near checkpoint failure times.</li> <li>If a specific operator is failing snapshot, note which one (e.g., custom sink, third-party connector).</li> </ul> </li> <li> <p>Validate Checkpointing Configuration</p> <ul> <li>Are you trying to checkpoint too frequently for your workload?</li> <li>Compare: 1/ Checkpoint interval vs. average checkpoint duration. 2/ Size of checkpoints (state size growth trend).</li> </ul> </li> <li>Immediate Stabilization Options. Depending on what you found:<ul> <li>Storage issue: fix permissions/quotas; once resolved, checkpointing should resume without job restart.</li> <li>Timeout / too heavy:<ul> <li>Temporarily increase checkpoint timeout.</li> <li>Increase interval to reduce pressure.</li> </ul> </li> <li>Operator-specific bug:<ul> <li>Decide whether to temporarily disable that operator, hotfix its code, or deploy a version that bypasses the failing path.</li> </ul> </li> </ul> </li> <li>When Job is Unstable: Consider taking a manual savepoint (if still possible) or stopping ingest (e.g., pausing Kafka consumer, if your environment supports it), then restarting from the last successful checkpoint/savepoint.</li> </ol>"},{"location":"cookbook/job_lifecycle/#validation_3","title":"Validation","text":"<ul> <li>Check that new checkpoints complete successfully over at least a few consecutive attempts.</li> <li>Monitor checkpoint duration and size for stability.</li> </ul>"},{"location":"cookbook/job_lifecycle/#rollback-contingency_1","title":"Rollback / Contingency","text":"<p>If your configuration changes worsen things:</p> <ul> <li>Revert checkpoint interval/timeout to previous values.</li> <li>If no quick fix is available, escalate: consider pausing job input or taking it offline with communication to downstream consumers.</li> </ul>"},{"location":"cookbook/job_lifecycle/#gotchas_10","title":"Gotchas","text":"<ul> <li>Changes in upstream schema or data volume spikes can indirectly cause checkpoint issues (e.g., bigger state, slower snapshots).</li> <li>If using a shared object store, another workload may have changed performance characteristics.</li> </ul>"},{"location":"cookbook/job_lifecycle/#73-state-corruption-or-version-mismatch","title":"7.3 State corruption or version mismatch.","text":""},{"location":"cookbook/job_lifecycle/#context_15","title":"Context","text":""},{"location":"cookbook/job_lifecycle/#preconditions-checklist_14","title":"Preconditions / Checklist","text":""},{"location":"cookbook/job_lifecycle/#inputs-parameters_14","title":"Inputs / Parameters","text":""},{"location":"cookbook/job_lifecycle/#procedure_14","title":"Procedure","text":""},{"location":"cookbook/job_lifecycle/#rollback_12","title":"Rollback","text":""},{"location":"cookbook/job_lifecycle/#gotchas_11","title":"Gotchas","text":""},{"location":"cookbook/terraform/","title":"Terraform for Confluent Cloud Flink","text":"Version <ul> <li>Created November 2024</li> <li>Updated January 2026</li> </ul> <p>This guide covers using Terraform to deploy and manage Confluent Cloud infrastructure for Kafka and Flink applications. </p>"},{"location":"cookbook/terraform/#overview","title":"Overview","text":"<p>The Confluent Terraform Provider enables infrastructure-as-code management for Confluent Cloud resources including environments, Kafka clusters, Schema Registry, Flink compute pools, and Flink SQL statements.</p> Terraform Core Principles <p>When running <code>terraform apply</code>, Terraform calculates differences between the current state (tracked in its state file) and the desired state, then applies only necessary changes. This enables incremental building of infrastructure.</p> <p>The <code>terraform plan</code> command performs the following steps by default: </p> <ul> <li>It first runs a refresh operation to update its in-memory state with the actual, current configuration of your remote infrastructure by making API calls to the provider.</li> <li>It then compares this updated state with the desired state defined in your local Terraform configuration files (.tf files).</li> <li>Finally, it outputs a set of proposed changes (add, change, or destroy) required to make the running infrastructure match your configuration files. </li> <li>States are saved in a file (not commited to git) named: <code>terraform.tfstate</code>.</li> </ul> <p>For Confluent Cloud, start with an environment, service account, and role binding, then add resources over time. Terraform automatically determines the correct order of operations based on resource dependencies.</p> <p>For managing different stages (development, staging, production), use separate configurations with dedicated state files.</p> <p>Key Resources:</p> <ul> <li>Confluent Terraform Provider Documentation</li> <li>Sample Project Tutorial</li> <li>Configuration Examples</li> </ul>"},{"location":"cookbook/terraform/#prerequisites","title":"Prerequisites","text":""},{"location":"cookbook/terraform/#terraform-cli","title":"Terraform CLI","text":"<p>Install the latest Terraform CLI:</p> <pre><code>terraform version\n</code></pre>"},{"location":"cookbook/terraform/#confluent-cloud-api-keys","title":"Confluent Cloud API Keys","text":"<p>Create API keys with OrganizationAdmin role at confluent.cloud/settings/api-keys.</p> <p>For production, use service account keys rather than user keys. Create a service account for the Terraform runner and assign the OrganizationAdmin role following the RBAC guide.</p> <p>List existing keys:</p> <pre><code>confluent api-key list | grep &lt;cc_userid&gt;\n</code></pre>"},{"location":"cookbook/terraform/#environment-variables","title":"Environment Variables","text":"<p>Export credentials as environment variables:</p> <pre><code>export TF_VAR_confluent_cloud_api_key=\"&lt;your-api-key&gt;\"\nexport TF_VAR_confluent_cloud_api_secret=\"&lt;your-api-secret&gt;\"\n</code></pre> <p>Do not commit <code>terraform.tfstate</code> or environment variable files to git.</p>"},{"location":"cookbook/terraform/#provider-configuration","title":"Provider Configuration","text":"<ul> <li> <p>Create <code>main.tf</code> with the Confluent provider:   <pre><code>terraform {\n  required_providers {\n    confluent = {\n      source  = \"confluentinc/confluent\"\n      version = \"2.57.0\"\n    }\n  }\n}\n\nprovider \"confluent\" {\n  cloud_api_key    = var.confluent_cloud_api_key\n  cloud_api_secret = var.confluent_cloud_api_secret\n}\n\ndata \"confluent_organization\" \"my_org\" {}\n\ndata \"confluent_flink_region\" \"flink_region\" {\n  cloud  = var.cloud_provider\n  region = var.cloud_region\n}\n</code></pre></p> </li> <li> <p>Define variables in <code>variables.tf</code> that will be available to all .tf files in the same folder:   <pre><code>variable \"confluent_cloud_api_key\" {\n  description = \"Confluent Cloud API Key\"\n  type        = string\n  sensitive   = true\n}\n\nvariable \"confluent_cloud_api_secret\" {\n  description = \"Confluent Cloud API Secret\"\n  type        = string\n  sensitive   = true\n}\n\nvariable \"cloud_provider\" {\n  description = \"Cloud provider (AWS, GCP, AZURE)\"\n  type        = string\n  default     = \"AWS\"\n}\n\nvariable \"cloud_region\" {\n  description = \"Cloud region\"\n  type        = string\n  default     = \"us-west-2\"\n}\n\nvariable \"prefix\" {\n  description = \"Prefix for resource names\"\n  type        = string\n  default     = \"j9r\"\n}\n</code></pre></p> </li> <li> <p>Initialize Terraform:   <pre><code>terraform init\n</code></pre></p> </li> </ul>"},{"location":"cookbook/terraform/#resource-definitions","title":"Resource Definitions","text":"<p>Build infrastructure incrementally following dependency order. The deployment/cc-terraform folder contains a complete base infrastructure example.</p>"},{"location":"cookbook/terraform/#environment-layer","title":"Environment Layer","text":"<p>Create the Confluent Cloud environment with Stream Governance:</p> <pre><code>resource \"confluent_environment\" \"env\" {\n  display_name = \"${var.prefix}-env\"\n\n  stream_governance {\n    package = \"ESSENTIALS\"\n  }\n}\n\nresource \"confluent_service_account\" \"env-manager\" {\n  display_name = \"${var.prefix}-env-manager\"\n  description  = \"Service account to manage ${var.prefix} environment\"\n  depends_on   = [confluent_environment.env]\n}\n\nresource \"confluent_role_binding\" \"env-admin\" {\n  principal   = \"User:${confluent_service_account.env-manager.id}\"\n  role_name   = \"EnvironmentAdmin\"\n  crn_pattern = confluent_environment.env.resource_name\n}\n</code></pre> <p>Importing existing resources: It is relevant to reuse an existing environment and different resources. For existing environments, use an <code>imports.tf</code> file that link values to variables like (this import file may be .gitignore):</p> <pre><code>import {\n  to = confluent_environment.env\n  id = \"env-abc123\"\n}\n\nimport {\n  to = confluent_service_account.env-manager\n  id = \"sa-xyz789\"\n}\n</code></pre>"},{"location":"cookbook/terraform/#kafka-layer","title":"Kafka Layer","text":"<p>Create a Kafka cluster with API keys:</p> <pre><code>resource \"confluent_kafka_cluster\" \"standard\" {\n  display_name = \"${var.prefix}-kafka\"\n  availability = \"SINGLE_ZONE\"\n  cloud        = var.cloud_provider\n  region       = var.cloud_region\n  standard {}\n\n  environment {\n    id = confluent_environment.env.id\n  }\n}\n\nresource \"confluent_api_key\" \"kafka-api-key\" {\n  display_name = \"kafka-api-key\"\n  description  = \"Kafka API Key\"\n  owner {\n    id          = confluent_service_account.env-manager.id\n    api_version = confluent_service_account.env-manager.api_version\n    kind        = confluent_service_account.env-manager.kind\n  }\n  managed_resource {\n    id          = confluent_kafka_cluster.standard.id\n    api_version = confluent_kafka_cluster.standard.api_version\n    kind        = confluent_kafka_cluster.standard.kind\n    environment {\n      id = confluent_environment.env.id\n    }\n  }\n}\n</code></pre> <p>Import existing Kafka cluster definition</p> <pre><code>terraform import confluent_kafka_cluster.standard env-abc123/lkc-xyz789\n</code></pre>"},{"location":"cookbook/terraform/#schema-registry","title":"Schema Registry","text":"<p>Schema Registry is auto-provisioned with the environment. Reference it as a data source using <code>data</code> construct:</p> <pre><code>data \"confluent_schema_registry_cluster\" \"essentials\" {\n  environment {\n    id = confluent_environment.env.id\n  }\n  depends_on = [confluent_kafka_cluster.standard]\n}\n\nresource \"confluent_api_key\" \"schema-registry-api-key\" {\n  display_name = \"schema-registry-api-key\"\n  owner {\n    id          = confluent_service_account.env-manager.id\n    api_version = confluent_service_account.env-manager.api_version\n    kind        = confluent_service_account.env-manager.kind\n  }\n  managed_resource {\n    id          = data.confluent_schema_registry_cluster.essentials.id\n    api_version = data.confluent_schema_registry_cluster.essentials.api_version\n    kind        = data.confluent_schema_registry_cluster.essentials.kind\n    environment {\n      id = confluent_environment.env.id\n    }\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#flink-layer","title":"Flink Layer","text":"<p>Flink requires two service accounts with specific role bindings:</p> <ol> <li>flink-app - Runtime principal for Flink statements</li> <li>flink-developer-sa - Deploys Flink statements</li> </ol> <pre><code># Flink runtime principal\nresource \"confluent_service_account\" \"flink-app\" {\n  display_name = \"${var.prefix}-flink-app\"\n  description  = \"Service account as which Flink statements run\"\n}\n\nresource \"confluent_role_binding\" \"flink-app-clusteradmin\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"CloudClusterAdmin\"\n  crn_pattern = confluent_kafka_cluster.standard.rbac_crn\n}\n\nresource \"confluent_role_binding\" \"flink-app-sr-read\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"DeveloperRead\"\n  crn_pattern = \"${data.confluent_schema_registry_cluster.essentials.resource_name}/subject=*\"\n}\n\nresource \"confluent_role_binding\" \"flink-app-sr-write\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"DeveloperWrite\"\n  crn_pattern = \"${data.confluent_schema_registry_cluster.essentials.resource_name}/subject=*\"\n}\n\n# Flink developer (deploys statements)\nresource \"confluent_service_account\" \"flink-developer-sa\" {\n  display_name = \"${var.prefix}-fd-sa\"\n  description  = \"Service account to deploy Flink statements\"\n}\n\nresource \"confluent_role_binding\" \"flink-developer\" {\n  principal   = \"User:${confluent_service_account.flink-developer-sa.id}\"\n  role_name   = \"FlinkDeveloper\"\n  crn_pattern = confluent_environment.env.resource_name\n}\n\nresource \"confluent_role_binding\" \"flink-assigner\" {\n  principal   = \"User:${confluent_service_account.flink-developer-sa.id}\"\n  role_name   = \"Assigner\"\n  crn_pattern = \"${data.confluent_organization.my_org.resource_name}/service-account=${confluent_service_account.flink-app.id}\"\n}\n\nresource \"confluent_api_key\" \"flink-api-key\" {\n  display_name = \"flink-api-key\"\n  description  = \"Flink API Key owned by flink-developer-sa\"\n  owner {\n    id          = confluent_service_account.flink-developer-sa.id\n    api_version = confluent_service_account.flink-developer-sa.api_version\n    kind        = confluent_service_account.flink-developer-sa.kind\n  }\n  managed_resource {\n    id          = data.confluent_flink_region.flink_region.id\n    api_version = data.confluent_flink_region.flink_region.api_version\n    kind        = data.confluent_flink_region.flink_region.kind\n    environment {\n      id = confluent_environment.env.id\n    }\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#flink-compute-pools","title":"Flink Compute Pools","text":"<pre><code>resource \"confluent_flink_compute_pool\" \"default\" {\n  display_name = \"default\"\n  cloud        = upper(data.confluent_flink_region.flink_region.cloud)\n  region       = data.confluent_flink_region.flink_region.region\n  max_cfu      = 50\n  environment {\n    id = confluent_environment.env.id\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#deploying-flink-statements","title":"Deploying Flink Statements","text":"<p>Flink SQL statements can be deployed using the <code>confluent_flink_statement</code> resource. See the Flink Statement Resource Documentation. We recommend to separate the deployment from the infrastructure but reference the terraform state from to get definitions from already deployed resources. </p> <p>See e2e-demos/cc-cdc-tx-demo/cc-flink-sql/terraform folder for such approach.</p>"},{"location":"cookbook/terraform/#single-compute-pool-configuration","title":"Single Compute Pool Configuration","text":"<p>When managing a single compute pool, configure Flink parameters in the provider block:</p> <pre><code>provider \"confluent\" {\n  organization_id       = var.organization_id\n  environment_id        = var.environment_id\n  flink_compute_pool_id = var.flink_compute_pool_id\n  flink_rest_endpoint   = var.flink_rest_endpoint\n  flink_api_key         = var.flink_api_key\n  flink_api_secret      = var.flink_api_secret\n  flink_principal_id    = var.flink_principal_id\n}\n\nresource \"confluent_flink_statement\" \"create_table\" {\n  statement = \"CREATE TABLE orders (order_id STRING, amount DECIMAL(10,2));\"\n  properties = {\n    \"sql.current-catalog\"  = var.environment_display_name\n    \"sql.current-database\" = var.kafka_cluster_display_name\n  }\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#multiple-compute-pools-configuration","title":"Multiple Compute Pools Configuration","text":"<p>When managing multiple compute pools, specify resources explicitly:</p> <pre><code>resource \"confluent_flink_statement\" \"transaction_faker\" {\n  organization {\n    id = data.confluent_organization.my_org.id\n  }\n  environment {\n    id = confluent_environment.env.id\n  }\n  compute_pool {\n    id = confluent_flink_compute_pool.data-generation.id\n  }\n  principal {\n    id = confluent_service_account.flink-app.id\n  }\n  rest_endpoint = data.confluent_flink_region.flink_region.rest_endpoint\n\n  properties = {\n    \"sql.current-catalog\"  = confluent_environment.env.display_name\n    \"sql.current-database\" = confluent_kafka_cluster.standard.display_name\n  }\n\n  credentials {\n    key    = confluent_api_key.flink-api-key.id\n    secret = confluent_api_key.flink-api-key.secret\n  }\n\n  statement      = file(\"${path.module}/sql/create_table.sql\")\n  statement_name = \"create-orders-table\"\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#statement-lifecycle-management","title":"Statement Lifecycle Management","text":"<p>Use the <code>stopped</code> attribute to control statement execution:</p> <pre><code>resource \"confluent_flink_statement\" \"streaming_job\" {\n  statement = \"INSERT INTO sink SELECT * FROM source;\"\n  stopped   = false  # Set to true to stop the statement\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre> <p>For complex deployments with many statements, blue/green releases, consider using shift_left_utils.</p>"},{"location":"cookbook/terraform/#workflow-commands","title":"Workflow Commands","text":""},{"location":"cookbook/terraform/#standard-workflow","title":"Standard Workflow","text":"<pre><code>terraform init      # Initialize provider\nterraform validate  # Validate configuration\nterraform plan      # Preview changes\nterraform apply     # Apply changes\n</code></pre>"},{"location":"cookbook/terraform/#auto-approve-for-cicd","title":"Auto-approve for CI/CD","text":"<pre><code>terraform apply --auto-approve\n</code></pre>"},{"location":"cookbook/terraform/#retrieve-outputs","title":"Retrieve Outputs","text":"<pre><code>terraform output\nterraform output -json &gt; outputs.json\n</code></pre>"},{"location":"cookbook/terraform/#handle-401-errors","title":"Handle 401 Errors","text":"<p>If you encounter 401 errors, verify the API key environment variables are set correctly.</p>"},{"location":"cookbook/terraform/#resource-importer","title":"Resource Importer","text":"<p>The Confluent Resource Importer exports existing Confluent Cloud resources to Terraform configuration files. This is useful for:</p> <ul> <li>Discovering existing resource IDs for import statements</li> <li>Generating Terraform configurations from existing infrastructure</li> <li>Cross-checking deployed resources against Terraform definitions</li> </ul>"},{"location":"cookbook/terraform/#using-the-importer","title":"Using the Importer","text":"<p>Create a separate folder for the importer (e.g., <code>importer/main.tf</code>):</p> <pre><code>terraform {\n  required_providers {\n    confluent = {\n      source  = \"confluentinc/confluent\"\n      version = \"2.57.0\"\n    }\n  }\n}\n\nprovider \"confluent\" {}\n\nresource \"confluent_tf_importer\" \"cloud_resources\" {\n  output_path = \"${path.module}/imported_infrastructure\"\n\n  # Supported resource types (cannot mix Cloud and Kafka resources)\n  resources = [\n    \"confluent_environment\",\n    \"confluent_service_account\",\n    \"confluent_kafka_cluster\"\n  ]\n}\n</code></pre>"},{"location":"cookbook/terraform/#supported-resource-types","title":"Supported Resource Types","text":"<p>The importer supports these resource types (but Cloud and Kafka resources cannot be imported simultaneously):</p> Mode Resources Cloud <code>confluent_environment</code>, <code>confluent_service_account</code>, <code>confluent_kafka_cluster</code> Kafka <code>confluent_kafka_topic</code>, <code>confluent_kafka_acl</code>, <code>confluent_connector</code> Schema <code>confluent_schema</code>"},{"location":"cookbook/terraform/#running-the-importer","title":"Running the Importer","text":"<pre><code>export CONFLUENT_CLOUD_API_KEY=\"&lt;your-api-key&gt;\"\nexport CONFLUENT_CLOUD_API_SECRET=\"&lt;your-api-secret&gt;\"\n\ncd importer\nterraform init\nterraform apply -auto-approve\n</code></pre> <p>The importer creates:</p> <ul> <li><code>imported_infrastructure/main.tf</code> - Resource definitions</li> <li><code>imported_infrastructure/terraform.tfstate</code> - State file with resource IDs</li> <li><code>imported_infrastructure/variables.tf</code> - Variable definitions</li> </ul>"},{"location":"cookbook/terraform/#extracting-import-ids","title":"Extracting Import IDs","text":"<p>After running the importer, extract resource IDs from the generated files:</p> <pre><code># Find environment IDs\ngrep -A2 \"display_name.*j9r-env\" imported_infrastructure/main.tf\n\n# Find service account IDs from state\ngrep -A5 \"j9r\" imported_infrastructure/terraform.tfstate | grep '\"id\"'\n</code></pre>"},{"location":"cookbook/terraform/#creating-importstf","title":"Creating imports.tf","text":"<p>Use the discovered IDs to create an <code>imports.tf</code> file in your main configuration:</p> <pre><code>import {\n  to = confluent_environment.env\n  id = \"env-nknqp3\"\n}\n\nimport {\n  to = confluent_kafka_cluster.standard\n  id = \"env-nknqp3/lkc-3mnm0m\"\n}\n\nimport {\n  to = confluent_service_account.env-manager\n  id = \"sa-7vvrvw\"\n}\n\nimport {\n  to = confluent_flink_compute_pool.default\n  id = \"env-nknqp3/lfcp-80r0n7\"\n}\n</code></pre>"},{"location":"cookbook/terraform/#cleanup","title":"Cleanup","text":"<p>After importing resources into your main Terraform state, the importer folder can be deleted:</p> <pre><code>rm -rf importer/\n</code></pre>"},{"location":"cookbook/terraform/#iterative-development","title":"Iterative Development","text":"<p>Build Terraform manifests incrementally:</p> <ol> <li>Add resources and variables</li> <li>Run validation:    <pre><code>terraform validate\nterraform plan\nterraform apply\n</code></pre></li> <li>Review outputs and add dependent resources</li> <li>Repeat</li> </ol> <p>This approach provides clear understanding of dependencies and makes troubleshooting easier.</p>"},{"location":"cookbook/terraform/#adding-flink-to-an-existing-environment","title":"Adding Flink to an Existing Environment","text":"<p>This section covers adding Flink resources to an existing Confluent Cloud environment that already has Kafka and Schema Registry.</p>"},{"location":"cookbook/terraform/#step-1-import-existing-resources","title":"Step 1: Import Existing Resources","text":"<p>Use the Resource Importer (see previous section) or manually create <code>imports.tf</code> with your existing resource IDs:</p> <pre><code># imports.tf - Reference existing resources\nimport {\n  to = confluent_environment.env\n  id = \"env-abc123\"\n}\n\nimport {\n  to = confluent_kafka_cluster.standard\n  id = \"env-abc123/lkc-xyz789\"\n}\n\nimport {\n  to = confluent_service_account.env-manager\n  id = \"sa-123abc\"\n}\n</code></pre>"},{"location":"cookbook/terraform/#step-2-create-resource-definitions-for-existing-resources","title":"Step 2: Create Resource Definitions for Existing Resources","text":"<p>Create minimal resource blocks that match your existing infrastructure:</p> <pre><code># env.tf - Match your existing environment\nresource \"confluent_environment\" \"env\" {\n  display_name = \"my-existing-env\"\n  stream_governance {\n    package = \"ADVANCED\"  # or \"ESSENTIALS\"\n  }\n}\n\n# kafka.tf - Match your existing cluster\nresource \"confluent_kafka_cluster\" \"standard\" {\n  display_name = \"my-existing-kafka\"\n  availability = \"SINGLE_ZONE\"\n  cloud        = \"AWS\"\n  region       = \"us-west-2\"\n  standard {}\n  environment {\n    id = confluent_environment.env.id\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#step-3-add-flink-specific-resources","title":"Step 3: Add Flink-Specific Resources","text":"<p>See the quickstart TF sample for some examples.</p> <p>Create <code>flink.tf</code> with the Flink service accounts, role bindings, and API keys:</p> <pre><code># flink.tf - New Flink resources\n\n# Data source for Schema Registry (already exists)\ndata \"confluent_schema_registry_cluster\" \"essentials\" {\n  environment {\n    id = confluent_environment.env.id\n  }\n}\n\n# Flink runtime principal - runs Flink statements\nresource \"confluent_service_account\" \"flink-app\" {\n  display_name = \"${var.prefix}-flink-app\"\n  description  = \"Service account as which Flink statements run\"\n}\n\n# Role bindings for flink-app\nresource \"confluent_role_binding\" \"flink-app-clusteradmin\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"CloudClusterAdmin\"\n  crn_pattern = confluent_kafka_cluster.standard.rbac_crn\n}\n\nresource \"confluent_role_binding\" \"flink-app-sr-read\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"DeveloperRead\"\n  crn_pattern = \"${data.confluent_schema_registry_cluster.essentials.resource_name}/subject=*\"\n}\n\nresource \"confluent_role_binding\" \"flink-app-sr-write\" {\n  principal   = \"User:${confluent_service_account.flink-app.id}\"\n  role_name   = \"DeveloperWrite\"\n  crn_pattern = \"${data.confluent_schema_registry_cluster.essentials.resource_name}/subject=*\"\n}\n\n# Flink developer - deploys Flink statements\nresource \"confluent_service_account\" \"flink-developer-sa\" {\n  display_name = \"${var.prefix}-fd-sa\"\n  description  = \"Service account to deploy Flink statements\"\n}\n\nresource \"confluent_role_binding\" \"flink-developer\" {\n  principal   = \"User:${confluent_service_account.flink-developer-sa.id}\"\n  role_name   = \"FlinkDeveloper\"\n  crn_pattern = confluent_environment.env.resource_name\n}\n\nresource \"confluent_role_binding\" \"flink-assigner\" {\n  principal   = \"User:${confluent_service_account.flink-developer-sa.id}\"\n  role_name   = \"Assigner\"\n  crn_pattern = \"${data.confluent_organization.my_org.resource_name}/service-account=${confluent_service_account.flink-app.id}\"\n}\n\n# Flink API key for deploying statements\nresource \"confluent_api_key\" \"flink-api-key\" {\n  display_name = \"${var.prefix}-flink-api-key\"\n  description  = \"Flink API Key owned by flink-developer-sa\"\n  owner {\n    id          = confluent_service_account.flink-developer-sa.id\n    api_version = confluent_service_account.flink-developer-sa.api_version\n    kind        = confluent_service_account.flink-developer-sa.kind\n  }\n  managed_resource {\n    id          = data.confluent_flink_region.flink_region.id\n    api_version = data.confluent_flink_region.flink_region.api_version\n    kind        = data.confluent_flink_region.flink_region.kind\n    environment {\n      id = confluent_environment.env.id\n    }\n  }\n}\n</code></pre> <p>Create <code>pools.tf</code> with Flink compute pools:</p> <pre><code># pools.tf - Flink compute pools\n\nresource \"confluent_flink_compute_pool\" \"default\" {\n  display_name = \"default\"\n  cloud        = upper(data.confluent_flink_region.flink_region.cloud)\n  region       = data.confluent_flink_region.flink_region.region\n  max_cfu      = 50\n  environment {\n    id = confluent_environment.env.id\n  }\n}\n</code></pre>"},{"location":"cookbook/terraform/#step-4-apply-the-configuration","title":"Step 4: Apply the Configuration","text":"<pre><code>terraform init\nterraform plan    # Verify imports and new resources\nterraform apply\n</code></pre> <p>The plan should show:</p> <ul> <li>Existing resources being imported (environment, kafka cluster, service accounts)</li> <li>New Flink resources being created (flink-app, flink-developer-sa, role bindings, compute pool)</li> </ul>"},{"location":"cookbook/terraform/#removing-flink-resources","title":"Removing Flink resources","text":"<p>Compute pools with running Flink statements cannot be deleted - stop all statements first.</p> <ul> <li> <p>If the compute pool is in your Terraform state, simply remove or comment out the resource block in the tf and output.tf and apply   <pre><code>terraform plan\nterraform apply\n</code></pre></p> </li> <li> <p>To destroy only a specific compute pool without removing it from config:   <pre><code>terraform destroy -target=confluent_flink_compute_pool.data-generation\n</code></pre></p> </li> <li> <p>If you want to stop managing it with Terraform but keep it running:   <pre><code>terraform state rm confluent_flink_compute_pool.data-generation\n</code></pre></p> </li> </ul>"},{"location":"labs/","title":"Local code and demonstrations, plus other public references","text":"<p>This section lists the current demonstrations and labs in this git repository or the interesting public repositories about Flink</p>"},{"location":"labs/#1-flink-sql","title":"1. Flink SQL","text":"<p>Under code/flink-sql folder</p>"},{"location":"labs/#11-basic-sql-getting-started","title":"1.1 Basic SQL &amp; Getting Started","text":"Sample Path Description Basic SQL (employees, dedup, aggregation) flink-sql/00-basic-sql CSV employees demo: create table, deduplication (ROW_NUMBER), aggregation by department, streaming vs batch. Confluent Cloud: customers table, dedup, tombstone creation, snapshot query. Terraform in <code>cc-flink/terraform/</code>. OSS / local Flink flink-sql/00-basic-sql/oss-flink <code>create_customers.sql</code>, <code>create_orders.sql</code> for local Flink."},{"location":"labs/#12-kafka-connectors","title":"1.2 Kafka &amp; Connectors","text":"Sample Path Description Kafka + Flink (Docker, local, Confluent) flink-sql/01-kafka-flink Docker Compose (Kafka + Flink 1.20), word count from Kafka topic (<code>user_msgs</code> \u2192 <code>word_count</code>), SPLIT + UNNEST. Local Kafka + Flink binary, Confluent Cloud Kafka (TO DO). Optional Datagen connector configs in <code>datagen-config/</code>. Kafka-Flink Docker flink-sql/01-kafka-flink/kafka-flink-docker <code>docker-compose.yaml</code>, Kafka config, Flink Kafka connector JARs, <code>starting_script.sql</code>."},{"location":"labs/#13-nested-rows-arrays-json","title":"1.3 Nested Rows, Arrays &amp; JSON","text":"Sample Path Description Nested ROW, ARRAY_AGG, UNNEST flink-sql/03-nested-row Nested user schema (<code>vw.nested_user.sql</code>, <code>dml.nested_user.sql</code>), ARRAY_AGG with tumbling window (<code>vw.array_of_rows.sql</code>), CROSS JOIN UNNEST. Sample <code>fligh_out.json</code>. Truck loads (last record per key from array) flink-sql/03-nested-row/truck_loads Source with <code>ARRAY&lt;ROW&gt;</code>, UNNEST, upsert sink to keep last load per truck. DDL, DML, insert data. Suite / asset ARRAY_AGG flink-sql/03-nested-row/cc-array-agg Aggregate asset IDs per suite with changelog (upsert). <code>cc_array_agg.sql</code>, <code>cc_array_agg_on_row.sql</code>. Healthcare CDC transformation flink-sql/03-nested-row/cc-flink-health CDC JSON \u2192 flattened Member/Provider entities. Schemas, Flink SQL transforms, Python producer, Schema Registry, test data."},{"location":"labs/#14-joins","title":"1.4 Joins","text":"Sample Path Description Joins playground (overview) flink-sql/04-joins Stream-stream joins (orders, products, shipments): left join, CTAS upsert, interval join. Order-per-minute windowing, UNNEST product_ids, avg volume per minute. Data skew (salted joins). <code>docker-compose.yaml</code>. CC stream-to-stream (orders, products, shipments) flink-sql/04-joins/cc DDLs and inserts for orders, products, shipments; order\u2013product join, CTAS joins; avg product volume per minute. Inner join with dedup (Asset / AssetType) flink-sql/04-joins/cc_inner_join_with_dedup Entity\u2013entityType: ROW_NUMBER dedup on raw assets and types, inner join, ARRAY_AGG for subtype/type names. Confluent Cloud. Data skew (salted joins) flink-sql/04-joins/data_skew Join user\u2013group with optional salting to handle skew. DML and insert scripts. Event status processing flink-sql/04-joins/event_status_processing CDC-style <code>event_status</code> (eventId, eventProcessed, eventTime). DDL and processing pattern. Group / user hierarchy flink-sql/04-joins/group_users Hierarchy in one table (e.g. hospital \u2192 department \u2192 group \u2192 person). Self-joins, ARRAY_AGG, two-level hierarchy; dim_latest_group_users with tombstone handling for group-level deletes. Rule match on sensors flink-sql/04-joins/rule_match_on_sensors Temporal join of sensor stream to static <code>sensor_rules</code> (tenant, parameter, thresholds). Faker-based sensors source."},{"location":"labs/#15-changelog-mode-append-retract-upsert","title":"1.5 Changelog Mode (Append, Retract, Upsert)","text":"Sample Path Description Changelog mode (append, retract, upsert) flink-sql/05-changelog Orders + products: append vs retract vs upsert. Impact on aggregation, join, dedup. CTAS for enriched_orders, user_order_quantity. Diagrams in <code>docs/</code>. Makefile for deploy/insert."},{"location":"labs/#16-schema-evolution","title":"1.6 Schema Evolution","text":"Sample Path Description Schema refactoring (products \u2192 discounts) flink-sql/07-schema-refactoring PostgreSQL products table refactored: discount moved to <code>discounts</code> table. Flink join products + discounts, support old (inline discount) and new (discount_id) schema. <code>alter_product.sql</code>, <code>pd_discounts.sql</code>, <code>ps_products_v1.sql</code>."},{"location":"labs/#17-snapshot-query-search-external-tables","title":"1.7 Snapshot Query &amp; Search External Tables","text":"Sample Path Description Snapshot query flink-sql/08-snapshot-external-query Confluent Cloud snapshot query: Datagen connector, <code>SET sql.snapshot.mode = 'now'</code>, count on append-only table. External table lookup flink-sql/08-snapshot-external-query Use KEY_SEARCH_AGG with external table deployed on AWS RDS Postgresql"},{"location":"labs/#18-temporal-joins","title":"1.8 Temporal Joins","text":"Sample Path Description Temporal joins (versioned table) flink-sql/09-temporal-joins Join orders to versioned <code>currency_rates</code> with <code>FOR SYSTEM_TIME AS OF orders.order_time</code>. Confluent Cloud: DDLs, insert orders/rates, behavior when rates arrive late. Local: CSV + <code>temporal_joins.sql</code>."},{"location":"labs/#19-windowing","title":"1.9 Windowing","text":"Sample Path Description Tumbling &amp; hopping windows flink-sql/10-windowing Tumbling window: distinct order count per minute. Hopping: 10-minute window, 5-minute slide. Uses deduplicated orders from CC marketplace. <code>cc-flink/create_unique_oder.sql</code>, <code>order_per_minute.sql</code>."},{"location":"labs/#110-puzzles-exercises","title":"1.10 Puzzles &amp; Exercises","text":"Sample Path Description SQL puzzles (overview) flink-sql/11-puzzles Highest transaction per day, employees data, table DDLs. Test env: Flink binary or Kubernetes. Compute ETA (shipment) flink-sql/11-puzzles/compute_eta ETA from shipment events: source <code>shipment_events</code>, <code>shipment_history</code> (ARRAY_AGG per shipment), join + UDF <code>estimate_delivery</code>. Confluent Cloud; Python UDF mock in <code>udf/</code>. <code>deploy_flink_statements.py</code>, Makefile. Create tables (tx, loans, employees) flink-sql/11-puzzles <code>create_tx_table.sql</code>, <code>create_loans_table.sql</code>, <code>data/create_employees.sql</code>, <code>highest_tx_per_day.sql</code>."},{"location":"labs/#111-ai-ml-integration","title":"1.11 AI / ML Integration","text":"Sample Path Description AI agents (anomaly detection) flink-sql/12-ai-agents Reefer temperature: Faker source, burst view, <code>ML_DETECT_ANOMALIES</code> (ARMA) in tumbling window. Confluent Flink built-in ML."},{"location":"labs/#112-sql-execution-runtimes","title":"1.12 SQL Execution &amp; Runtimes","text":"Sample Path Description Flink SQL Quarkus flink-sql/flink-sql-quarkus Table API + SQL examples as Quarkus app. Run in IDE or submit JAR to Flink (Docker). Flink Python SQL runner flink-sql/flink-python-sql-runner Python-based SQL runner entry. Triage (DDL only) flink-sql/triage DDLs: <code>ddl.employees.sql</code>, <code>dd.departments.sql</code>, <code>ddl.jobs.sql</code>."},{"location":"labs/#2-flink-java-datastream-api","title":"2. Flink Java (DataStream API)","text":""},{"location":"labs/#21-quickstart-word-count","title":"2.1 Quickstart &amp; Word Count","text":"Sample Path Description DataStream quickstart flink-java/datastream-quickstart Minimal DataStream v2 job: <code>NumberMapJob</code>. Build, submit to Flink cluster, K8s OSS deployment YAML. Word count (file) flink-java/datastream-samples/01-word-count Read file, count word occurrences. FileSource connector."},{"location":"labs/#22-datastream-samples-my-flink","title":"2.2 DataStream Samples (my-flink)","text":"Sample Path Description Word count, filters, joins flink-java/my-flink WordCount (file), PersonFiltering, LeftOuterJoin / RightOuterJoin / FullOuterJoin / InnerJoin (persons + locations). Data in <code>data/</code>. Bank fraud flink-java/my-flink (bankfraud) <code>Bank.java</code>, <code>BankDataServer.java</code>, <code>AlarmedCustomer.java</code>, <code>LostCard.java</code> \u2013 fraud/lost-card style flows. Datastream aggregations flink-java/my-flink (datastream) <code>ComputeAggregates.java</code>, <code>ProfitAverageMR.java</code>, <code>WordCountSocketStreaming.java</code>. Kafka telemetry flink-java/my-flink (kafka) <code>TelemetryAggregate.java</code> \u2013 Kafka source aggregation. Taxi stats flink-java/my-flink (taxi) <code>TaxiStatistics.java</code>; test in <code>TestTaxiStatistics.java</code>. Data: <code>cab_rides.txt</code>, etc. Windows (sales) flink-java/my-flink (windows) <code>TumblingWindowOnSale.java</code>. Sale data server in <code>sale/</code>. Docker / K8s flink-java/my-flink/src/main/docker Dockerfile variants (JVM, legacy-jar, native). <p>Note: Some code uses deprecated DataSet API; Table API ports live under <code>table-api/</code>.</p>"},{"location":"labs/#23-batch-processing-table-api","title":"2.3 Batch Processing &amp; Table API","text":"Sample Path Description Loan batch processing flink-java/loan-batch-processing DataStream: read loan CSV, stats (avg amount, income, counts by status/type), write CSV. Table API: fraud analysis, optional DuckDB JDBC sink. <code>DataStreamJob.java</code>, <code>TableApiJobComplete.java</code>, <code>FraudCountTableApiJob.java</code>. <code>verify_duckdb.sh</code>, <code>query_duckdb.sql</code>."},{"location":"labs/#24-sql-runner-jar","title":"2.4 SQL Runner (JAR)","text":"Sample Path Description SQL runner flink-java/sql-runner Run SQL scripts as Flink job (from Flink OSS examples). JAR with SQL in <code>sql-scripts/</code> (e.g. <code>main.sql</code>, <code>statement-set.sql</code>). Docker and K8s deployment."},{"location":"labs/#3-table-api","title":"3. Table API","text":""},{"location":"labs/#31-java-confluent-cloud","title":"3.1 Java \u2013 Confluent Cloud","text":"Sample Path Description Confluent Cloud Table API table-api/ccf-table-api Confluent examples: catalogs, unbounded tables, transforms, table pipelines, changelogs, integration. <code>Example_00_HelloWorld</code> \u2026 <code>Example_08_IntegrationAndDeployment</code>, <code>TableProgramTemplate.java</code>. JShell init: <code>jshell-init.jsh</code>. Java Table API (Confluent) table-api/java-table-api Join order\u2013customer, deduplication, PersonLocationJoin. Env vars for CC. JShell with pre-initialized TableEnvironment."},{"location":"labs/#32-java-oss-local","title":"3.2 Java \u2013 OSS &amp; Local","text":"Sample Path Description OSS Table API + SQL table-api/oss-table-api-with-sql <code>PersonLocationJoin</code> with persons/locations files. <code>cloud-template.properties</code>. Simplest local Table API table-api/simplest-table-api-for-local In-memory table, filter, print. Session job or Application mode."},{"location":"labs/#33-python-table-api","title":"3.3 Python Table API","text":"Sample Path Description PyFlink table-api/python-table-api <code>first_pyflink.py</code>, <code>word_count.py</code> \u2013 basic Table API in Python."},{"location":"labs/#4-local-end-to-end-demonstrations","title":"4. Local end-to-end demonstrations","text":"<p>All the local demonstrations run on local Kubernetes, some on Confluent Cloud. Most of them are still work in progress.</p> <p>See the e2e-demos folder for a set of available demos based on the Flink local deployment or using Confluent Cloud for Flink.</p>"},{"location":"labs/#41-confluent-cloud-transaction-processing-rds-kafka-flink-iceberg","title":"4.1 Confluent Cloud \u2013 Transaction processing (RDS \u2192 Kafka \u2192 Flink \u2192 Iceberg)","text":"<p>Full Confluent Cloud demo: AWS RDS PostgreSQL \u2192 Debezium CDC \u2192 Kafka \u2192 Flink (dedup, enrichment, sliding-window aggregates) \u2192 TableFlow \u2192 Iceberg/S3, AWS Glue, Athena. ML scoring via ECS/Fargate. Outbox pattern notes. Terraform: IaC (RDS, Confluent, connectors, compute pool), cc-flink-sql/terraform (Flink statements)</p>"},{"location":"labs/#public-repositories-with-valuable-demonstrations","title":"Public repositories with valuable demonstrations","text":"<ul> <li>apache flink 2.x Playground</li> <li>Personal Flink demo repository for Data as a product, ksql to Flink or Spark to Flink automatic migration test sets.</li> <li>Confluent Flink how to</li> <li>Confluent demonstration scene: a lot of Kafka, Connect, and ksqlDB demos</li> <li>Confluent developer SQL training</li> <li>Flink Confluent Cloud for Apache Flink Online Store Workshop, support Confluent enablement Tours.</li> <li>Confluent Quick start with streaming Agents</li> <li> <p>lab 3- Agentic Fleet Management Using Confluent Intelligence</p> </li> <li> <p>Demonstrate Flink SQL test harness tool for Confluent Cloud Flink.</p> </li> <li> <p>Shoes Store Labs  to run demonstrations on Confluent Cloud. </p> </li> <li>Confluent Cloud and Apache Flink in EC2 for Finserv workshop</li> <li>Online Retailer Stream Processing Demo using Confluent for Apache Flink</li> </ul>"},{"location":"labs/#interesting-blogs","title":"Interesting Blogs","text":"<ul> <li>Building Streaming Data Pipelines, Part 1: Data Exploration With Tableflow</li> <li>Building Streaming Data Pipelines, Part 2: Data Processing and Enrichment With SQL</li> </ul>"},{"location":"labs/#other-related-assets","title":"Other related assets","text":"<ul> <li>Shift Left End to End Demonstration</li> </ul>"},{"location":"methodology/coe/","title":"Center of Excellence for Stream Processing Establishment","text":"<p>CoE is a centralized team or a virtual organization whose mission is to establish and disseminate best practices, provide expert consultation, and accelerate the adoption and successful, standardized deployment for real-time stream processing applications.</p>"},{"location":"methodology/coe/#goals","title":"Goals","text":"<p>Data Streaming CoE needs to address architecture, high availability, DR, security, multi-tenancy, streaming applications deployment best practices, business transformation with streaming capabilities. </p> <p>In particular the following items need to be addresses and built:</p> <ol> <li>Goal and Responsibilities: of the CoE team, or community of practices team.</li> <li>Roles and Responsibilities: to outline the roles to run streaming applications and platform. </li> <li>Governance model: to define the decisions to take when building, maintaining, and growing of the streaming platform.  Should include Multi-tenancy, how shared services are charged back to different lines of business.</li> <li>Architectural Patterns: To define reference architectures for typical use cases, addressing different DR patterns base on RPO and RTO.</li> <li>Access control: To address how to manage user and group access, to data sources, and Flink applications. </li> <li>Development best practices: Recommend practices to move to a streaming platform. </li> <li>Operations best practices: Reference runbooks to maintain and manage the specific implementation of a streaming data platform. Includes Application savepoints, checkpoints, blue/green deployment, failover, troubleshooting.</li> <li>Sizing: Deliver tools and knowledge to size their platform for growth. </li> <li>Project plan references: to highlight what are the development and deployment high level activities for a streaming project. </li> <li>Community development: to define the communications, collaboration, and engagement strategies to increase awareness and adoption of stream processing. </li> </ol>"},{"location":"methodology/coe/#user-roles","title":"User Roles","text":"<p>There will be light modification of roles when using a managed services, versus managing the platform.</p>"},{"location":"methodology/coe/#enablement","title":"Enablement","text":""},{"location":"methodology/coe/#enablement-for-flink-data-engineers","title":"Enablement for Flink Data Engineers","text":""},{"location":"methodology/coe/#enablement-for-sres","title":"Enablement for SREs","text":""},{"location":"methodology/data_as_a_product/","title":"Moving to a Data as a Product Architecture","text":"Version <p>Created 01/2025 Update 12/2025</p> <p>This chapter provides a practical overview of current data lake and lakehouse challenges, discusses the implementation of 'data as a product' principles, and demonstrates how real-time streaming can be effectively integrated into modern data architectures.</p>"},{"location":"methodology/data_as_a_product/#context","title":"Context","text":""},{"location":"methodology/data_as_a_product/#operational-data-and-analytical-data","title":"Operational Data and Analytical data","text":"<p>The classical data landscape is split between operational data, which powers real-time applications, and analytical data, which provides historical insights for decision-making and machine learning. This separation has created complex and fragile data architectures, marked by problematic ETL processes and intricate data pipelines. The challenge lies in effectively bridging these two distinct data planes to ensure seamless data flow and integration.</p> Two data planes: real-time applications, and analytical data <p>The initial data platform architecture comprised a database on one side and a data warehouse on the other, with ETL jobs facilitating data movement between them. This setup can lead to bottlenecks, especially when different teams are working on various parts of an application but all relying on the same data source. It might also complicate scalability and flexibility.</p> <p>To address scaling challenges and support unstructured data, the second generation of data platforms, emerging in the mid-2000s, adopted distributed object storage, leading to the development of the Data Lake.</p> <p>The medallion architecture, a three-layered approach, is a common framework for organizing data lakes. This structure, as illustrated in the figure below, is driven by several key motivations:</p> Medallion Architecture <ul> <li>Leveraging cloud object storage to accommodate large volumes of both structured and unstructured data.</li> <li>Implementing data pipelines to transform data progressively, from raw landing zones to business-level aggregates.</li> <li>Facilitating data management and governance through data cataloging and distributed query tools.</li> <li>Organizing data based on its transformation stage, rather than business domains or specific use cases.</li> </ul> <p>Data product and its extension with Data Mesh helps to restructure those two planes with a domain and use case centric approach, and not a technology stack.</p>"},{"location":"methodology/data_as_a_product/#current-challenges","title":"Current Challenges","text":"<p>In Lakehouse or data lake architecture: </p> <ul> <li>We observe complex ETL jobs landscape, with high failure rate.</li> <li>Not all data needs the three layers architecture, but a more service contract type of data usage. Data becoming a product like a microservice.</li> <li>There is a latency issue to get the data, we talk about T + 1 to get fresh data. The + 1 can be one day or one hour, but it has latency that may not what business requirements need.</li> <li>Simple transformations need to be done with the ETL or ELT tool with the predefined staging. Not specific use-case driven implementation of the data retrieval and processing. </li> <li>Data are pulled from their sources and between layers. It could be micro-batches, or long-running batches. At the bronze layer, the data are duplicated, and there is minimum of quality control done.</li> <li>In the silver layer the filtering and transformations are also generic with no specific business context.</li> <li>The gold layer includes all data of all use cases. This is where most of the work is done for data preparation and develop higher quality level. This is the layer with a lot of demands from end-user and continuous update and new aggregation developments. </li> <li>This is the final consumer of the data lake gold layer that are pulling the data with specific Service Level Objectives. </li> <li>Data created at the gold level, most likely needs to be reingected to the operational databases to be visible to operation applications. This introduces the concept of reverse ETL. </li> <li>Each layer may have dfferent actors responsible to process the data: data platform engineer, analytic engineers and data modelers, and at the application level, the application developers.</li> <li>Storing multiple copies of data across layers inflates cloud storage expenses. Data become quickly stale and unreliable.</li> <li>Constant movement of data through layers results in unnecessary processing and query inefficiencies.</li> <li>The operational estate is also continuously growing, by adding mobile applications, serverless functions, cloud native apps, etc...</li> </ul>"},{"location":"methodology/data_as_a_product/#core-principles-for-data-mesh","title":"Core Principles for Data Mesh","text":"<p>To address the concerns of siloed and incompatible data, while addressing scaling to constant change of data landscape, adding more data source and consumers, adding more transformations and processing resources, the data mesh is based on four core principles:</p> <ol> <li> <p>Domain-oriented decentralized data ownership and architecture. The components are the analytical data, the metadata and the computer resources to serve it. Data ownership is linked to the DDD bounded context. For a product management use case, the bounded context of a <code>Product</code>, supports operational APIs and analytical data endpoints to address active users, feature usage, and conversion rates, for example: </p> <p> Data as a product  - Bounded context </p> <p>Also multiple bounded contexts could be presented via their dependencies to other domain operational and analytical data endpoints.</p> </li> <li> <p>Data as a product, includes clear scope definition, product ownership and metrics to ensure data quality, user acceptance, lead time for data consumption. Data as a product includes documenting the users, how they access the data, and for what kind of operations. The accountability of the data quality shifts to the source of the data. It encapsulates three structural components: 1/ code (data pipelines, schema definitions, APIs, event processing, monitoring metrics, access control), 2/ data and metadata in a polyglot form (events, REST, tables, graphs, batch files...), 3/ infrastructure (to run code, store data and metadata).</p> <p> Data as a product: component view </p> </li> <li> <p>Self-serve data infrastructure as a platform, to enable domain autonomy, as microservices are defined and orchestrated. It includes callable polyglot data storage, data products schema, data pipeline declaration and orchestration, data products lineage, compute and data locality. The capabilities includes 1/ infrastructure provisioning via code for storage, service accounts, access policies, server provisioning for running code and jobs, 2/ data product interface, declarative interfaces to manage the life cycle of a data product, 3/ supervision plane to present the relation between data products, support discovery, build data catalog, to execute semantic query.</p> <p> Data as a product: infrastructure platform </p> </li> <li> <p>Federated governance to address interoperability of the data products. This needs to support decentralized and domain self-sovereignty, interoperability through standardization. </p> </li> </ol> Moving to Kafka and real-time processing is not the full story <p>Changing the batch pipeline processing technologies to real-time processing using the medallion architecture does not solve the previously mentionned problems. We still need to shift paradign and adopt a data as a product centric architecture. The following diagram illustrates the mediallon layers, done with Flink processing and Kafka topics for storage.</p> <p> Real-time intgration </p> <p>Using topics as data record storage and Flink statements for transforming, filtering and enriching to the silver layer, also using kafka topics is the same ETL approach but with different technologies. </p> <p>Another, more detailed view, using Kafka Connectors will look like in the diagram below, where the three layers are using the Kimdall practices of source processing, intermediates and sinks.</p> <p> Generic source to sink pipeline </p> <p>Even if append-logs are part of the data as a product architecture, there are more to address and to organize the component development.</p>"},{"location":"methodology/data_as_a_product/#a-data-product-approach","title":"A Data Product Approach","text":"<p>As seen previously, domains need to host and serve their domain datasets in an easily consumable way, rather than flowing the data from domains into a centrally owned data lake or platform. Dataset from one domain may be consumed by another domains in a format suitable for its own application. Consumer pulls the dataset.</p> Data product reused by other domains <p>So developing data as a product means shifting from push and ingest of ETL and ELT processes to serving and pull model across all domains. </p>"},{"location":"methodology/data_as_a_product/#data-as-a-product","title":"Data as a Product","text":"<p>Data products serve analytical data, they are self-contained, deployable, valuable and exhibit eight characteristics:</p> <ul> <li>Discoverable: data consumers can easily find the data product for their use case.  A common implementation is to have a registry, a data catalogue, of all available data products with their meta information. Domain data products need to register themselves to the catalog.</li> <li>Addressable: with a unique address accessible programmatically. This implies to define naming convention and may be SDK code.</li> <li>Self describable: Clear description of the purpose and usage patterns as well as the semantics and syntax. The schema definition and registry are used for that purpose. </li> <li>Trustworthy: clear definition of the Service Level Objectives and Service Level Indicators conformance. </li> <li>Native access: adapt the data access interface to the consumer: APIs, events, SQL views, reports, widgets</li> <li>Composable: integrate with other data products, for joining, filtering and aggregation. Nedd to define standards for field type formatting, identifying polysemes across different domains, datasets address conventions, common metadata fields, event formats such as CloudEvents. Federated identity may also being used to keep unique identifier cross domain for a business entity.</li> <li>Valuable: represent a cohesive concept within its domain. Sourced from unstructured, semi-structured and structured data. To maximize value within a data mesh, data products should have narrow, specific definitions, enabling reusable blueprints and efficient management.</li> <li>Secure: with access control rules and enforcement, and single sign on capability.</li> </ul> <p>To support the implementation of those characteristics, it is relevant to name a domain data product owner, who is also responsible to measure data quality, the decreased lead time of data consumption, and the data user satisfaction, or net promoter score. The most important questions a product owner should be able to answer are:</p> <ol> <li>Who are the data users?</li> <li>How do they use the data?</li> <li>What are the native methods that they are comfortable with to consume the data?</li> </ol> <p>Data products are not data applications, data warehouses, PDF reports, dashboards, tables (without proper metadata), or kafka topics. The data products may, and should be shared using streams, to be able to replay from sources of events and scale the consumption. </p>"},{"location":"methodology/data_as_a_product/#elements-of-a-data-product","title":"Elements of a Data Product","text":"<p>The following elements are part of a data product owner to develop and manage, with application developers:</p> <ul> <li>Metadata of what the data product is, human readable, parseable for tool to build and deploy data product to orchestration layer. This includes using naming convention, and polyglot definition. </li> <li>API definition for request-response consumptions</li> <li>Event model definition for asynch consumptions</li> <li>Storage definition, service account, roles and access policies</li> <li>Table definitions</li> <li>Flink statement definitions for deduplication, enrichment, aggregation, and deployment definitions</li> <li>Microservice code implementation, packaging and deployment definitions</li> </ul> <p>All those elements can be defined as code in a git repository or between a gitops repo and a code repository. It is recommended to keep one bounded context per repository.</p> Data lake, lakehouse and data warehouse <p>Data lake is no more a central piece of the architecture with complex pipelines, they are becoming a node in the data mesh, to expose a dataset. It may not be used as the source of truth is becoming the immutable distributes logs and storage that holds the dataset available for replayability. Datawarehouse for business intelligence is also a node, and consumer of the data product.</p>"},{"location":"methodology/data_as_a_product/#data-contracts-for-streaming-products","title":"Data Contracts for Streaming Products","text":"<p>In a Data Mesh, the \"Product\" is defined by its interface. The industry is moving toward Data Contracts as a formal mechanism to define the agreement between a Flink producer and downstream consumers.</p> <p>A Data Contract goes beyond simple schema definition to include:</p> <ul> <li>Schema Definition: Using Protobuf or Avro with the Confluent Schema Registry as the enforcement mechanism. The schema registry provides version management and compatibility checking.</li> <li>Watermark Alignment Expectations: The contract should specify the expected watermark strategy (e.g., bounded out-of-orderness with a maximum delay of 5 seconds).</li> <li>Lateness Tolerance: Define how late-arriving data is handled. This includes specifying the allowed lateness window and what happens to events that arrive after the window closes.</li> <li>Data Quality Guarantees: Expected completeness, accuracy thresholds, and error handling behavior.</li> </ul> <p>Example contract specification elements for a Flink streaming product:</p> <pre><code>contract:\n  name: customer-events-v1\n  schema:\n    format: avro\n    registry: schema-registry.example.com\n    subject: customer-events-value\n    compatibility: BACKWARD\n  streaming:\n    watermark:\n      strategy: bounded-out-of-orderness\n      max_delay_seconds: 5\n    lateness:\n      allowed_lateness_seconds: 60\n      late_data_handling: side_output\n  slo:\n    freshness_seconds: 10\n    completeness_percent: 99.5\n</code></pre> <p>The contract becomes the primary interface documentation, enabling consumers to understand not just what data they receive, but when and how reliably they can expect to receive it.</p>"},{"location":"methodology/data_as_a_product/#dual-nature-storage-streaming-and-batch","title":"Dual-Nature Storage: Streaming and Batch","text":"<p>Many analytical consumers still require SQL/Batch access while real-time applications need live streams. A Flink-based data product can simultaneously exist as a \"Live Stream\" (Kafka) and a \"Historical Table\" (Iceberg/Lakehouse).</p> <p>Apache Iceberg and Confluent TableFlow serve as bridges between these two access patterns:</p> <ul> <li>Flink SQL can write to a unified table that serves both real-time alerts and long-term BI queries.</li> <li>The same data product exposes multiple access interfaces: a Kafka topic for streaming consumers and an Iceberg table for batch analytics.</li> </ul> <pre><code>-- Flink SQL writing to both Kafka and Iceberg\nINSERT INTO iceberg_catalog.db.customer_events\nSELECT * FROM kafka_customer_events;\n\n-- The Iceberg table is queryable by Spark, Trino, or other batch engines\n-- while Kafka consumers receive the same events in real-time\n</code></pre> <p>This dual-nature approach fulfills the \"Polyglot\" requirement of a data product, allowing each consumer to access data in their preferred format without requiring separate data pipelines.</p> TableFlow for Unified Access <p>Confluent TableFlow provides automatic materialization of Kafka topics into Iceberg tables, maintaining consistency between the streaming and batch representations. This reduces the operational burden of maintaining separate pipelines for different access patterns.</p>"},{"location":"methodology/data_as_a_product/#methodology","title":"Methodology","text":"<p>Defining, designing and implementing data products follow the same principles as other software development and should start by the end goal and use case. This should solidify clear product objectives. Domain discovery is part of the DDD methodology, and in the data product a domain may be more oriented to source and some to consumers. But use cases and what needs to be created as analytical data should be the main goals of the design and implementation activities. Source domain datasets represent the facts of the business. The source domain datasets capture the data that is mapped very closely to what the operational systems of their origin, generate.</p> <p>Consumer domain datasets, on the other hand, are built to serve a tightly coupled group of use cases. Distinct from source domain datasets, they undergo more structural modifications as they process source domain events into aggregated formats optimized for a specific access model.</p>"},{"location":"methodology/data_as_a_product/#formalize-the-use-cases-user-stories","title":"Formalize the use cases / user stories","text":"<p>The following table illustrates some use cases:</p> User Story Data as a Product As a marketing strategist, I need to provide predictive churn scores and customer segmentation based on behavior and demographics. This will allow me to proactively target at-risk customers with personalized retention campaigns and optimize marketing spend. <ul><li>Churn probability scores for each customer.</li><li>Customer segments based on churn risk and value.</li> <li>Key factors influencing churn.</li></ul> As a product manager, I need to visualize key product usage metrics and performance indicators. This will enable me to monitor product adoption, identify usage patterns, and make data-driven decisions for product improvements. <ul><li>Active users, feature usage, and conversion rates.</li><li>Historical trends and comparisons of product performance.</li><li>Breakdowns of product usage by customer segment</li><li>Alerts for anomalies or significant changes in product usage</li></ul> As a supply chain manager, I need to get real-time visibility into inventory levels, supplier performance, and delivery timelines. This will help me proactively identify potential disruptions, optimize inventory management, and ensure timely product delivery. <ul><li>Real-time inventory levels across all warehouses.</li><li>Supplier performance metrics, such as on-time delivery rates and quality scores.</li><li>Predictive alerts for potential stockouts or delivery delays.</li><li>Visualizations of delivery routes and timelines.</li><li>Historical data that can be used to perform trend analysis, and find bottlenecks.</li></ul> As a Consultant Director, I need to be able to continuously access a holistic view of each consultant, including their skill level, matching resume, current training and skill levels, and certification status, so that I can effectively staff projects, identify skill gaps, plan professional development, and ensure compliance. <ul><li>Aggregated and real-time data on consultant skill levels</li> <li>Matching resumes (potentially key skills extracted)</li><li>Current training completions and skill levels derived from training</li><li>Certification statuses</li><li>Visualization of skill gaps by practice area or project type</li></ul> <p>Using a classical system context diagram for the supply chain management use case, we may define the high level view of a data product as:</p> Data as a product: system context view <p>The skill analysis use case may define the following data product:</p> <ul> <li> <p>Certification Compliance Tracker:</p> <ul> <li>Data: Real-time status of all consultant certifications, including expiration dates and renewal progress.</li> <li>Value Proposition: Ensures the organization maintains necessary certifications for compliance and client engagements, mitigating potential risks and penalties.</li> <li>Potential Features: Automated alerts for upcoming expirations; reporting on certification coverage by practice area or client; integration with certification management platforms.</li> </ul> </li> </ul>"},{"location":"methodology/data_as_a_product/#using-bounded-context","title":"Using Bounded Context","text":"<p>Data as a product is designed with a domain-driven model combined with analytical and operational use cases. </p> <p>The methodology to define data product may be compared to the event-driven microservice adoption. Business, operational application, manages their aggregates but also are responsible to publish the business events, as facts, to share their datasets. Aggregation processing is considered as a service pushing data product to other consumers. The aggregate models make specific queries on other data products and serve the results with SLOs.</p> <p>The design starts by the user input, which are part of a business domain and bounded context. The data may be represented as DDD aggregate with a semantic model. Entities and Value objects are represented to assess the need to reuse other data product and potentially assess the need for anti-corruption layer. </p> What should be part of a bounded context for data as a product <p>A Bounded Context should encapsulate everything needed to model and implement a specific business capability or set of related capabilities. This typically includes:</p> <ul> <li>Entities: Domain objects with identity that persist over time and represent core concepts of the subdomain. Their behavior and attributes are specific to this context.</li> <li>Value Objects: Immutable objects that describe characteristics of entities. Their meaning is specific to the context.</li> <li>Aggregates: Clusters of related entities and value objects that are treated as a single unit for data changes. One entity within the aggregate serves as the root and is responsible for maintaining the consistency of the entire aggregate. Transactions should operate on aggregates.</li> <li>Domain Services: Operations that don't naturally belong to an entity or value object but are still part of the domain logic within this context. They often involve interactions between multiple aggregates or external systems.</li> <li>Domain Events: Significant occurrences within the domain that the business cares about. They are immutable records of something that has happened and can trigger actions within the same or other bounded contexts.</li> <li>Repositories: Interfaces for persisting and retrieving aggregates within the bounded context. The actual implementation of the repository might use a specific database technology.</li> <li>Factories: Objects responsible for creating complex domain objects, often encapsulating complex instantiation logic.</li> <li>Use Cases/Application Services: (Sometimes considered outside the core domain but within the Bounded Context) These orchestrate interactions between domain objects to fulfill specific user requests or system behaviors. They reside at the application layer and interact with repositories and domain services.</li> <li>Data Transfer Objects (DTOs): Objects used to transfer data across boundaries (e.g., between layers or bounded contexts). Their structure is often optimized for transport rather than representing the domain model directly.</li> <li>Infrastructure Concerns: Code related to persistence, messaging, external service integrations, and UI specific to this bounded context.</li> </ul> <p>In data products, DDD bounded context, translates to defining clear boundaries for the data products, ensuring each product serves a specific business domain. A <code>customer data product</code> and a <code>product inventory data product</code> would be distinct bounded contexts, each with its own data model and terminology.</p> <p>Data pushed to higher consumer are part of the semantic model, and of the event-driven design. Analytics Engineers and Data Modellers building aggregate Data Products know exactly what to collect and what quality metrics to serve. The aggregation may use lean-pull mechanism, focusing on their use case needs only. The data is becoming a product as close to the operational source, so shifting the processing to the left of the architecture. This Shift-Left approach where quality controls, validation, and governance mechanisms are embedded as early in the data lineage map as possible. The consumption patterns are designed as part of the data product, and may include APIs, events, real-time streams or even scheduled batch. </p> <p>Source data domains need to make easily consumable historical snapshots of their datasets available, not just timed events. These snapshots should be aggregated based on a time frame that matches the typical rate of change within their domain.</p> <p>Even though domains now own their datasets instead of a central platform, the essential tasks of cleansing, preparing, aggregating, and serving data persist, as does the use of data pipelines, which are now integrated into domain logic.</p> Data product within bounded context <p>Each domain dataset must establish Service Level Objectives for the quality of the data it provides: timeliness, error rates...</p> <p>Moving from technical data delivery to product thinking requires changes in how organizations approach data management. The data product is decomposed of real-time events exposed on event streams, and aggregated analytical data exposed as serialized files on an object store.</p> <p>New requirements are added to the context of the source semantic model.</p>"},{"location":"methodology/data_as_a_product/#motivations-for-moving-to-data-stream-processing","title":"Motivations for Moving to Data Stream Processing","text":"<p>The Data integration adoption is evolving with new needs to act on real-time data and reduce batch processing cost and complexity. The following table illustrates the pros and cons of data integration practices for two axes: time to insights and data integity</p> Time to insights Data integrity Low High High Lakehouse or ELT: <ul><li>+ Self-service</li><li>- Runaway cost</li><li>- No knowledge of data lost</li><li>- complext data governance</li><li>- data silos.</li></ul> Data Stream Platform: <ul><li>+ RT decision making</li><li>+ Operation and analytics on same platform</li><li>+ Single source of truth</li><li>+ Reduced TCO</li><li>+ Governance</li></ul> Low Hand coding: <ul><li>+ customized solution specific to needs.</li><li>- Slow</li><li>- difficult to scale</li><li>- opaque</li><li>- challenging governance.</li></ul> ETL:<ul><li>+ Rigorous</li><li>+ data model design</li><li>+ governed</li><li>+ reliable</li><li>- Slow</li><li>- Point to point</li><li>- Difficult to scale.</li></ul>"},{"location":"methodology/data_as_a_product/#assessment-questions","title":"Assessment questions","text":"<p>Try to get an understanding of the data integration requirements by looking at:</p> <ul> <li>Current data systems and data producers to a messaging system like Kafka</li> <li>Development time to develop new streaming logic or ETL job</li> <li>What are the different data landing zones and for what purpose. Review zone ownership.</li> <li>Level of Lakehouse adoption and governance, which technology used (Iceberg?)</li> <li>Is there a data loop back from the data lake to the OLTP?</li> <li>Where data cleaning is done?</li> <li>Is there any micro-batching jobs currently done, at which frequency, for which consuners?</li> <li>What data governance used?</li> <li>How data quality control is done?</li> </ul>"},{"location":"methodology/data_as_a_product/#migration-context","title":"Migration Context","text":"<p>A direct \"lift and shift\" approach\u2014where batch SQL scripts are converted to Flink statements on a one-to-one basis\u2014 may not be recommended. Refactoring is essential, as SQL processing often differs significantly when  addressing complex and stateful operators, such as joins.</p> <p>Most of the filtering and selection scripts can be ported 1 to 1. While most stateful processing needs to be refactorized and deeply adapted to better manage states and complexity.</p> <p>There is a repository with tools to process existing Spark/dbt project to find dependencies between tables, use local LLM to do some migration, and create Flink query pipelines per sink table. </p>"},{"location":"methodology/data_as_a_product/#measuring-data-product-slos-in-flink","title":"Measuring Data Product SLOs in Flink","text":"<p>The \"Trustworthy\" and \"Reliable\" pillars of Data Mesh require measurable Service Level Objectives (SLOs). A Data Product Owner should monitor specific metrics for each Flink job that powers their data product.</p>"},{"location":"methodology/data_as_a_product/#freshness","title":"Freshness","text":"<p>Freshness measures how current the data is. In Flink, this is tracked by comparing the <code>currentEmitWatermark</code> against the wall clock time.</p> <ul> <li>Metric: <code>currentProcessingTime - currentEmitWatermark</code></li> <li>Target: Define acceptable lag (e.g., &lt; 30 seconds for near-real-time products)</li> <li>Monitoring: Expose via Flink metrics and alert when the gap exceeds thresholds</li> </ul> <pre><code>-- Flink SQL hint for watermark configuration\nCREATE TABLE orders (\n    order_id STRING,\n    order_time TIMESTAMP(3),\n    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND\n) WITH (...);\n</code></pre>"},{"location":"methodology/data_as_a_product/#completeness","title":"Completeness","text":"<p>Completeness addresses how the product handles late-arriving data. Flink provides mechanisms to track and report on data that arrives after the allowed lateness window.</p> <ul> <li>Side Outputs for Late Data: Configure side outputs to capture late events rather than silently dropping them. These become part of the product's quality report.</li> <li>Late Data Metrics: Track the percentage of events arriving late and the distribution of lateness.</li> </ul> <pre><code>// Java DataStream API - capturing late data\nOutputTag&lt;Event&gt; lateOutputTag = new OutputTag&lt;Event&gt;(\"late-data\"){};\n\nSingleOutputStreamOperator&lt;Result&gt; result = stream\n    .keyBy(Event::getKey)\n    .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n    .allowedLateness(Time.minutes(1))\n    .sideOutputLateData(lateOutputTag)\n    .process(new MyWindowFunction());\n\n// Late data stream for quality reporting\nDataStream&lt;Event&gt; lateStream = result.getSideOutput(lateOutputTag);\n</code></pre> <p>See Flink side output mecanism and Late events.</p>"},{"location":"methodology/data_as_a_product/#cost-per-product-finops","title":"Cost-per-Product (FinOps)","text":"<p>Resource tagging enables \"Billback\" or \"Showback\" for specific data products.</p> <ul> <li>Resource Labels: Tag Flink jobs, Kafka topics, and storage with product identifiers.</li> <li>Compute Metrics: Track CPU, memory, and checkpoint storage per product.</li> <li>Cost Allocation: Map infrastructure costs to business domains based on resource consumption.</li> </ul> Metric Description Target Freshness Lag Watermark delay vs wall clock &lt; 30s Completeness Percentage of on-time events &gt; 99.5% Late Data Rate Events arriving after allowed lateness &lt; 0.1% Cost per Million Events Infrastructure cost normalized by throughput Varies by product"},{"location":"methodology/data_as_a_product/#data-discovery-via-flink-catalogs","title":"Data Discovery via Flink Catalogs","text":"<p>One of the biggest hurdles in Data Mesh is Discoverability. If a Flink job creates a view or a table, users in other domains need a mechanism to find it.</p>"},{"location":"methodology/data_as_a_product/#centralized-catalogs","title":"Centralized Catalogs","text":"<p>The Flink Catalog serves as the registration point for data products:</p> <ul> <li>Hive Catalog: Traditional metadata store, widely supported across batch and streaming engines.</li> <li>AWS Glue Catalog: Managed catalog service integrated with AWS analytics ecosystem.</li> <li>DataHub Integration: Connect Flink metadata to a company-wide data discovery portal.</li> </ul> <pre><code>-- Register a catalog in Flink OSS SQL\nCREATE CATALOG iceberg_catalog WITH (\n    'type' = 'iceberg',\n    'catalog-type' = 'hive',\n    'uri' = 'thrift://hive-metastore:9083',\n    'warehouse' = 's3://data-lake/warehouse'\n);\n\n-- Create a table that becomes discoverable\nCREATE TABLE iceberg_catalog.sales.daily_revenue (\n    date_key DATE,\n    region STRING,\n    total_revenue DECIMAL(18,2)\n) WITH (...);\n</code></pre>"},{"location":"methodology/data_as_a_product/#making-products-discoverable","title":"Making Products Discoverable","text":"<p>\"Data as a Product\" means registering Flink SQL metadata so it is searchable in a company-wide data portal, not hidden in JAR files or deployment scripts.</p> <ul> <li>Automated Registration: CI/CD pipelines should register table metadata to the catalog upon deployment.</li> <li>Rich Metadata: Include descriptions, ownership, SLOs, and lineage information.</li> <li>Search and Browse: Users can find products by domain, data type, or semantic meaning.</li> </ul>"},{"location":"methodology/data_as_a_product/#versioning-and-schema-evolution","title":"Versioning and Schema Evolution","text":"<p>Data products must remain \"Secure\" and \"Interoperable\" even when they change. Versioning a streaming product presents unique challenges. (see query evolution in cookbook chapter).</p>"},{"location":"methodology/data_as_a_product/#state-compatibility-challenges","title":"State Compatibility Challenges","text":"<p>When the data product logic changes (e.g., a new calculation for \"Churn Probability\"), the Flink job's state must be considered:</p> <ul> <li>Savepoint Compatibility: Changes to state schema may require state migration or fresh start.</li> <li>Versioned Output Topics: Use versioned Kafka topics (e.g., <code>customer-churn-v2</code>) to avoid breaking downstream consumers.</li> <li>Blue-Green Deployment: Run both old and new versions in parallel during transition periods.</li> </ul> <pre><code># Blue-Green deployment pattern\n# 1. Deploy new version reading from same source, writing to new topic\nflink run -s savepoint-path job-v2.jar --output-topic customer-churn-v2\n\n# 2. Migrate consumers to v2 topic\n# 3. Deprecate and eventually stop v1 job\n</code></pre> <p>See shift_left tool to support blue/green deployment</p>"},{"location":"methodology/data_as_a_product/#migration-strategies","title":"Migration Strategies","text":"Strategy Use Case Trade-offs In-place upgrade Compatible schema changes Requires savepoint compatibility Blue-Green Breaking changes Double resource cost during transition Versioned topics Major logic changes Consumers must migrate Parallel processing Gradual rollout Increased complexity"},{"location":"methodology/data_as_a_product/#some-implementation-challenges","title":"Some implementation challenges","text":""},{"location":"methodology/data_as_a_product/#git-project-organization","title":"Git project organization","text":"<p>For closely related Bounded Contexts within the same application we can have a repository with different folders per bounded context.</p> <p>The internal structure of each Bounded Context folder typically follows a layered or modular architecture:</p> <ul> <li>domain/: Contains the core domain logic: entities, value objects, aggregates, domain services, and domain events. This layer should be independent of any infrastructure concerns.</li> <li>application/: Contains use cases or application services that orchestrate interactions with the domain layer to fulfill specific business requirements. It often handles transactions and authorization.</li> <li> <p>infrastructure/: Contains the implementation details for interacting with the outside world:</p> <ul> <li>persistence/: Repository implementations using specific database technologies.</li> <li>messaging/: Implementations for sending and receiving domain events or commands using message queues or kafka topics.</li> <li>external-services/: Clients for interacting with other systems or APIs.</li> </ul> </li> <li> <p>interfaces/ (or api/, web/): Contains the entry points to the Bounded Context, such as REST API controllers, GraphQL resolvers, or UI-specific code. It's responsible for request handling and response formatting, often using DTOs to translate between the interface and the application layer.</p> </li> <li>tests/: Contains unit tests, integration tests, and potentially end-to-end tests for the Bounded Context.</li> <li>shared/ (within a Bounded Context): Might contain utilities or helper classes specific to this Bounded Context.</li> </ul> <p>This structure can be in <code>src/main/java</code> for Java project, or src for python/ Fast API project.</p>"},{"location":"methodology/data_as_a_product/#joins-considerations","title":"Joins considerations","text":"<p>The SQL, LEFT JOIN, joins records that match and don\u2019t match on the condition specified. For non matching record the left columns are populated with NULL. SQL supports LEFT ANTI JOIN, but not Flink. So one solution in Flink SQL is to use a null filter on the left join condition:</p> <pre><code>from table_left\nleft join table_right\n    on table_left.column_used_for_join = table_right.column_used_for_join\n    where table_right.column_used_for_join is NULL;\n</code></pre>"},{"location":"methodology/data_as_a_product/#state-management-as-a-product-lifecycle-challenge","title":"State Management as a Product Lifecycle Challenge","text":"<p>In a data product, the Flink state (the checkpoint and savepoint) is part of the product's technical debt and asset. \"Owning the data\" also means \"owning the state.\"</p> <p>State as a Product Asset:</p> <ul> <li>Savepoints are Data: Savepoints contain aggregated values, window contents, and keyed state that may represent hours or days of processing. They are not disposable artifacts.</li> <li>Recovery Dependency: The ability to recover a data product depends on savepoint availability and compatibility.</li> <li>Migration Complexity: State schema evolution requires careful planning and testing.</li> </ul> <p>Operational Considerations:</p> <ul> <li>Savepoint Retention Policy: Define how long savepoints are retained and archived. Consider compliance and recovery requirements.</li> <li>State Size Monitoring: Large state can impact checkpoint duration and recovery time. Track state size as a product metric.</li> <li>Compatibility Testing: Before upgrading job logic, validate that the new code can restore from existing savepoints.</li> </ul> <pre><code># Taking a savepoint before upgrade\nflink savepoint &lt;job-id&gt; s3://savepoints/product-name/\n\n# Validating state compatibility (conceptual)\nflink run --dry-run --restore-from savepoint-path new-job.jar\n</code></pre> <p>State Ownership Questions:</p> <ol> <li>Who is responsible for savepoint storage and retention?</li> <li>What is the recovery point objective (RPO) for this data product?</li> <li>How is state migration tested before production deployment?</li> <li>What is the fallback strategy if state restoration fails?</li> </ol> <p>Including state management in the data product definition ensures that operational concerns are addressed alongside functional requirements.</p>"},{"location":"methodology/data_as_a_product/#source-of-information-go-deeper","title":"Source of information - go deeper","text":"<ul> <li>Martin Fowler - Designing Data Product</li> <li>Data Mesh Principals - Zhamak Dehghani</li> <li>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh - Zhamak Dehghani</li> <li>Confluent Blog - Data Products, Data Contracts, and Change Data Capture - Adam Bellemare</li> </ul>"},{"location":"techno/cc-flink-demo/","title":"A Full Demo Scriptto present Confluent Flink Serverless Offering","text":""},{"location":"techno/cc-flink-demo/#environments","title":"Environments","text":""},{"location":"techno/cc-flink-demo/#kafka-cluster","title":"Kafka Cluster","text":""},{"location":"techno/cc-flink-demo/#topics","title":"Topics","text":""},{"location":"techno/cc-flink-demo/#connectors","title":"Connectors","text":""},{"location":"techno/cc-flink-demo/#provider-integrations","title":"Provider Integrations","text":""},{"location":"techno/cc-tableflow/","title":"Confluent Tableflow","text":"<p>This chapter is based to public knowledge, product documentation, and customer engagements experiences. </p> <p>The first level of information is the product blog, and the main product page as well as the product documentation for Confluent Cloud.</p> <p>This video deep dive into the technology</p>"},{"location":"techno/cc-tableflow/#goals","title":"Goals","text":"<p>TableFlow allows to represent a Kafka topic and associated schema as a table in Apache Iceberg or Delta Lake format. It becomes the mediation layer between operational data and analytical data zone. It is using the schema registry to get the schema definition of the table.  It addresses a unified storage view on top of object storage.</p> <p>Kafka topic is the source of truth of the data. Tableflow supports the open table format: a table and catalog for analytics. It is part of the data as a product architecture.</p> <p>For Data engineers in data lakehouse environment, kafka topic is seen as table.</p> open table format <p>Open table formats are an open-source technology for storing tabular data that builds on top of existing file formats like Parquet or CSV files. It adresses the needs for query performance and reliability of data lake tables, by adding metadata on top of the tabular data. It was developed to bring ACID guarantees, on write operations.</p>"},{"location":"techno/cc-tableflow/#pains","title":"Pains","text":"<p>The classical high level view to move data from Kafka topics to lake house often rely on complex ETL pipelines, manual data wrangling, and custom governance processes. It is error prone work to map each one into an Iceberg table by hand.</p> <p></p> <ul> <li>At the ingestion layer the type conversion, schematization, synchronize metadata to catalog, perform tables management</li> <li>The bronze landing zone will have raw tables with Iceberg Metadata.</li> <li>At the data preparation layer the ELT batch processing addresses deduplication, business metric creations, enforcing business rules and constraints.</li> <li>A lot of infrastructure to manage to consume the data out of Kafka, use custom program to transform to Iceberg and Parquet tables.</li> </ul>"},{"location":"techno/cc-tableflow/#value-propositions","title":"Value Propositions","text":"<ul> <li>It is a Cloud service, per region.</li> <li>The data from the topic is moved to object storage in parquet format with Iceberg metadata. </li> <li>Need to bring your own object storage (s3 bucket) or use Confluent Cloud internal storage which is on top of Object Storage</li> <li>Work with private network, using gateway private endpoints to S3. No traffic over public internet.</li> <li>Data refreshness is down to 15mn, default 6 hours. For higher need, it can read from broker directly, at the minute level.</li> <li>Start from the earliest offset.</li> <li>Can compact multiple small files in bigger file.</li> <li>It keeps track of committed osffset in iceberg.</li> <li>Write data as encrypted at source level.</li> <li>Charge for sink connector and egress is waived, pricing is based on per topic/hour and GB processed.</li> <li>When using both Iceberg  and Delta metadata, the data will not be duplicated in the S3 bucket.</li> <li>This is a simple to setup by enabling Tableflow synching at the Kafka topic level.</li> <li>Support Upsers semantic</li> </ul>"},{"location":"techno/cc-tableflow/#current-limitations","title":"Current limitations","text":"<ul> <li>DLQ not supported yet</li> <li>Iceberg format is not supported in Databricks</li> <li>Catalog integration through private link, one catalog per cluster.</li> <li>Debezium CDC support</li> </ul>"},{"location":"techno/cc-tableflow/#architecture","title":"Architecture","text":"<ul> <li>Kafka cluster on Confluent Cloud</li> <li>Tableflow own catalog</li> <li>S3 access policy and service role</li> <li>The data is visible into the bucket after 15mn or 250MB filled.</li> <li>300 MBs per kafka partition uncompressed payload</li> <li>Busy topic the quicker the data will be visible in the table</li> <li>Amazon Athena by using the AWS Glue Data Catalog</li> </ul>"},{"location":"techno/cc-tableflow/#special-capabilities","title":"Special Capabilities","text":"<ul> <li>Upserts: update on the same key- Tombstone records are supported as Delete operation</li> <li>High performance unbound deduplication window</li> <li>Supports: 8+ B Unique rows per table</li> <li>DLQ: events that fails to materialize to the table are logged in a separate destination: this is relevant for zero tolerance for data loss. Each topic may have its own DLQ.</li> <li>Tableflow enables integrating with the AWS Glue Data Catalog as an external catalog, allowing the metadata of Apache Iceberg\u2122 tables materialized by Tableflow to be published to AWS Glue. The AWS Glue Data Catalog integrates with Tableflow at the kafka cluster level, enabling the automatic publication of all Tableflow-enabled topics as tables within Glue</li> <li>Perform all write and maintenance operations through Tableflow, to ensure consistency and correctness.</li> </ul>"},{"location":"techno/cc-tableflow/#iceberg-tables-in-s3","title":"Iceberg Tables in S3","text":"<p>The integration process includes:</p> <ul> <li>Getting a S3 bucket</li> <li> <p>Creating an IAM Role with policy to read, putobject,.. on S3 bucket. Or if using an existing IAM Role (e.g. <code>j9r-role</code> so the role can access S3:     <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:ListAllMyBuckets\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    },\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:ListBucket\",\n            \"s3:GetBucketLocation\",\n            \"s3:ListBucketMultipartUploads\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::j9r-demo-buckets\"\n        ]\n    },\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:PutObject\",\n            \"s3:PutObjectTagging\",\n            \"s3:GetObject\",\n            \"s3:AbortMultipartUpload\",\n            \"s3:ListMultipartUploadParts\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::j9r-demo-buckets/*\"\n        ]\n    }\n    ]\n}\n</code></pre></p> </li> <li> <p>Creating a Confluent Provider Integration to grant CC access to the AWS S3 bucket. It uses IAM Roles based authorization. The  provider integration to act-as a trusted identity. See step by step instructions. When asked to enter AWS role ARN, use your existing IAM role (e.g. <code>arn:aws:iam::8.....5:role/j9r-role</code>).</p> </li> <li> <p>Update the IAM role trust relationships by specifying the principal and externalId from Confluent provider integration IAM role to be able to assume the AWS role we configured (e.g. <code>j9r-role</code>).     <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"arn:aws:iam::8......2:role/cspi-1y3y5\"\n        },\n        \"Action\": \"sts:AssumeRole\",\n        \"Condition\": {\n            \"StringEquals\": {\n                \"sts:ExternalId\": \"636d..........dd\"\n            }\n        }\n    },\n    {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"arn:aws:iam::8.....2:role/cspi-1y3y5\"\n        },\n        \"Action\": \"sts:TagSession\"\n    }\n    ]\n}\n</code></pre></p> </li> <li> <p>Enable Tableflow at the topic level</p> </li> <li>Wait for the topic to be materialized as an Iceberg table in that bucket. This may take a few minutes depending on segment roll and throughput.</li> <li>Integrate Tableflow with AWS Glue Data Catalog: Tableflow &gt; External Catalog Integrations &gt; AWS Glue:<ul> <li>Provide an IAM role for Confluent to assume, with the required Glue permissions (Create/Get/Update/Delete Database/Table, etc.)     <pre><code># statement to add to the list of statements\n{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"glue:GetTable\",\n        \"glue:GetDatabase\",\n        \"glue:DeleteTable\",\n        \"glue:DeleteDatabase\",\n        \"glue:CreateTable\",\n        \"glue:CreateDatabase\",\n        \"glue:UpdateTable\",\n        \"glue:UpdateDatabase\"\n    ],\n    \"Resource\": [\n        \"arn:aws:glue:us-west-2:829250931565:*\"\n    ]\n}\n</code></pre></li> <li>Launch the integration. Tableflow then publishes Iceberg table metadata to AWS Glue for all materialized tables in that Kafka cluster. Here is a view of the created tables from the transaction processing: </li> </ul> </li> </ul>"},{"location":"techno/cc-tableflow/#external-query","title":"External query","text":"<ul> <li>Need to define a catalog like AWS Glue, Databricks Unity Catalog: The Cluster Id will become the database in Glue.</li> </ul>"},{"location":"techno/cc-tableflow/#query-with-aws-athena","title":"Query with AWS Athena","text":"<ul> <li>Configure AWS Glue Catalog integration in Tableflow (see above)</li> <li>Ensure you have the required permissions for Amazon Athena to access the table\u2019s storage bucket. This translates as: attach an IAM policy granting s3:GetObject, s3:ListBucket, s3:GetBucketLocation, etc., to the IAM Role Athena uses (often the Glue Service Role), ensure the S3 Bucket Policy allows access from that role/account, and check AWS KMS permissions if encryption is used.</li> <li>Ensure AWS Athena has read-only access to the Glue catalog and the storage bucket.</li> </ul>"},{"location":"techno/cc-tableflow/#query-tableflow-tables-with-duckdb","title":"Query TableFlow tables with Duckdb","text":"<ul> <li>Install the DuckDB command-line tool.     <pre><code>curl https://install.duckdb.org | sh\nor\nuv add duckdb-cli\n</code></pre></li> <li>Create a new Tableflow API Key for the cluster where Tableflow is enabled, use the key name as a the duckdb CLIENT_ID and the api key secret as CLIENT_KEY</li> <li>Retrieve the connection detail (REST Catalog Endpoint) for the TableFlow API, something like <code>https://tableflow.&lt;&lt;REGION&gt;&gt;.aws.confluent.cloud/iceberg/catalog/organizations/YOUR-ORG-ID/environments/YOUR-ENV-ID</code></li> <li>Match the database to the Kafka cluster_id and the topic name being the table name</li> <li> <p>In the duckdb shell, add the iceberg extension. The installation is done only one time.     <pre><code>INSTALL iceberg;\nLOAD iceberg;\n</code></pre></p> </li> <li> <p>Add a secret definition:     <pre><code>CREATE SECRET iceberg_secret (\n    TYPE ICEBERG,\n    CLIENT_ID     'YOUR_CLIENT_ID',\n    CLIENT_SECRET 'YOUR_CLIENT_SECRET',\n    ENDPOINT      'YOUR_ENDPOINT_URL',\n    OAUTH2_SCOPE  'catalog'\n)\n</code></pre></p> </li> <li> <p>Attach the tableflow catalog by using an alias: <code>ice_cat</code> <pre><code>ATTACH 'warehouse' AS ice_cat (\n    TYPE iceberg,\n    SECRET iceberg_secret,\n    ENDPOINT 'YOUR_ENDPOINT_URL'\n    );\n</code></pre></p> </li> <li> <p>Run SQL queries on top of the tableflow tables:     <pre><code>SELECT * FROM ice_cat.\"lkc-3mnm0m\".\"customer_analytics_c360\";\n-- add joins, filters,..\n</code></pre></p> </li> <li> <p>In case of session timeout, restart DuckDB and rerun recreate the secret and attach the catalog again.</p> </li> <li>See also a Python application using duckdb integration in this app</li> </ul>"},{"location":"techno/ccloud-flink/","title":"Confluent Cloud for Apache Flink","text":"Chapter updates <ul> <li>Created 10/2024 </li> <li>Review 10/31/24 Updated 12/10/2025</li> </ul> <p>Confluent Cloud for Apache Flink\u00ae is a cloud-native, managed service, for Flink, strongly integrated with the Confluent Cloud Kafka managed service. It is a simple, serverless and scalable way to build real-time, reusable data products over streams.</p> <p>Confluent Cloud Flink is built on the same open-source version as Apache Flink\u00ae with additional features:</p> <ul> <li>Regional service to run Flink in a serverless offering</li> <li>Auto-inference of the Confluent Cloud environment, Kafka cluster , topics and schemas, to Flink SQL constructs of catalog, databases and tables.</li> <li>Autoscaling capabilities, up and down</li> <li>Default system column for timestamps using the <code>$rowtime</code> column.</li> <li>Default watermark strategy based on <code>$rowtime</code>.</li> <li>Support for Avro, JSON Schema, and Protobuf.</li> <li>CREATE TABLE statements provision resources as Kafka topics and schemas (temporary tables not supported).</li> <li>Read from and write to Kafka in two modes: append-stream or update-stream (upsert and retract).</li> </ul> <p>Some limitations:</p> <ul> <li>No support for DataStream apps.</li> <li>No support or Flink connectors, only Kafka</li> </ul>"},{"location":"techno/ccloud-flink/#comparaison-with-apache-flink","title":"Comparaison with Apache Flink","text":"<p>The following table is current January 2026. Roadmap changes and product features are delivered on a weekly basis.</p> Apache Flink Confluent Cloud Flink Self managed clusters, versions, state backends, checkpointing, security, connectors, and integration with Kafka and other systems Serverless Flink service fully managed by Confluent, tightly integrated with Confluent Cloud Kafka, Schema Registry, security, and governance Create compute pools (regional, elastic resource pools) and run SQL/Table API statements against them Always on the latest Flink runtime; security patches and minor upgrades are applied automatically to running statements Tune parallelism Automatically scale up or down to meet the demands of the most complex workloads without overprovisioning DataStream,  SQL/Table API SQL/Table API Define catalogs/databases/tables, schemas, provision Kafka topic yourself. Environments, clusters, topics, and schemas become catalogs, databases, tables, and table schemas automatically. <code>Create table</code> provision topics and schemas Must define watermark strategies for event\u2011time processing by code System column $rowtime mapped to Kafka record timestamp with default watermark strategy. Multiple Kafka connectors, and other sources/sinks One Kafka connector with support to append/upsert mode from table configuration, Other system is via Confluent Cloud managed connectors Full SQL surface Limitation on Database and Catalog actions Integrate with your own IAM, ACLs, encryption, auditing, governance tooling Inherits Confluent Cloud IAM, RBAC, audit logs, and governance; Flink defines dedicated roles like FlinkDeveloper and FlinkAdmin layered on top of existing Kafka RBAC Support your networking integration NSame private networking and networking controls as Confluent Cloud Kafka (VPC peering / Private Link / CC Integrate metrics and logs into your own stack. Per cluster dashboard Confluent UI has built\u2011in Flink statements view, lag/throughput metrics, and error states. Native integration with Prometheus/Datadog via Confluent Cloud Metrics API and Notifications <p>Pay attention that most Flink managed services are cloud-hosted and not cloud-native: only the infrastructure layer is fully managed and there\u2019s still a lot of manual tasks for day 2 operations. </p> <p>References:</p> <ul> <li>Introducing Confluent Cloud for Apache Flink https://www.confluent.io/blog/introducing-flink-on-confluent-cloud/</li> <li>Stream Processing with Confluent Cloud for Apache Flink | Confluent Documentation</li> <li>Comparing Apache Flink with Confluent Cloud for Apache Flink | Confluent Documentation</li> <li>Manage Flink Compute Pools in Confluent Cloud for Apache Flink | Confluent Documentation</li> <li>Monitor and Manage Flink SQL Statements in Confluent Cloud for Apache Flink | Confluent Documentation</li> </ul>"},{"location":"techno/ccloud-flink/#key-concepts","title":"Key Concepts","text":"<ul> <li>This is a regional service, in one of the three major cloud providers. It is defined in a context of a Confluent's environment.</li> <li>Compute pools groups resources for running Flink clusters, which may scale down to zero. They run SQL statements. Maximum pool size is defined at creation. Statements, in different compute pools, are isolated from each other. </li> <li>Capacity is measured in Confluent Flink Unit, CFU. Each statement is at least 1 CFU-minute.</li> <li>A statement may be structural (DDL) and stop once completed, or runs in background to write data to table (DML).</li> <li>Supports multiple Kafka clusters within the same Confluent Cloud organization in a single region.s</li> <li>Any table created in CC Flink appears as a topic in CC Kafka. Kafka Topics and schemas are always in synch with Flink.</li> <li>The differences with the OSS version, is that the DDL statements of catalog, database, table are mapped to physical Kafka objects. Table is a schema and a topic, catalog is an environment, and database is a Kafka cluster.</li> <li>Developers work in a workspace, to manage their Apache Flink\u00ae streaming applications, allowing them to easily write, execute, and monitor real-time data processing queries using a user-friendly SQL editor. Workspaces are not mandatory, as Developers may also deploy Flink statements via CLI or REST API.</li> <li>CC offers the Autopilot feature, to automatically adjusts resources for SQL statements based on demand. When messages processing starts to be behind, Autopilot adjusts resource allocation.</li> <li>Supports role-based access control for both user and service accounts.</li> <li>Stream lineage provides insights at the topic level about data origin to destinations. </li> <li>For Watermark configuration, Confluent Cloud for Apache Flink\u00ae manages it automatically, by using the <code>$rowtime</code> column, which is mapped to the Kafka record timestamp, and by observing the behavior of the streams to dynamically adapt the configuration.</li> <li>Service accounts are used for production deployment to enforce security boundaries. Permissions are done with ACL and role binding. They can own any type of API keys that can be used for CLI or API access.</li> <li> <p>Snapshot query helps to do apoint-in-time/snapshot query, to get a result at the moment of query submission, and that query would transition to Completed once done. Generates only one final result set. It will query from kafka topic earliest record, until now, or can mix with Tableflow parquet table. This is a combination of Flink batch + time constraint query.      <pre><code>SET 'sql.snapshot.mode' = 'now';\nSELECT count(*) as nb_records from tablename;\n</code></pre></p> <p>See simple demo</p> </li> <li> <p>External lookups</p> </li> </ul> Statement life cycle <p>Use a service account for background statements. Submit a SQL statement using the client shell:</p> <pre><code>confluent flink shell --compute-pool ${COMPUTE_POOL_ID} --environment ${ENV_ID} --service-account ${account_id}\n</code></pre> <p>It is possible to pause and resume a SQL statement. See cookbook for the best practices and process to update existing statements. </p> How to change the CFU limit? <p>CFU can be changed via the console or the cli, up to the limit of 50. Going above developers need to open a ticket to the Confluent support.</p> What is behind a compute pool? <p>A compute pool groups 1 job manager and n task manager. Task manager resource configuration is not configurable and is designed to support small usage as well as moderate traffic. The limit to 50 CFUs is to address trade-off between coordination overhead and scaling needs. A Flink dag with source and sink operators impact the throughput of task manager so it is always challenging to assess how many task manager to be support by a job manager.  Large states are persisted to disk and this impact the compute pool resources too. </p> <ul> <li>Statement can be moved between compute pools</li> </ul>"},{"location":"techno/ccloud-flink/#confluent-cloud-architecture","title":"Confluent Cloud Architecture","text":"<p>The Confluent Cloud for Kafka and for Flink is based on the SaaS pattern of control and data planes. See this presentation - video from Frank Greco Jr.</p> <ul> <li>Each data plane is made of a VPC, a kubernetes cluster, a set of Kafka clusters and some management services to support platform management and communication with the control plane.</li> <li>The control plane is called  the mothership, and refers to VPC, services, Database to manage the multi-tenancy platform, a kubernetes cluster, Kafka cluster, and other components. This is where the Confluent console runs for users to administer the Kafka clusters. </li> <li>For each data plane VPC, outbound connections are allowed through internet gateways.</li> <li>There is a scheduler service to provision resources or assign cluster to existing resources. Target states are saved in a SQL database, while states are propagated from the different data planes to the mothership. This communication is async and leverage a global Kafka cluster.</li> <li>There are the concepts of physical Kafka clusters and logical clusters. Logical clusters are groupings of topics on the physical clusters isolated from each other via a prefix. Professional Confluent Cloud organization can only have logical clusters. Enterprise can have physical clusters.</li> </ul>"},{"location":"techno/ccloud-flink/#getting-started","title":"Getting Started","text":"<p>Install the Confluent CLI and get an Confluent Cloud account. </p> <p>See those tutorials for getting started.</p> <ul> <li>Quickstart with Console</li> <li>Apache Flink\u00ae SQL</li> <li>Confluent github, Flink workshop</li> <li>Java Table API Quick Start</li> </ul> <p>There is also a new confluent cli plugin: <code>confluent-flink-quickstart</code> to create an environment, a Flink compute pool, enable a schema registry, create a Kafka cluster and starts a Flink shell. </p> <pre><code>confluent flink quickstart --name my-flink-sql --max-cfu 10 --region us-west-2 --cloud aws\n</code></pre>"},{"location":"techno/ccloud-flink/#some-common-commands-to-manage-confluent-cloud-environment","title":"Some common commands to manage Confluent Cloud environment","text":"<pre><code># Create an environment\nconfluent environment create my_environment --governance-package essentials\n# Set the active environment.\nconfluent environment use &lt;environment id&gt;\n# Create a cluster\nconfluent Kafka cluster create my-cluster --cloud gcp --region us-central1 --type basic\n# Create Kafka API key\nconfluent Kafka cluster list\nexport CLID=&lt;Kafka cluster id&gt;\nconfluent api-key create --resource $CLID\n# Create a compute pool (adjust cloud and region settings as required).\nconfluent flink compute-pool create my-compute-pool --cloud gcp --region us-central1 --max-cfu 10\n# Create a Flink api key which is scoped in an environment + region pair\nconfluent api-key create --resource flink --cloud gcp --region us-central1\n# Define an api key for schema registry\nconfluent schema-registry cluster describe\nconfluent api-key create --resource &lt;schema registry cluster&gt;\n# Get the user id\nconfluent iam user list\n# To shutdown everything:\nconfluent environment list\nconfluent environment delete &lt;ENVIRONMENT_ID&gt;\n</code></pre> <p>For study and demonstration purpose, there is a read-only catalog named <code>examples</code> with database called <code>marketplace</code> which has data generators for different SQL tables. </p> <p>Set the namespace for future query work using:</p> <pre><code>use catalog examples;\nuse marketplace;\nshow tables;\n</code></pre> <p>To use your dedicated environment use the following syntax:</p> <pre><code>use catalog my-flink-sql_environment;\nuse  my-flink-sql_Kafka-cluster;\n</code></pre>"},{"location":"techno/ccloud-flink/#use-the-flink-sql-shell","title":"Use the Flink SQL shell","text":"<p>Using the confluent cli, we can access to the client via:</p> <pre><code>#  \nconfluent environment list\n\n# Get the compute pool id\nconfluent flink compute-pool list\nexport ENV_ID=$(confluent environment list -o json | jq -r '.[] | select(.name == \"aws-west\") | .id')\nexport COMPUTE_POOL_ID=$(confluent flink compute-pool list -o json | jq -r '.[0].id')\nconfluent flink shell --compute-pool $COMPUTE_POOL_ID --environment $ENV_ID\n</code></pre>"},{"location":"techno/ccloud-flink/#using-the-flink-editor-in-confluent-cloud","title":"Using the Flink editor in Confluent Cloud","text":"<p>Nothing special to mention, except that users need to recall that once the job is started, they cannot modify it:they need to stop before any future edition. Restarting may mean reprocess from the earliest records. It is recommended to persist the Flink statement in a git repository and manage the deployment using Confluent CLI or the shift_left CLI tool.</p>"},{"location":"techno/ccloud-flink/#using-the-flink-table-api","title":"Using the Flink Table API","text":"<p>Confluent Cloud for Flink supports the Table API, in Java or Python.</p> <p>The Table API code is a client SDK, it communicates with Confluent Cloud by using REST requests to send SQL statement to the job manager and is interpreted by the SQL engine. </p> <p>The Table API program acts as a client-side library for interacting with the Flink engine hosted in the cloud. It enables the submission of <code>Statements</code> and retrieval of <code>StatementResults</code>. The provided Confluent plugin integrates specific components for configuring the TableEnvironment, eliminating the need for a local Flink cluster. By including the <code>confluent-flink-table-api-java-plugin</code> dependency, Flink's internal components\u2014such as CatalogStore, Catalog, Planner, Executor, and configuration, are managed by the plugin and fully integrated with Confluent Cloud. This integration is via the REST API, so Confluent Table API plugin is an higher emcapsulation of the CC REST API. </p> <p>The code runs on an external systems, but uses an specific Flink environment for Confluent Cloud to submit the DAG to the remote engine.</p> <p>When running TableAPI with Confluent Cloud for Flink plugin, we need to provide configurations via properties file or Environment variables. </p> <ol> <li> <p>Set the environment variables to connect to Confluent Cloud:     <pre><code>export FLINK_API_KEY=\"&lt;your-flink-api-key&gt;\"\nexport FLINK_API_SECRET=\"&lt;your-flink-api-secret&gt;\"\nexport ORG_ID=\"&lt;your-organization-id&gt;\"\nexport ENV_ID=\"&lt;your-environment-id&gt;\"\nexport COMPUTE_POOL_ID=\"&lt;your-compute-pool-id&gt;\"\nexport CLOUD_PROVIDER=\"aws\"\nexport CLOUD_REGION=\"us-east-1\"\n</code></pre></p> </li> <li> <p>Create a Table environment in the Java or Python, using the template approach for URLs:     <pre><code>ConfluentSettings settings1 = ConfluentSettings.newBuilder()\n  .setRegion(\"us-east-1\")\n  .setCloud(\"aws\")\n  .setEndpointTemplate(\"https://flinkpls-dom123.{region}.{cloud}.confluent.cloud\")\n  .setArtifactEndpointTemplate(\"https://artifacts.{region}.{cloud}.custom-domain.com\")\n  // Other required settings...\n  .build();\n</code></pre></p> </li> <li> <p>Package and run</p> </li> </ol> <p>Read this chapter for more information.</p>"},{"location":"techno/ccloud-flink/#dlq-support","title":"DLQ support","text":"<p>In production deployment, Flink statements may fail because of serialization errors due to one of the following reasons:</p> <ul> <li>Schema does not exists</li> <li>There are one or more bad messages in the topic that are not compliant with the schema</li> <li>Got some connection challenge to the schema registry</li> </ul> <p>Dead Letter Queue is now supported via SQL configuration to the underlying Kafka connector. By integrating Custom Deserialization Error Handling Strategies, data engineers can ensure that only valid, correctly processed messages move downstream, maintaining data quality and integrity. This feature reduces the risk of system crashes and downtime caused by unhandled exceptions, ensuring continuous data processing and availability.</p> <p>In order to re-process the data, the Data engineer will have to write a specific SQL statement that reads from the DLQ. </p> <p>For certain queries (like stateful operations, such as joins) data engineers need to consider that reprocessing DLQ data at a later moment will result in incorrect results downstream, because of the order in how data is being processed. If correct results are required, the only solution will be to fully reprocess data from a topic without bad messages.</p> <p>All Flink tables have <code>error-handling.mode</code> as a table option, with the default being <code>fail</code>. See product documentation.</p> <ul> <li> <p>If desired, you can run an ALTER TABLE to change this to <code>ignore</code> or <code>log</code>. Those alteration should be done to topics created outside of CREATE table done with Flink SQL. The CDC output topics are good candidates for such modifications.</p> <pre><code>ALTER TABLE raw_users_table SET ('error-handling.mode' = 'log');\n</code></pre> <p>The DLQ topic uses a specific generic DLQ schema, which includes information such as the key and value as bytes (since deserialization failed, there\u2019s nothing else to represent), plus the schema ID that was being tried. It includes metadata like error message and the statement ID that triggers the error. </p> </li> <li> <p>or add this config to the created table:     <pre><code>create table src_users_table (...) WITH (\n    ....\n    'error-handling.mode' = 'log'\n)\n</code></pre></p> </li> </ul> Potential error <p>It is possible to get the following error when altering table failed registering schemas: unable to register schema on 'error_log-value': schema registry request failed error code: 42205: Subject error_log-value in context  is not in read-write mode. In this case, you will run into this error as flink is trying to register a schema for the DLQ and Schema Regisry is being schema-linked as a result the default context is in Read only mode. You need to create your own DLQ table. </p> <ul> <li> <p>Or create a special DLQ topic:     <pre><code>CREATE TABLE `my_error_log` (\n    `error_timestamp` TIMESTAMP_LTZ(3) NOT NULL,\n    `error_code` INT NOT NULL,\n    `error_reason` STRING NOT NULL,\n    `error_message` STRING NOT NULL,\n    `error_details` MAP&lt;STRING NOT NULL, STRING&gt; NOT NULL,\n    `processor` STRING NOT NULL,\n    `statement_name` STRING,\n    `affected_type` STRING NOT NULL,\n    `affected_catalog` STRING,\n    `affected_database` STRING,\n    `affected_name` STRING,\n    `source_record` ROW&lt;`topic` STRING, `partition` INT, `offset` BIGINT, `timestamp` TIMESTAMP_LTZ(3), `timestamp_type` STRING, `headers` MAP&lt;STRING NOT NULL, VARBINARY&gt;, `key` VARBINARY, `value` VARBINARY&gt;\n) WITH (\n    'value.avro-registry.schema-context' = '.flink-stage',\n    'value.format' = 'avro-registry'\n    )\n</code></pre></p> <p>Alter the source table to enable with a DLQ that was created above.</p> <pre><code>ALTER TABLE src_users_table SET ('error-handling.mode' = 'log', 'error-handling.log.target' = 'my_error_log' );\n</code></pre> </li> </ul>"},{"location":"techno/ccloud-flink/#networking-overview","title":"Networking overview","text":"<p>See the product documentation for managing Networking on Confluent Cloud. </p> <p>Kafka clusters have the following properties:</p> <ul> <li>Basic and standard clusters are multi-tenant and accessible via secured (TLS encrypted) public endpoints.</li> <li>Using private link does not expose Kafka clusters to the public.</li> <li>Enterprise clusters are accessible through secure AWS PrivateLink or Azure Private Link connections.</li> <li>Secure public endpoints are protected by a proxy layer that prevents types of DoS, DDoS, syn flooding, and other network-level attacks.</li> <li>A Confluent Cloud network is an abstraction for a single tenant network environment. See setup CC network on AWS.. </li> <li>For AWS and Confluent Dedicated Clusters, networking can be done via VPC peering, transit gateway, inbound and outbound private link (for Kafka and Flink): this is a one-way connection access from a VPC to CC.</li> <li>Flink Private Networking requires a PrivateLink Attachment (PLATT) to access Kafka clusters with private networking. It is used to connect clients such as confluent CLI, the console, the rest api or terraform with Flink. Flink-to-Kafka is routed internally within Confluent Cloud.</li> </ul> <ul> <li>PLATT is independant of the network type: PrivateLink, VPC peering or transit GTW.</li> </ul>"},{"location":"techno/ccloud-flink/#autopilot","title":"Autopilot","text":"<p>Autopilot automatically scales up and down compute pool resources needed by SQL statements. It uses the property of parallelism for operator to be able to scale up and down. <code>SELECT</code> always runs a parallelism of 1. Only <code>CREATE TABLE AS</code>, <code>INSERT INTO</code> and <code>EXECUTE STATEMENT SET</code> are considered by Autopilot for scaling. Global aggregate are not parallelized. The main goal of the auto scaler is to maintain optimum  throughput and number of resources (or CFUs). </p> <p>The SQL workspace reports the scaling status.  It is important that each job has a maximum parallelism, limited by the number of resource available. For source operators within a Flink DAG the limit is the number of partitions in the input topics. </p> <p>If there is some data skew and one operator is set with a parallel of 1 then there is no need to scale.</p> <p>When the compute pool is exhausted, try to add more CFU or stop some running statements to free up resources.</p> <p>The autoscaler is using historical metrics to take the decision to scale up. 3 to 4 minutes of data are needed. A job should scale up within minutes if the backlog is constantly growing, and scale down if there are no input data and the backlog. The interesting metrics is the pending records. The algorithm needs to take into account the pending records amount, the current processing capacity, the time to scale up, but also the input data rate, the output data rate for each operator in the DAG. There is no way updfront to estimate the needed capacity. This is why it is important to assess the raw input table/kafka size and avoid restarting the first Flink statements that are filtering, deduplicating records to reduce the number of messages to process downstream of the data pipeline.</p> <p>Autopilot exposes the CFU usage in CFU minutes via the metrics API at the compute pool level.</p> <p>When multiple statements are in the same compute pool, new statement will not get resource until existing one scales down. Consider looking at Statement in Pending state and reallocated them to other compute pool. The total number of jobs is less than the CFU limit.</p> When a statement is not scaling up what can be done? <pre><code>Consider looking at the CFU limit of the compute poolas it may has been reached. The Flink job may have reached it\u2019s effective max parallelism, due to not enough Kafka topic partition from the input tables. Consider looking at the data skew, as a potential cause for scale-ups inefficiency. \nInternally to Confluent Cloud for Flink, checkpoints may take a long time. The autopilot may rescale only after the current checkpoint has completed or 2 checkpoints have failed in a row.\n</code></pre> <p>See discussion of adaptive scheduler from Flink FLIP-291.</p>"},{"location":"techno/ccloud-flink/#cross-region-processing","title":"Cross-region processing","text":"<p>Within an environment, there is one schema registry. We can have multiple Kafka clusters per region and multiple Flink compute pools per region. Any tables created in both region with the same name will have the value and key schemas shared in the central schema registry. The SQL Metastore, Flink compute pools and Kafka clusters are regional. </p>"},{"location":"techno/ccloud-flink/#monitoring-and-troubleshouting","title":"Monitoring and troubleshouting","text":"<p>The metrics API documentation.</p> <p>You must create an API key to authenticate your requests to the Metrics API.</p>"},{"location":"techno/ccloud-flink/#statement-monitoring","title":"Statement monitoring","text":"<p>Once the Flink SQL statement runs, Data Engineers may use the Console, (Environment &gt; Flink &gt; Flink page &gt; Flink statements) to assess the list of statements and their state of processing. The product documentation - Monitoring and manage Flink SQL statements goes into the details of it.</p> <p>See the monitoring product documentation for explanations of the different fields. The following fields are important to consider:</p> Field Why to consider Status Verify the state of the Flink query Statement CFU Server resource used by the statement Messages Behind Is the query behind, is there some backpressure applied Message out Rate of messages created by the query State Size in GB Keep it low, alert at 300+ GB <p>Look at the statement status, consider failed, pending, degraded. Some issues are recoverables, some not:</p> Recoverable Non-recoverable User Kafka topic deletion, loss of access to cloud resources De/Serialization exception, arithmetic exception, any exception thrown in user code System checkpointing failure, networking disruption Actions If recovery takes a long time or fails repeatedly, and if this is a user execption, the message will be in the status.detail of the statement, else the user may reach to the support. User needs to fix the query or data. <p>Be sure to enable cloud notifications and at least monitor topic consumer lag metric. As a general practices, monitoring for <code>current_cfus = cfu_limit</code> to avoid exhaustion of compute pools.  </p> <p>The <code>flink/pending.records</code> is the most important metrics to consider. It corresponds to consumer lag in Kafka and \u201cMessages Behind\u201d in the Confluent Cloud UI. Monitor for high and increasing consumer lag.</p> <p>At the Statement level we can get the following metrics, over time:</p> <p></p>"},{"location":"techno/ccloud-flink/#degraded-statement","title":"Degraded Statement","text":"<p>In degraded mode, statement could not make progress..</p> <p>For internal system error, this may be linked to resources issue. See standard resolution approaches:</p> <ul> <li>run your query with the EXPLAIN </li> <li>Access Queery Profiler to look for bottlenexts, data flow issue at the operator level.</li> <li>Verify CFU usage</li> </ul>"},{"location":"techno/ccloud-flink/#query-profiler","title":"Query Profiler","text":"<p>Query Profiler helps developers to get visibility into each operator of the query DAG, with metrcis like CPU utilization, state size, ...</p> <p></p> <p>Developers can now visualize data flow, track data volume processed by each operator, and identify operators experiencing high latency, unbounded state growth, or backpressure.</p> <p>As explained in Flink Dataflow concept, a task is responsible for executing a specific part of the data processing logics. As a task may be divided into multiple subtasks, the metrics at the task level will be calculated by aggregating together the metrics from the subtasks inside the task. </p> <p>As Flink optimizes the operators execution plan, so operator level metrics are also reported. Operators which are chained together form tasks, tasks will be presented visually by grouping operators together inside a rectangle.</p> <p>Consider assessing the following metrics:</p> <ul> <li>Backpressure</li> <li>Busyness</li> <li>Bytes in and out</li> <li>State Size</li> <li>Watermark</li> </ul> <p>watermark and watermark alignment status is presenter for the partitions in a Kafka topic that Flink reads data from. The watermark alignment status has 3 columns: blocked, active, idle with percentages to represent how much time the Kafka partition is contributing to watermark alignment. Partition watermarks metric is available for Flink source operators only. Developer may be able to see idle partitions, or idleness behavior over time (this last issue may impact the record processing with delay from second to several minutes).</p> <p>Metric skew is presented at the task level. The skew tell developers if there is a particular subtask inside the task that is problematic and they then may take the relevant actions to fix it. Data skew metrics are for throughput, state size, busyness and backpressure.  They are aggregated up to create task level and operator level. Data skew indicates the degree of variation in these performance metrics across subtasks and operators, telling developers how well balanced the workloads are. </p> <p>In Confluent Cloud, Flink uses kafka topic/partition as sources and sinks. The topic partitioning significantly increases scalability and performance as it allows for higher throughput of message processing. This applies to the Flink sources or sink connectors.</p> <p>A Flink statement, is a Flink application, but the Flink UI is not exposed. This WebApp uses an old software stack, that may not fit well in modern SaaS platform.</p> EXPLAIN vs Query Profiler <p>The EXPLAIN command outputs the Flink query plan showing the operations that Flink will execute and the changelog mode used to handle state updates.  This gives users insight into how Flink understands and plans to execute the Statement. Developers use it during development and adjustment. There is no data metrics.</p> <p>Query profiler is dynamic with data metrics. It helps developers to diagnose performance issues during or after statement execution.</p> <p>See the product demonstration - how to guide to analyze a temporal join with EXPLAIN and Query profiler. To avoid statement to stop change the SQL as: <pre><code>create table last_orders(\n  order_id VARCHAR(2147483647) NOT NULL PRIMARY KEY not enforced,\n  ts TIMESTAMP_LTZ(3),\n  customer_id INT,\n  product_id VARCHAR(2147483647),\n  name VARCHAR(2147483647),\n  email VARCHAR(2147483647),\n  price DOUBLE\n) DISTRIBUTED BY (order_id) WITH (\n  'changelog.mode' = 'upsert'\n) as \nSELECT\n  o.order_id,\n  o.`$rowtime` as ts,\n  c.customer_id,\n  o.product_id,\n  c.name,\n  c.email,\n  o.price\nFROM examples.marketplace.orders o\nJOIN examples.marketplace.customers FOR SYSTEM_TIME AS OF o.`$rowtime` c\nON o.customer_id = c.customer_id\nWHERE o.`$rowtime` &gt;= CURRENT_TIMESTAMP - INTERVAL '1' HOUR;\n</code></pre></p>"},{"location":"techno/ccloud-flink/#metrics","title":"Metrics","text":"<ul> <li>Confluent Cloud for Apache Flink supports metrics integrations with services like Prometheus, Grafana and Datadog.</li> </ul> <ul> <li>Flink monitoring statement product documentation</li> <li>Docker compose, Prometheus setup and Grafana Dashboard for Confluent Cloud for Flink reporting.</li> </ul>"},{"location":"techno/ccloud-flink/#compute-pool-monitoring","title":"Compute pool monitoring","text":"<p>See Grafana integration.</p>"},{"location":"techno/ccloud-flink/#some-common-errors","title":"Some common errors","text":"<ul> <li> <p>Some query with a lot of joins (10+) on static data, do not returns the same results when doind a SELECT from in CC Workspace. This behavior for foreground query, may be possible, as query plan construction may have timedout. When quesry start to be complex and need to process multiple real records and not just some test data, it is recommended to move to INSERT INTO sink_table SELECT ... and then use the Workspace to look at the table inside the sink_table. The query will run in background and may take sometime to deploy, but will run.</p> </li> <li> <p>DML statement failing, or being degraded, or pending can be notified to external system. See the notification for CC documentation</p> </li> </ul>"},{"location":"techno/ccloud-flink/#role-base-access-control","title":"Role Base Access Control","text":"<p>The product documentation goes into sufficient details on how RBAC works for Flink. </p> <ul> <li>Remember some inport facts:<ul> <li>When registering to CC, one org is created with the user who registered. Other users are invited.</li> </ul> </li> </ul> <p>The following table list some classical use case and expected role: | Use Case | Least privilege role needed | | -------- | ----------- | | Create environment  |  EnvironmentAdmin |  | Create compute pool | FlinkAdmin scoped at environment level | | Deploy flink statement | FlinkDeveloper (FlinkAdmin too) |  | Create R/W Schemas in SR | DataSteward | | Admin the org | OrganizationAdmin |  </p> <p>Examples of Terraform definitions for service accounts, roles, and role binding cc-terraform for my env.</p>"},{"location":"techno/ccloud-flink/#understanding-pricing","title":"Understanding pricing","text":"<p>The CFU pricing is here. Price per hour computed by the minute. </p> <p>Some core principals:</p> <ul> <li>Flink SQL runs each statement independently of any others.</li> <li>Not overpay for processing capacity. Pay for what is used. Increment at the minute level.</li> <li>Short live queries cost a real minimum, and can be done in a shared compute pool</li> <li>Long running queries cost is aggregated per hour with minute increment. So a statement starting at 1 CFU for 10 minutes then 3 CFUs for 30 and back to 2 for 10 and 1 for 10 will use 10 + 90 + 20 + 10 = 130 CFUs for the hour.</li> <li>Statement throughput generally scales linearly in the number of CFUs available to a statement.</li> <li>The Max CFU parameter is a just for Budget control</li> </ul> <p>To estimate CFU consumption we need to:</p> <ol> <li>Expected record per second (RPS) / throughput </li> <li>Message size and total number of messages to process</li> <li> <p>Type of SQL, select only, or joins, grouping...</p> <ul> <li>simple 1 to 1 select stateless transformation is determined by how much write volume the sink topic can handle.</li> <li>For Joins, aggregates, ... the way in which a statement must access and maintains the state is more influential than the raw quantity of state.</li> <li>The total of all CFU estimates across the workload will provide a rough approximation of total CFUs required</li> </ul> </li> </ol> <p>Several factors significantly affect statement throughput so pricing:</p> <ul> <li>State Overhead: The overhead related to maintaining state affects JOINs and aggregations more than the quantity of state itself. </li> <li>Records in topic: if there some burst in ingress traffic to Kafka topics, then Flink may need more resources for a short time period to process the lag. </li> <li>CPU Load: The complexity of the operations performed by the statement is a major contributor to CPU load.</li> <li>Minimum CFU Consumption: Every statement will consume at least 1 CFU, and for most workloads, CFU consumption is directly proportional to the number of statements execute.</li> <li>VM machine type may also impact throughput metrics</li> </ul>"},{"location":"techno/ccloud-flink/#scoping-workload","title":"Scoping workload","text":"<ul> <li>Assess the number of record per seconds</li> <li>Be sure to clarrify that Confluent Cloud for Flink is used for streaming not batching. Operations like update, truncate, delete table are not supported.</li> <li>For stateless the attainable throughput of the statement per CFU will generally be determined by how much write volume the sink topic can handle.</li> <li>Most important throughput factor is the State size, its access and management. </li> <li>Statement throughput generally scales linearly in the number of CFUs available to a statement.</li> <li>UDF impacts throughtput.</li> <li>For each statement, assess the number of records to process per seconds or minutes. Consider ingress message size and egress message size as SQL may generates less data. Also Windowing will generate less messages too. Joins will impact performance depending if they are static or with time window. </li> <li>Look at Kafka message sizes (bytes) as well as message throughput.</li> </ul> <p>As a base for discussion, 10k record/s per CPU is reachable for simple Flink stateless processing.</p>"},{"location":"techno/ccloud-flink/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>CC Flink is a regional, multi-AZ service. </li> <li>In case of Job failure, failed jobs auto-restart using the last known states loaded from the last checkpoint.</li> <li>Checkpoint is used for in-region fault tolerance. Checkpoints capture the state of a Flink job at regular intervals, including Kafka consumer offsets, operator states, and internal timers. In CC checkpoints are done every minute.</li> <li>In case of Cloud Provider failure, there is no protection, for a region lost. To address that, architects need to set up a cross region DR strategy.</li> <li>All Flink DR options first require a DR strategy for Kafka &amp; Schema Registry (SR). It needs to have an exact replication of the data (including offsets) and schemas.</li> <li>On CC, cluster link and schema link supports data and schema replication.</li> </ul> <p>As any flink solution, the following need to be deeply assessed:</p> <ul> <li>What Flink application and SQL statements to consider in scope of DR?</li> <li>Can Flink's state be recreated?  This is driven by the underlying Kafka Clusters RPO and their retention.</li> <li>How long is tolerable to recreate that state? This is driven by the overall RTO. </li> <li>What is the semantic expected by consumer apps? This is driven by consuming apps tolerances. Semantics options are: exactly-once, at-least once (duplicate possible), at-most once (data loss and duplicate possible).</li> <li>Is the Flink job processing deterministic? will a Flink job always output the same results?</li> </ul> <p>A generic view of DR components is presented in following figure:</p>"},{"location":"techno/ccloud-flink/#active-active","title":"Active / Active","text":"<p>The approach is to have two identical Flink jobs or pipelines of jobs run in parallel continuously in both regions. They process the same data, with some replication delay in the secondary region.</p> <p>This is recommended for low RTO requirements, with Flink jobs with large states, or solutions requiring Exactly-Once semantics, or when it is critical that the 2 regions have exactly the same data results.</p> <p>To consider:</p> <ul> <li>Setup replication only to the input topics. </li> <li>Mirror configuration like service accounts, RBACs, private networking...</li> <li>Ensure Flink jobs have deterministic query results.</li> <li>Jobs should support out-of-order arrival between input tables.</li> </ul>"},{"location":"techno/ccloud-flink/#active-passive","title":"Active / Passive","text":"<p>Flink jobs are started, in second region, only on failover.</p> <p>This approach is possible for stateless jobs, or when states can be created quickly: Flink Jobs Window Size and time to recompute job state &lt; RTO. Solutions based on at-least once, or at-most-once. Even for stateless jobs, Exactly-Once semantics is not supported.</p> <p>To consider:</p> <ul> <li>More complicated to orchestrate as the process needs to recreate tables and jobs during failover</li> <li>Topic retention &gt; Time window needed to recreate state (dictated by window size or TTL). Without enough retention, results will be wrong, or only subset of queries would work.</li> <li>Any time window and aggregation needs to use event time and not processing time.</li> <li>Setup replication only to the input topics. </li> </ul>"},{"location":"techno/ccloud-flink/#some-faqs","title":"Some FAQs","text":"<ul> <li>Processing time support? It is better to use event time to ensure results are correct, deterministic and reproductible. Using processing time for windowing operations may lead to non-determnistic results. Processing time may be relevant for temporal joins.</li> <li>Is Hive load/unload function supported? The Flink OSS has this load/unload Hive functions capability, but all those Hive functions are already available in CC Flink.</li> <li>How to add Jar? ADD and REMOVE JAR are meant for testing purposes in OSS Flink. For UDFs, Jars can be uploaded via Confluent Console, CLI, REST API or Terraform.</li> <li>How to manage Catalog and Database? In Confluent Cloud Catalog is a Confluent Environment so no direct management from Flink session. Database is a Kafka cluster so the same logic applies.</li> </ul>"},{"location":"techno/ccloud-flink/#vscode-extension","title":"VScode extension","text":"<p>VScode extension to manage Confluent Cloud or Platform resources. </p>"},{"location":"techno/ccloud-flink/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Confluent Flink workshop to learn how to build stream processing applications using Apache Flink\u00ae on Confluent Cloud.</li> <li>Shoe-store workshop with Terraform and SQL demonstration using DataGen.</li> <li>SQL coding practices from this repo. and this.</li> </ul>"},{"location":"techno/cp-flink/","title":"Confluent Platform for Flink","text":"<p>The official product documentation after 07/2025 release is here. </p> <p>CP Flink is the Confluent Flink product that is extremely flexible and configurable to address a large set of user's requirements. It is a supported Flink with a subset of Flink components: SQL, Table API, DataStream API, ProcessFunction. Using Kafka, FileSystem, JDBC and CDC connectors, and using a central management feature for Kubernettes deployment and security.</p> <p>The main features are:</p> <ul> <li>Fully compatible with open-source Flink. </li> <li>Deploy on Kubernetes using Helm. It is only supported on kube. On Kubernetes, machine or Flink process failures will be recovered by Kubernetes, guaranteeing high uptime, and low latency.</li> <li>Define environment, which does a logical grouping of Flink applications with the goal to provide access isolation, and configuration sharing.</li> <li>Deploy application with user interface and task manager cluster</li> <li>Exposes custom kubernetes operator for specific CRD</li> <li>When integrated with Kafka, Kafka CP can run on bare metal while CP Flink on kube.</li> </ul> <p>The figure below presents the Confluent Flink components deployed on Kubernetes:</p> Confluent Platform for Flink <ul> <li>CFK supports the management of custom CRD, based on the Flink for Kubernetes Operator. (CFK 3.0.0 supports CP 8.0.0)</li> <li>CMF (Confluent Manager for Apache Flink) adds security control, and a REST API server for the cli or a HTTP client</li> <li>FKO is the open source Flink for Kubernetes Operator</li> <li>Flink cluster are created from command and CRDs and run Flink applications within an environment</li> </ul> <p>Be sure to have confluent cli.</p>"},{"location":"techno/cp-flink/#references-compendium","title":"References Compendium","text":"<ul> <li>Platform overview</li> <li>QuickStart using Docker compose with the All in one git repo.</li> <li>Kubernetes Operator</li> <li>Scripted Confluent Platform Demo</li> <li>Kafka Connector</li> </ul>"},{"location":"techno/cp-flink/#specific-concepts-added-on-top-of-flink","title":"Specific concepts added on top of Flink","text":"<ul> <li> <p>Confluent Manager for Flink (CMF) provides:</p> <ul> <li>Job life-cycle management for Flink jobs.</li> <li>Integration with Confluent Platform for authentication and authorization (RBAC).</li> <li>Well-defined REST APIs and command-line interfaces (CLIs).</li> <li>Store metadata in its own embedded database</li> <li>It is a kubernetes operator to manage custom resources</li> </ul> </li> <li> <p>Environment: for access control isolation, and Flink configuration sharing</p> </li> <li>Compute pool represents resources to run Task manager and Job manager. Each Flink SQL statement is associated with exactly one Compute Pool. See example of pool definition and in cmf folder</li> <li> <p>SQL catalog to group database concept for Flink SQL table queries. It references a Schema Registry instance and one or more Kafka clusters.</p> </li> <li> <p>See Product FAQs</p> </li> </ul>"},{"location":"techno/cp-flink/#product-set-up","title":"Product Set Up","text":"<p>The Confuent Flink images and Helm chart references:</p> <ul> <li>Maven repo: https://packages.confluent.io/maven/io/confluent/flink/</li> <li>Docker Images: https://hub.docker.com/r/confluentinc/cp-flink</li> <li>Helm charts are used to install the Confluent Manager for Apache Flink (CMF) application and the Flink Kubernetes operator. (e.g. helm pull confluentinc/confluent-manager-for-apache-flink). </li> <li>CP Flink may be deployed in air-gapped environments by downloading from the internet using docker pull and then stored in a tar archive using docker image save.</li> </ul> <p>Demonstrations:</p> <ul> <li>See my dedicated chapter for Confluent Plaform Kubernetes deployment.</li> <li>See the makefile to deploy CMF, and the product documentation </li> </ul>"},{"location":"techno/cp-flink/#deployment-architecture","title":"Deployment architecture","text":"<ul> <li>A Flink cluster always needs to run in one K8s cluster in one region, as the Flink nodes are typically exchanging a lot of data, requiring low latency network. See K8S deployment architecture - chapter</li> </ul> <ul> <li>For failover between data centers, the approach if to share durable storage (HDFS, S3, Minio) accessible between data centers, get Kafka topic replicated by keeping offset numbers, and being able to restart Flink Application from checkpoints.</li> </ul> <ul> <li>Also recall that Flink may be able to interact with multiple Kafka Clusters at the same time, which may be interesting to read from one Kafka cluster in Region A and write to another Kafka cluster in Region B.</li> </ul>"},{"location":"techno/cp-flink/#important-source-of-information-for-deployment","title":"Important Source of Information for Deployment","text":"<ul> <li>Confluent Platform deployment using kubernetes operator</li> <li>Deployment overview and for Apache Flink. </li> <li>CP Flink supports K8s HA only.</li> <li>Flink fine-grained resource management documentation.</li> <li>CP v8 announcement: builds on Kafka 4.0, next-gen Confluent Control Center (integrating with the open telemetry protocol (OTLP),  Confluent Manager for Apache Flink\u00ae (CMF)), Kraft native (support significantly larger clusters with millions of partitions), Client-Side field level encryption. CP for Flink support SQL, Queues for Kafka is now in Early Access, </li> </ul>"},{"location":"techno/cp-flink/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Authentication and authorization includes access to Confluent Platform components from external systems and users and for intra-components communication. The following figure illustrates what needs to be covered:</p> <p></p> <p>By default CP components are not configured to include encryption, authentication and authorization.</p> <ul> <li>See managing security in Confluent Platform</li> <li>TLS is used to encrypt communication</li> <li>OAuth 2.0 supported</li> <li>Possible to use Confluent Cloud for SSO</li> <li>LDAP supports for users and group</li> </ul>"},{"location":"techno/cp-flink/#authentication","title":"Authentication","text":"<p>The product documentations for CP authentication can be summarized as:</p> <ul> <li>Kafka brokers accesses from producer/consumer applications can be done using SASL (Simple Authentication Security Layer), mutualTLS to verify clients and server identities to ensure that traffic is secure and trusted in both directions.</li> <li>Admin REST API authentication is done via HTTP Basic Auth</li> <li>Access to the C3 may be done via SSO</li> <li>It supports using LDAP for authentication or OAuth/OIDC where all identities across Confluent Platform workloads and interfaces can be hosted on a single identity provider providing for a unified identity management solution. See a Keycloak docker compose demo, and my keycloak deployment on k8s.</li> <li>See SSL/TLS on how to use openssl for cryptography encryption.</li> <li>By default inter broker communication is PLAINTEXT, but it can be set to SSL</li> <li>Secrets can be saved not AWS Secrets Manager or HashiCorp Vault.</li> </ul>"},{"location":"techno/cp-flink/#authorization","title":"Authorization","text":"<p>The product documentations for CP Authorization highlights the following major points:</p> <ul> <li>Access to Confluent Platform resources as clusters, topics, consumer groups, and connectors are controlled via ACLs. ACLs are stored in the KRaft-based Kafka cluster metadata.</li> <li>Group based authorization can be defined in LDAP</li> <li>The control of who can access what, is via RBAC with roles such as <code>ClusterAdmin</code> or <code>DeveloperRead</code> assigned to users and service accounts.</li> <li>ACLs may be used, instead of RBAC</li> </ul>"},{"location":"techno/cp-flink/#metadata-management-service-mds-for-rbac-for-flink","title":"MetaData Management Service (MDS) for RBAC for Flink","text":"<p>Flink Job represents code. So it is critical to setup authentication and authorization. Flink supports TLS/SSL for authentication and encryption of network traffic between Flink processes, both for internal connectivity (between Flink processes) and external connectivity (From outside to Flink processes).</p> <p>Recall that, keystore and a truststore must be set up such that the truststore trusts the keystore\u2019s certificates.</p> <p>CMF supports mTLS authentication and OAuth authentication. As CMF is deployed via kubernetes operator, SREs have control over who may deploy Flink applications. All access should go through CMF to comply with the authentication and authorization requirements.</p> <ul> <li> <p>For mTLS the approach is to define a mtls-values.yaml and use it when deploying or upgrading the cmf helm chart.     <pre><code>helm upgrade --install cmf confluentinc/confluent-manager-for-apache-flink -f mtls-values.yaml\n</code></pre></p> <p>See and example of mtls-values.yaml which declares the truststore file to use. The certificates should be defined in a secret.</p> </li> </ul> Defining SSL certs <pre><code>Use [keytool](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html) to create self-certified certificates and keys.\n```sh\nkeytool -genkeypair -alias flink.internal -keystore internal.keystore -dname \"CN=flink.internal\" \\\n        -storepass internal_store_password -keyalg RSA -keysize 4096 -storetype PKCS12\n```\nDefine a secret with the base64 string of the certificates:\n```sh\ncat your_certificate.crt | base64 -w 0\ncat your_private_key.key | base64 -w 0\n```\n\nAnd a secret.yaml\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n    name: cmf-certs\ntype: kubernetes.io/tls\ndata:\n    tls.crt: &lt;base64_encoded_certificate&gt;\n    tls.key: &lt;base64_encoded_private_key&gt;\n```\n</code></pre>"},{"location":"techno/cp-flink/#roles","title":"Roles","text":"<p>The Flink Kubernetes operator installs two custom roles: flink-operator and flink:</p> <ul> <li><code>flink-operator</code> is used to manage <code>FlinkDeployment</code> resources: meaning it creates and manages the JobManager deployment for each Flink job (and related resources). </li> <li><code>flink</code> role is used by the jobManager process of each job to create and manage the taskManager and configMap resources.</li> </ul> <p>With FKO, job and task managers are dep loyed together so there is no need for the <code>flink</code> service account to have permissions to launch additional pods. K8s handles the TaskManager Pod lifecycle. With native k8s mode, flink code may could inherit the JobManager's Kubernetes permissions, and so being able to launch other pods to access sensitive cluster resoirces.</p>"},{"location":"techno/cp-flink/#rbac","title":"RBAC","text":"<ul> <li>CMF communicates with the MDS to validate whether the principal of the incoming request is allowed to perform a specific action.</li> <li>CMF requires its own service principal to authorize user principals.</li> </ul>"},{"location":"techno/cp-flink/#metdata-service-mds","title":"Metdata service (MDS)","text":"<p>MDS manages a variety of metadata about your Confluent Platform installation. It is configured on each Kafka broker. It is possible to centralize into a hub and spoke architecture of multiple kafka clusters, Connect and schema registry. MDS maintains a local cache of authorization data that is persisted to an internal Kafka topic named _confluent-metadata-auth.</p> <ul> <li>Metadata Service Overview</li> <li>Single broker Kafka+MDS Deployment</li> <li>Git repo with working CP on K8s deployment using RBAC via SASL/Plain and LDAP</li> <li>Configure CP Flink to use MDS for Auth</li> <li>Additional Flink RBAC Docs</li> <li>How to secure a Flink job with RBAC</li> <li>Best Practices for K8s + RBAC</li> </ul>"},{"location":"techno/cp-flink/#sql-specific","title":"SQL specific","text":"<p>As of November 2025 SQL support is still under-preview.</p>"},{"location":"techno/cp-flink/#applications","title":"Applications","text":"<p>Important points:</p> <ul> <li>After CMF is installed and running, it will continuously watch Flink applications.</li> <li>Environment control k8s namespace in which the Flink application is deployed.</li> <li>Suspended app, does not consume resource, and may be restored from its savepoint.</li> <li> <p>CLI, REST API or the Control Center Console may be used to deploy app.</p> </li> <li> <p>See the getting started with a Java project for Flink, see also the Confluent specific settings.</p> </li> </ul>"},{"location":"techno/cp-flink/#understanding-sizing","title":"Understanding Sizing","text":"<p>The observed core performance rule is that Flink can process ~10,000 records per second per CPU core. This baseline may decrease with larger messages, bigger state, key skew, or high number of distinct keys.</p> <p>There are a set of characteristics to assess before doing sizing:</p>"},{"location":"techno/cp-flink/#statement-complexity-impact","title":"Statement Complexity Impact","text":"<p>Statement complexity reflects the usage of complex operators like:</p> <ul> <li>Joins between multiple streams</li> <li>Windowed aggregations</li> <li>Complex analytics functions</li> </ul> <p>More complex operations require additional CPU and memory resources.</p>"},{"location":"techno/cp-flink/#architecture-assumptions","title":"Architecture Assumptions","text":"<ul> <li>Source &amp; Sink latency: Assumed to be minimal</li> <li>Key size: Assumed to be small (few bytes)</li> <li>Minimum cluster size: 3 nodes required</li> <li>CPU limit: Maximum 8 CPU cores per Flink node</li> </ul>"},{"location":"techno/cp-flink/#state-management-checkpointing","title":"State Management &amp; Checkpointing","text":"<ul> <li>Working State: Kept locally in RocksDB</li> <li>Backup: State backed up to distributed storage</li> <li>Recovery: Checkpoint size affects recovery time</li> <li>Frequency: Checkpoint interval determines state capture frequency</li> <li>Aggregate state size consumes bandwidth and impacts recovery latency.</li> </ul>"},{"location":"techno/cp-flink/#resource-guidelines","title":"Resource Guidelines","text":"<p>Typical Flink node configuration includes:</p> <ul> <li>4 CPU cores with 16GB memory</li> <li>Should process 20-50 MB/s of data</li> <li>Jobs with significant state benefit from more memory</li> </ul> <p>Scaling Strategy: Scale vertically before horizontally</p>"},{"location":"techno/cp-flink/#latency-impact-on-resources","title":"Latency Impact on Resources","text":"<p>Lower latency requirements significantly increase resource needs:</p> <ul> <li>Sub-500ms latency: +50% CPU, frequent checkpoints (10s), boosted parallelism</li> <li>Sub-1s latency: +20% CPU, extra buffering memory, 30s checkpoints</li> <li>Sub-5s latency: +10% CPU, moderate buffering, 60s checkpoints</li> <li>Relaxed latency (&gt;5s): Standard resource allocation Stricter latency requires more frequent checkpoints for faster recovery, additional memory for buffering, and extra CPU for low-latency optimizations.</li> </ul>"},{"location":"techno/cp-flink/#estimation-heuristics","title":"Estimation Heuristics","text":"<p>The estimator increases task managers until there are sufficient resources for:</p> <ul> <li>Expected throughput requirements</li> <li>Latency-appropriate checkpoint intervals</li> <li>Acceptable recovery times</li> <li>Handling data skew and key distribution</li> </ul> Important Notes <ul> <li>These are estimates - always test with your actual workload</li> <li>Start with conservative estimates and adjust based on testing</li> <li>Monitor your cluster performance and tune as needed</li> <li>Consider your specific data patterns and business requirements</li> </ul> <p>See this Flink estimator project for a tool to help estimating cluster sizing.</p>"},{"location":"techno/cp-flink/#monitoring","title":"Monitoring","text":""},{"location":"techno/cp-flink/#troubleshouting","title":"Troubleshouting","text":""},{"location":"techno/cp-flink/#submitted-query-pending","title":"Submitted query pending","text":"<p>When submitting SQL query from the Flink SQL, it is possible it goes in pending: the job manager pod is created and running, but the task manager is pending. Assess what is going on with <code>kubectl describe pod &lt;pod_id&gt; -n flink</code>, which gives error message like insufficient cpu. Adding compute pool cpu is worse as the compute pool spec for task manager and job manager is for the default settings to create those pods.</p>"},{"location":"techno/fk-k8s-monitor/","title":"Monitoring Flink on Kubernetes","text":"<p>In this chapter, we assume Flink operator is deployed and some FlinkDeployments are running.</p>"},{"location":"techno/fk-k8s-monitor/#verify-operator","title":"Verify operator","text":"<ul> <li>Get the namespace wher Flink operator runs</li> </ul> <pre><code>kubectl use \nkubectl get pods \n</code></pre>"},{"location":"techno/fk-k8s-monitor/#monitoring-operator-health","title":"Monitoring Operator Health","text":""},{"location":"techno/fk-k8s-monitor/#prometheus","title":"Prometheus","text":""},{"location":"techno/fk-k8s-monitor/#dynatrace","title":"Dynatrace","text":""},{"location":"techno/sizing/","title":"Sizing","text":"<p>Sizing a Flink cluster is a complex process influenced by many factors, including workload demands, application logic, data characteristics, expected state size, required throughput and latency, concurrency, and hardware. </p> <p>Because of those variables, every Flink deployment needs a unique sizing approach. The most effective method is to run a real job, on real hardware and tune Flink to that specific workload.</p> <p>For architects seeking sizing guidance, it's helpful to consider:</p> <ul> <li>the workload semantic complexity, with the usage of aggregations, joins, windows, processing type, </li> <li>the input throughput (MB/s or records/second), </li> <li>the expected state size (GB), </li> <li>the expected latency.</li> </ul> <p>While Kafka sizing estimates are based on throughput and latency, this is a very crude method for Flink, as it overlooks many critical details. </p> <p>For new Flink deployments, a preliminary estimate can be provided, but it's important to stress its inexact nature.  A simple Flink job can process approximately 10,000 records per second per CPU. However, a more substantial job, based on benchmarks, might process closer to 5,000 records per second per CPU. Sizing may use record size, throughput, and Flink statement complexity to estimate CPU load.</p>"},{"location":"techno/sizing/#cp-flink-estimation","title":"CP Flink Estimation","text":"<p>I tentatively built a flink-estimator webapp with backend estimator for Apache Flink or Confluent Platform Cluster sizing.</p> <p>The tool needs to be simple, so it persists the estimation as json on the local disk. </p> <p>To access this web app there is a docker image at dockerhub - flink-estimator. </p> <p>The approach is to clone the repository: https://github.com/jbcodeforce/flink-estimator </p> <ul> <li>use <code>docker-compose up -d</code> </li> <li> <p>deploy it to a local kubernetes cluster:      <pre><code>kubectl apply -k k8s\n</code></pre></p> </li> <li> <p>Access via web browser http://localhost:8002/</p> </li> </ul> <p></p>"},{"location":"techno/sizing/#cc-flink-estimation","title":"CC Flink Estimation","text":"<p>The focus on Confluent Cloud Flink is to assess the number of CFU per statements.</p>"}]}